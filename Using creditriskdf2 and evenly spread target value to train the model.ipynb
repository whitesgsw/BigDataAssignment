{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "creditRiskdf2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "creditRiskdf3: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val creditRiskdf1 = spark.read.option(\"header\",\"true\").csv(\"creditRiskdf2_1.csv\")\n",
    "val creditRiskdf2 = spark.read.option(\"header\",\"true\").csv(\"creditRiskdf2_2.csv\")\n",
    "val creditRiskdf3 = spark.read.option(\"header\",\"true\").csv(\"creditRiskdf2_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "import org.apache.spark.ml.Pipeline\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1and2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "creditRiskdf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val creditRiskdf1and2 = creditRiskdf1.union(creditRiskdf2)\n",
    "var creditRiskdf = creditRiskdf1and2.union(creditRiskdf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "test_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "validation_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var Array(train_df,test_df,validation_df)=creditRiskdf.randomSplit(\n",
    "    Array(0.7,0.2,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskFeatures: Array[String] = Array(EXT_SOURCE_2, EXT_SOURCE_3, DAYS_BIRTH, AMT_CREDIT, AMT_ANNUITY, DAYS_ID_PUBLISH, previous_loans_CNT_PAYMENT_mean, previous_loans_SELLERPLACE_AREA_max, DAYS_LAST_PHONE_CHANGE, DAYS_REGISTRATION, CODE_GENDER_F, REGION_POPULATION_RELATIVE, client_cash_CNT_INSTALMENT_max_mean, bureau_DAYS_CREDIT_mean, previous_loans_NAME_CONTRACT_STATUS_Refused_count_norm, client_installments_NUM_INSTALMENT_VERSION_max_mean, bureau_DAYS_CREDIT_min, previous_loans_NAME_YIELD_GROUP_high_count_norm, previous_loans_RATE_DOWN_PAYMENT_sum, previous_loans_DAYS_FIRST_DUE_min, client_cash_SK_DPD_DEF_sum_max, bureau_AMT_CREDIT_SUM_LIMIT_mean, bureau_DAYS_CREDIT_sum, previous_loans_DAYS_LAST_DUE_1ST_VERSION_max, previous_loans_PRODUCT_COMBINATION_Cash X-Sell: low_count_norm, ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val creditRiskFeatures = creditRiskdf.columns\n",
    "val features = creditRiskFeatures.filter(!_.contains(\"TARGET\")).filter(!_.contains(\"SK_ID_CURR\")) //Make sure we exclude the ID column and label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskFeatures: Array[String] = Array(EXT_SOURCE_2, EXT_SOURCE_3, DAYS_BIRTH, AMT_CREDIT, AMT_ANNUITY, DAYS_ID_PUBLISH, previous_loans_CNT_PAYMENT_mean, previous_loans_SELLERPLACE_AREA_max, DAYS_LAST_PHONE_CHANGE, DAYS_REGISTRATION, CODE_GENDER_F, REGION_POPULATION_RELATIVE, client_cash_CNT_INSTALMENT_max_mean, bureau_DAYS_CREDIT_mean, previous_loans_NAME_CONTRACT_STATUS_Refused_count_norm, client_installments_NUM_INSTALMENT_VERSION_max_mean, bureau_DAYS_CREDIT_min, previous_loans_NAME_YIELD_GROUP_high_count_norm, previous_loans_RATE_DOWN_PAYMENT_sum, previous_loans_DAYS_FIRST_DUE_min, client_cash_SK_DPD_DEF_sum_max, bureau_AMT_CREDIT_SUM_LIMIT_mean, bureau_DAYS_CREDIT_sum, previous_loans_DAYS_LAST_DUE_1ST_VERSION_max, previous_loans_PRODUCT_COMBINATION_Cash X-Sell: low_count_norm, ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val creditRiskFeatures = creditRiskdf.columns\n",
    "for (colName<-creditRiskFeatures){\n",
    "     |   creditRiskdf=creditRiskdf.withColumn(colName,col(colName).cast(\"Float\"))\n",
    "     | }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df9: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "res8: Long = 2359\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df9=creditRiskdf.filter($\"TARGET\" === 1.0)\n",
    "df9.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df10: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "res9: Long = 33224\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df10=creditRiskdf.filter($\"TARGET\" === 0.0)\n",
    "df10.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df11: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "df12: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Now from 33224 rows of TARGET==0, we randomly extract 2300 rows and combine it with df9\n",
    "//To get a dataframe of 2300 target==1 and 2300 target ==0\n",
    "var Array(df11,df12)=df10.randomSplit(\n",
    "    Array(0.071,0.929))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df13: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df13=df9.union(df11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Long = 4704\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df13.count()//The df13 contains 2300 rows of TARGET==1 and 2300 rows of TARGET==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res24: Long = 2345\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df13.filter($\"TARGET\" === 0.0).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res25: Long = 2359\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df13.filter($\"TARGET\" === 1.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_7dfcbcd84946\n",
       "labelIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_30afb35e5c2a\n",
       "model: org.apache.spark.ml.classification.LogisticRegression = logreg_d87b5777f29e\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(features).setOutputCol(\"features\")\n",
    "val labelIndexer = new StringIndexer().setInputCol(\"TARGET\").setOutputCol(\"label\")\n",
    "val model = new LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_32cc07b75833\n",
       "df2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 187 more fields]\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(features).setOutputCol(\"features\")\n",
    "val df2 = assembler.transform(df13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_daa40bb8d872\n",
       "df3: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 188 more fields]\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelIndexer = new StringIndexer().setInputCol(\"TARGET\").setOutputCol(\"label\")\n",
    "val df3 = labelIndexer.fit(df2).transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_26f22abc20b5, numClasses = 2, numFeatures = 186\n",
       "predictions2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 191 more fields]\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = new LogisticRegression().fit(df3)\n",
    "val predictions2 = model.transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "import org.apache.spark.mllib.regression.LabeledPoint\n",
       "import org.apache.spark.mllib.util.MLUtils\n",
       "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area under the receiver operating characteristic (ROC) curve : 0.7051602943316483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 191 more fields]\n",
       "predictionLabelsRDD1: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[1756] at map at <console>:53\n",
       "binMetrics1: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@4206f5e1\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions2 = lrModel.transform(df13)\n",
    "val predictionLabelsRDD1 = predictions2.select(\"prediction\", \"label\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics1 = new BinaryClassificationMetrics(predictionLabelsRDD1)\n",
    "println(\"area under the receiver operating characteristic (ROC) curve : \" + binMetrics1.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lp: org.apache.spark.sql.DataFrame = [label: double, prediction: double]\n",
       "counttotal: Long = 4704\n",
       "correct: Long = 3317\n",
       "wrong: Long = 1387\n",
       "truep: Long = 1651\n",
       "truen: Long = 1666\n",
       "falseN: Long = 679\n",
       "falseP: Long = 708\n",
       "ratioWrong: Double = 0.29485544217687076\n",
       "ratioCorrect: Double = 0.7051445578231292\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lp = predictions2.select( \"label\", \"prediction\")\n",
    "val counttotal = predictions2.count()\n",
    "val correct = lp.filter($\"label\" === $\"prediction\").count() //correct prediction\n",
    "val wrong = lp.filter(not($\"label\" === $\"prediction\")).count() //wrong prediction\n",
    "val truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" === $\"prediction\").count() //Correct prediction of label=0\n",
    "val truen = lp.filter($\"prediction\" === 1.0).filter($\"label\" === $\"prediction\").count() //Correct prediction of label=1\n",
    "val falseN = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" === $\"prediction\")).count() //Wrong prediction when label=1\n",
    "val falseP = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" === $\"prediction\")).count() //Wrong prediction when lable =0\n",
    "val ratioWrong=wrong.toDouble/counttotal.toDouble //Wrong prediction ratio\n",
    "val ratioCorrect=correct.toDouble/counttotal.toDouble //Correct predicition ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+--------------------+----------+\n",
      "|SK_ID_CURR|            features|label|         probability|prediction|\n",
      "+----------+--------------------+-----+--------------------+----------+\n",
      "|  100049.0|(186,[0,1,2,3,4,5...|  0.0|[0.41297183801247...|       1.0|\n",
      "|  100902.0|(186,[0,1,2,3,4,5...|  0.0|[0.40944020354736...|       1.0|\n",
      "|  101042.0|(186,[0,1,2,3,4,5...|  0.0|[0.38802373681337...|       1.0|\n",
      "|  101197.0|(186,[0,1,2,3,4,5...|  0.0|[0.63103137138405...|       0.0|\n",
      "|  101277.0|(186,[0,1,2,3,4,5...|  0.0|[0.91460324467911...|       0.0|\n",
      "|  101286.0|(186,[0,1,2,3,4,5...|  0.0|[0.60208368726355...|       0.0|\n",
      "|  101495.0|(186,[0,1,2,3,4,5...|  0.0|[0.51719934824546...|       0.0|\n",
      "|  101859.0|(186,[0,1,2,3,4,5...|  0.0|[0.63584047678167...|       0.0|\n",
      "|  101980.0|(186,[0,1,2,3,4,5...|  0.0|[0.72854178873096...|       0.0|\n",
      "|  102003.0|(186,[0,1,2,3,4,5...|  0.0|[0.78686212416355...|       0.0|\n",
      "|  102025.0|(186,[0,1,2,3,4,5...|  0.0|[0.67750359233151...|       0.0|\n",
      "|  102521.0|(186,[0,1,2,3,4,5...|  0.0|[0.91508860492092...|       0.0|\n",
      "|  102574.0|(186,[0,1,2,3,4,5...|  0.0|[0.54665255658381...|       0.0|\n",
      "|  102586.0|(186,[0,1,2,3,4,5...|  0.0|[0.83555990512389...|       0.0|\n",
      "|  102679.0|(186,[0,1,2,3,4,5...|  0.0|[0.48316722722034...|       1.0|\n",
      "|  102875.0|(186,[0,1,2,3,4,5...|  0.0|[0.77534384471543...|       0.0|\n",
      "|  102955.0|(186,[0,1,2,3,4,5...|  0.0|[0.60410124247854...|       0.0|\n",
      "|  103134.0|(186,[0,1,2,3,4,5...|  0.0|[0.66772921874347...|       0.0|\n",
      "|  103310.0|(186,[0,1,2,3,4,5...|  0.0|[0.78393980493061...|       0.0|\n",
      "|  103325.0|(186,[0,1,2,3,4,5...|  0.0|[0.61388217224368...|       0.0|\n",
      "+----------+--------------------+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions2.select (\"SK_ID_CURR\",\"features\", \"label\",\"probability\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
