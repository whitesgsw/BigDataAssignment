{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload clean dataset 'creditriskdf1' with outliers removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "creditRiskdf2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "creditRiskdf3: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val creditRiskdf1 = spark.read.option(\"header\",\"true\").csv(\"creditRiskdf1_1.csv\")\n",
    "val creditRiskdf2 = spark.read.option(\"header\",\"true\").csv(\"creditRiskdf1_2.csv\")\n",
    "val creditRiskdf3 = spark.read.option(\"header\",\"true\").csv(\"creditRiskdf1_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "import org.apache.spark.ml.Pipeline\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1and2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "creditRiskdf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val creditRiskdf1and2 = creditRiskdf1.union(creditRiskdf2)\n",
    "var creditRiskdf = creditRiskdf1and2.union(creditRiskdf3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataframe to extract a test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskFeatures: Array[String] = Array(EXT_SOURCE_2, EXT_SOURCE_3, DAYS_BIRTH, AMT_CREDIT, AMT_ANNUITY, DAYS_ID_PUBLISH, previous_loans_CNT_PAYMENT_mean, previous_loans_SELLERPLACE_AREA_max, DAYS_LAST_PHONE_CHANGE, DAYS_REGISTRATION, CODE_GENDER_F, REGION_POPULATION_RELATIVE, client_cash_CNT_INSTALMENT_max_mean, bureau_DAYS_CREDIT_mean, previous_loans_NAME_CONTRACT_STATUS_Refused_count_norm, client_installments_NUM_INSTALMENT_VERSION_max_mean, bureau_DAYS_CREDIT_min, previous_loans_NAME_YIELD_GROUP_high_count_norm, previous_loans_RATE_DOWN_PAYMENT_sum, previous_loans_DAYS_FIRST_DUE_min, client_cash_SK_DPD_DEF_sum_max, bureau_AMT_CREDIT_SUM_LIMIT_mean, bureau_DAYS_CREDIT_sum, previous_loans_DAYS_LAST_DUE_1ST_VERSION_max, previous_loans_PRODUCT_COMBINATION_Cash X-Sell: low_count_norm, ..."
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val creditRiskFeatures = creditRiskdf.columns\n",
    "val features = creditRiskFeatures.filter(!_.contains(\"TARGET\")).filter(!_.contains(\"SK_ID_CURR\")) //Make sure we exclude the ID column and label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskFeatures: Array[String] = Array(EXT_SOURCE_2, EXT_SOURCE_3, DAYS_BIRTH, AMT_CREDIT, AMT_ANNUITY, DAYS_ID_PUBLISH, previous_loans_CNT_PAYMENT_mean, previous_loans_SELLERPLACE_AREA_max, DAYS_LAST_PHONE_CHANGE, DAYS_REGISTRATION, CODE_GENDER_F, REGION_POPULATION_RELATIVE, client_cash_CNT_INSTALMENT_max_mean, bureau_DAYS_CREDIT_mean, previous_loans_NAME_CONTRACT_STATUS_Refused_count_norm, client_installments_NUM_INSTALMENT_VERSION_max_mean, bureau_DAYS_CREDIT_min, previous_loans_NAME_YIELD_GROUP_high_count_norm, previous_loans_RATE_DOWN_PAYMENT_sum, previous_loans_DAYS_FIRST_DUE_min, client_cash_SK_DPD_DEF_sum_max, bureau_AMT_CREDIT_SUM_LIMIT_mean, bureau_DAYS_CREDIT_sum, previous_loans_DAYS_LAST_DUE_1ST_VERSION_max, previous_loans_PRODUCT_COMBINATION_Cash X-Sell: low_count_norm, ..."
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val creditRiskFeatures = creditRiskdf.columns\n",
    "for (colName<-creditRiskFeatures){\n",
    "     |   creditRiskdf=creditRiskdf.withColumn(colName,col(colName).cast(\"Float\"))\n",
    "     | }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "test_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "validation_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var Array(train_df,test_df,validation_df)=creditRiskdf.randomSplit(\n",
    "    Array(0.7,0.2,0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe 'df13' with same amount of rows for TARGET==0 and TARGET==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res38: Long = 183875\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditRiskdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df9: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "res39: Long = 13312\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df9=creditRiskdf.filter($\"TARGET\" === 1.0)\n",
    "df9.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df10: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "res40: Long = 170563\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df10=creditRiskdf.filter($\"TARGET\" === 0.0)\n",
    "df10.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df11: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "df12: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Now from 33224 rows of TARGET==0, we randomly extract 2300 rows and combine it with df9\n",
    "//To get a dataframe of 2300 target==1 and 2300 target ==0\n",
    "var Array(df11,df12)=df10.randomSplit(\n",
    "    Array(0.078,0.922))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df13: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df13=df9.union(df11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res47: Long = 26446\n"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df13.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res48: Long = 13134\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df13.filter($\"TARGET\" === 0.0).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res49: Long = 13312\n"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df13.filter($\"TARGET\" === 1.0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with df13 instead of train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_015fcd2452b8\n",
       "labelIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_acca6523988e\n",
       "model: org.apache.spark.ml.classification.LogisticRegression = logreg_f38d23fd9ba4\n"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(features).setOutputCol(\"features\")\n",
    "val labelIndexer = new StringIndexer().setInputCol(\"TARGET\").setOutputCol(\"label\")\n",
    "val model = new LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_5ebc6a77e6fc\n",
       "df2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 187 more fields]\n"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(features).setOutputCol(\"features\")\n",
    "val df2 = assembler.transform(df13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_2e1d53c13f82\n",
       "df3: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 188 more fields]\n"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelIndexer = new StringIndexer().setInputCol(\"TARGET\").setOutputCol(\"label\")\n",
    "val df3 = labelIndexer.fit(df2).transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_1757437c3634, numClasses = 2, numFeatures = 186\n",
       "predictions2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 191 more fields]\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = new LogisticRegression().fit(df3)\n",
    "val predictions2 = model.transform(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model with test_df which have much more target==0 than target==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "import org.apache.spark.mllib.regression.LabeledPoint\n",
       "import org.apache.spark.mllib.util.MLUtils\n",
       "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_657f6e899512\n",
       "labelIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_5498f6595964\n",
       "model: org.apache.spark.ml.classification.LogisticRegression = logreg_87a1a325970d\n",
       "lrPipeline: org.apache.spark.ml.Pipeline = pipeline_f8a05caf6f32\n",
       "lrModel: org.apache.spark.ml.PipelineModel = pipeline_f8a05caf6f32\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(features).setOutputCol(\"features\")\n",
    "val labelIndexer = new StringIndexer().setInputCol(\"TARGET\").setOutputCol(\"label\")\n",
    "val model = new LogisticRegression()\n",
    "val lrPipeline = new Pipeline().setStages(Array(labelIndexer, assembler, model))\n",
    "\n",
    "val lrModel = lrPipeline.fit(df13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions3: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 191 more fields]\n"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions3 = lrModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area under the receiver operating characteristic (ROC) curve : 0.7051602943316483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictionLabelsRDD2: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[7917] at map at <console>:81\n",
       "binMetrics2: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@6911c705\n"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionLabelsRDD2 = predictions3.select(\"prediction\", \"label\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics2 = new BinaryClassificationMetrics(predictionLabelsRDD2)\n",
    "println(\"area under the receiver operating characteristic (ROC) curve : \" + binMetrics1.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lp: org.apache.spark.sql.DataFrame = [label: double, prediction: double]\n",
       "counttotal: Long = 36778\n",
       "correct: Long = 25653\n",
       "wrong: Long = 11125\n",
       "truedefault: Long = 34054\n",
       "truep: Long = 1882\n",
       "truen: Long = 23771\n",
       "falseN: Long = 10283\n",
       "falseP: Long = 842\n",
       "ratioWrong: Double = 0.3024906193920278\n",
       "ratioCorrect: Double = 0.6975093806079722\n",
       "ratiotrueN: Double = 0.6980384095847771\n"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lp = predictions3.select( \"label\", \"prediction\")\n",
    "val counttotal = predictions3.count()\n",
    "val correct = lp.filter($\"label\" === $\"prediction\").count() //correct prediction\n",
    "val wrong = lp.filter(not($\"label\" === $\"prediction\")).count() //wrong prediction\n",
    "val truedefault=lp.filter($\"label\"===1.0).count// Number of target==1\n",
    "val truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" === $\"prediction\").count() //Correct prediction of label=0\n",
    "val truen = lp.filter($\"prediction\" === 1.0).filter($\"label\" === $\"prediction\").count() //Correct prediction of label=1\n",
    "val falseN = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" === $\"prediction\")).count() //Wrong prediction when label=1\n",
    "val falseP = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" === $\"prediction\")).count() //Wrong prediction when lable =0\n",
    "val ratioWrong=wrong.toDouble/counttotal.toDouble //Wrong prediction ratio\n",
    "val ratioCorrect=correct.toDouble/counttotal.toDouble //Correct predicition ratio\n",
    "val ratiotrueN=truen.toDouble/truedefault.toDouble //Correct prediction ratio for target ==1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model can predict 69% of the default. and 69% of the non-default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.6932293117860128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_35eb89cda82e\n",
       "accuracy: Double = 0.6932293117860128\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//test of total accuracy\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions3)\n",
    "println(\"Test set accuracy = \" + accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model with df13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area under the receiver operating characteristic (ROC) curve : 0.6936940928235291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictionLabelsRDD1: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[8111] at map at <console>:80\n",
       "binMetrics1: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@559ae297\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionLabelsRDD1 = predictions2.select(\"prediction\", \"label\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics1 = new BinaryClassificationMetrics(predictionLabelsRDD1)\n",
    "println(\"area under the receiver operating characteristic (ROC) curve : \" + binMetrics1.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lp: org.apache.spark.sql.DataFrame = [label: double, prediction: double]\n",
       "counttotal: Long = 26446\n",
       "correct: Long = 18345\n",
       "wrong: Long = 8101\n",
       "truep: Long = 9202\n",
       "truen: Long = 9143\n",
       "falseN: Long = 3991\n",
       "falseP: Long = 4110\n",
       "ratioWrong: Double = 0.30632231717462\n",
       "ratioCorrect: Double = 0.69367768282538\n"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lp = predictions2.select( \"label\", \"prediction\")\n",
    "val counttotal = predictions2.count()\n",
    "val correct = lp.filter($\"label\" === $\"prediction\").count() //correct prediction\n",
    "val wrong = lp.filter(not($\"label\" === $\"prediction\")).count() //wrong prediction\n",
    "val truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" === $\"prediction\").count() //Correct prediction of label=0\n",
    "val truen = lp.filter($\"prediction\" === 1.0).filter($\"label\" === $\"prediction\").count() //Correct prediction of label=1\n",
    "val falseN = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" === $\"prediction\")).count() //Wrong prediction when label=1\n",
    "val falseP = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" === $\"prediction\")).count() //Wrong prediction when lable =0\n",
    "val ratioWrong=wrong.toDouble/counttotal.toDouble //Wrong prediction ratio\n",
    "val ratioCorrect=correct.toDouble/counttotal.toDouble //Correct predicition ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+--------------------+----------+\n",
      "|SK_ID_CURR|            features|label|         probability|prediction|\n",
      "+----------+--------------------+-----+--------------------+----------+\n",
      "|  100049.0|(186,[0,1,2,3,4,5...|  0.0|[0.41297183801247...|       1.0|\n",
      "|  100902.0|(186,[0,1,2,3,4,5...|  0.0|[0.40944020354736...|       1.0|\n",
      "|  101042.0|(186,[0,1,2,3,4,5...|  0.0|[0.38802373681337...|       1.0|\n",
      "|  101197.0|(186,[0,1,2,3,4,5...|  0.0|[0.63103137138405...|       0.0|\n",
      "|  101277.0|(186,[0,1,2,3,4,5...|  0.0|[0.91460324467911...|       0.0|\n",
      "|  101286.0|(186,[0,1,2,3,4,5...|  0.0|[0.60208368726355...|       0.0|\n",
      "|  101495.0|(186,[0,1,2,3,4,5...|  0.0|[0.51719934824546...|       0.0|\n",
      "|  101859.0|(186,[0,1,2,3,4,5...|  0.0|[0.63584047678167...|       0.0|\n",
      "|  101980.0|(186,[0,1,2,3,4,5...|  0.0|[0.72854178873096...|       0.0|\n",
      "|  102003.0|(186,[0,1,2,3,4,5...|  0.0|[0.78686212416355...|       0.0|\n",
      "|  102025.0|(186,[0,1,2,3,4,5...|  0.0|[0.67750359233151...|       0.0|\n",
      "|  102521.0|(186,[0,1,2,3,4,5...|  0.0|[0.91508860492092...|       0.0|\n",
      "|  102574.0|(186,[0,1,2,3,4,5...|  0.0|[0.54665255658381...|       0.0|\n",
      "|  102586.0|(186,[0,1,2,3,4,5...|  0.0|[0.83555990512389...|       0.0|\n",
      "|  102679.0|(186,[0,1,2,3,4,5...|  0.0|[0.48316722722034...|       1.0|\n",
      "|  102875.0|(186,[0,1,2,3,4,5...|  0.0|[0.77534384471543...|       0.0|\n",
      "|  102955.0|(186,[0,1,2,3,4,5...|  0.0|[0.60410124247854...|       0.0|\n",
      "|  103134.0|(186,[0,1,2,3,4,5...|  0.0|[0.66772921874347...|       0.0|\n",
      "|  103310.0|(186,[0,1,2,3,4,5...|  0.0|[0.78393980493061...|       0.0|\n",
      "|  103325.0|(186,[0,1,2,3,4,5...|  0.0|[0.61388217224368...|       0.0|\n",
      "+----------+--------------------+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions2.select (\"SK_ID_CURR\",\"features\", \"label\",\"probability\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.69367768282538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_88fbc19109f8\n",
       "accuracy: Double = 0.69367768282538\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions2)\n",
    "println(\"Test set accuracy = \" + accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
