{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 7\n",
    "# Big Data Group Project\n",
    "\n",
    "## Contributors:\n",
    "- 29233798: Joel Shien Yee Chin [Joel]\n",
    "- 18435688: Tim O'Doherty [Tim]\n",
    "- 22606127: Sean Whitehead [Sean]\n",
    "- 29650437: Lin Bai [Lin]\n",
    "\n",
    "\n",
    "## Phase 3: Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://b335dbce4d27:4040\n",
       "SparkContext available as 'sc' (version = 2.4.3, master = local[*], app id = local-1560072779294)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-09 09:32:55,706 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 283 more fields]\n",
       "creditRiskdf2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 283 more fields]\n",
       "creditRiskdf3: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 283 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load wrangled datasets from Phase 2: part1.csv; part2.csv; part3.csv\n",
    "\n",
    "val creditRiskdf1 = spark.read.option(\"header\",\"true\").csv(\"part1.csv\")\n",
    "val creditRiskdf2 = spark.read.option(\"header\",\"true\").csv(\"part2.csv\")\n",
    "val creditRiskdf3 = spark.read.option(\"header\",\"true\").csv(\"part3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1and2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 283 more fields]\n",
       "creditRiskdf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 283 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// combine the 3 dataframes to 1\n",
    "val creditRiskdf1and2 = creditRiskdf1.union(creditRiskdf2)\n",
    "val creditRiskdf = creditRiskdf1and2.union(creditRiskdf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: creditRiskdf.type = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 283 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditRiskdf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-09 09:33:50,305 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "Total data rows in loaded dataframes:183875\n",
      "Total data rows in combined dataframe:183875\n"
     ]
    }
   ],
   "source": [
    "// check that the rows have been aggregated correctly\n",
    "\n",
    "println(\"Total data rows in loaded dataframes:\"+(\n",
    "    creditRiskdf1.count()+creditRiskdf2.count()+creditRiskdf3.count()))\n",
    "\n",
    "println(\"Total data rows in combined dataframe:\"+creditRiskdf.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**\n",
    "- creditRiskdf: loaded and consolidated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Data cleansing and reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "incon_droplist: List[String] = List(previous_loans_DAYS_FIRST_DUE_mean, previous_loans_SELLERPLACE_AREA_min, previous_loans_DAYS_FIRST_DUE_sum)\n",
       "df: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 280 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Drop features with inconsistent values from dataset.\n",
    "\n",
    "val incon_droplist = List(\"previous_loans_DAYS_FIRST_DUE_mean\",\n",
    "                      \"previous_loans_SELLERPLACE_AREA_min\",\n",
    "                      \"previous_loans_DAYS_FIRST_DUE_sum\")\n",
    "\n",
    "val df = creditRiskdf.drop(incon_droplist:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 280 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var creditRiskdf = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskFeatures: Array[String] = Array(EXT_SOURCE_2, EXT_SOURCE_3, client_installments_AMT_PAYMENT_min_sum, DAYS_BIRTH, AMT_CREDIT, AMT_ANNUITY, DAYS_EMPLOYED, bureau_DAYS_CREDIT_ENDDATE_max, bureau_DAYS_CREDIT_max, DAYS_ID_PUBLISH, bureau_DAYS_ENDDATE_FACT_max, bureau_AMT_CREDIT_SUM_DEBT_mean, previous_loans_CNT_PAYMENT_mean, client_cash_CNT_INSTALMENT_FUTURE_min_max, previous_loans_SELLERPLACE_AREA_max, DAYS_LAST_PHONE_CHANGE, bureau_AMT_CREDIT_SUM_max, bureau_AMT_CREDIT_SUM_mean, DAYS_REGISTRATION, client_installments_DAYS_INSTALMENT_max_max, previous_loans_AMT_DOWN_PAYMENT_max, CODE_GENDER_F, REGION_POPULATION_RELATIVE, client_cash_CNT_INSTALMENT_max_mean, bureau_DAYS_CREDIT_UPDATE_max, bureau_AMT_CREDIT_SUM_sum, client_installments_NUM_INSTALMENT_VERSION_mean_max, bureau_DAYS_CR..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// change datatype of data from String to Float\n",
    "var creditRiskFeatures = creditRiskdf.columns\n",
    "\n",
    "for (colName<-creditRiskFeatures){\n",
    "     |   creditRiskdf = creditRiskdf.withColumn(\n",
    "         colName,col(colName).cast(\"Float\"))\n",
    "     | }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EXT_SOURCE_2: float (nullable = true)\n",
      " |-- EXT_SOURCE_3: float (nullable = true)\n",
      " |-- client_installments_AMT_PAYMENT_min_sum: float (nullable = true)\n",
      " |-- DAYS_BIRTH: float (nullable = true)\n",
      " |-- AMT_CREDIT: float (nullable = true)\n",
      " |-- AMT_ANNUITY: float (nullable = true)\n",
      " |-- DAYS_EMPLOYED: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_ENDDATE_max: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_max: float (nullable = true)\n",
      " |-- DAYS_ID_PUBLISH: float (nullable = true)\n",
      " |-- bureau_DAYS_ENDDATE_FACT_max: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_DEBT_mean: float (nullable = true)\n",
      " |-- previous_loans_CNT_PAYMENT_mean: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_FUTURE_min_max: float (nullable = true)\n",
      " |-- previous_loans_SELLERPLACE_AREA_max: float (nullable = true)\n",
      " |-- DAYS_LAST_PHONE_CHANGE: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_max: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_mean: float (nullable = true)\n",
      " |-- DAYS_REGISTRATION: float (nullable = true)\n",
      " |-- client_installments_DAYS_INSTALMENT_max_max: float (nullable = true)\n",
      " |-- previous_loans_AMT_DOWN_PAYMENT_max: float (nullable = true)\n",
      " |-- CODE_GENDER_F: float (nullable = true)\n",
      " |-- REGION_POPULATION_RELATIVE: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_max_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_UPDATE_max: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_sum: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_mean_max: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_mean: float (nullable = true)\n",
      " |-- client_installments_AMT_PAYMENT_sum_mean: float (nullable = true)\n",
      " |-- client_installments_DAYS_INSTALMENT_min_max: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Refused_count_norm: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_max_max: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_FUTURE_min_mean: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_max_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_high_count_norm: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_sum: float (nullable = true)\n",
      " |-- client_installments_DAYS_ENTRY_PAYMENT_sum_max: float (nullable = true)\n",
      " |-- previous_loans_DAYS_FIRST_DUE_min: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_DEF_sum_max: float (nullable = true)\n",
      " |-- previous_loans_DAYS_DECISION_max: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_min_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_LIMIT_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_sum: float (nullable = true)\n",
      " |-- previous_loans_DAYS_LAST_DUE_1ST_VERSION_max: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash X-Sell: low_count_norm: float (nullable = true)\n",
      " |-- previous_loans_HOUR_APPR_PROCESS_START_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_DEBT_max: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_min: float (nullable = true)\n",
      " |-- previous_loans_AMT_DOWN_PAYMENT_mean: float (nullable = true)\n",
      " |-- NAME_FAMILY_STATUS_Married: float (nullable = true)\n",
      " |-- previous_loans_CNT_PAYMENT_max: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_mean_min: float (nullable = true)\n",
      " |-- AMT_INCOME_TOTAL: float (nullable = true)\n",
      " |-- previous_loans_DAYS_LAST_DUE_sum: float (nullable = true)\n",
      " |-- client_cash_MONTHS_BALANCE_max_mean: float (nullable = true)\n",
      " |-- NAME_EDUCATION_TYPE_Higher education: float (nullable = true)\n",
      " |-- client_cash_MONTHS_BALANCE_sum_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_ENDDATE_mean: float (nullable = true)\n",
      " |-- REGION_RATING_CLIENT: float (nullable = true)\n",
      " |-- previous_loans_AMT_GOODS_PRICE_min: float (nullable = true)\n",
      " |-- bureau_DAYS_ENDDATE_FACT_min: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_ENDDATE_min: float (nullable = true)\n",
      " |-- previous_loans_AMT_ANNUITY_mean: float (nullable = true)\n",
      " |-- client_installments_AMT_PAYMENT_max_min: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_min_max: float (nullable = true)\n",
      " |-- client_installments_AMT_PAYMENT_sum_min: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Mortgage_count_norm: float (nullable = true)\n",
      " |-- previous_loans_AMT_ANNUITY_max: float (nullable = true)\n",
      " |-- previous_loans_AMT_ANNUITY_min: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_max: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_low_normal_count_norm: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash X-Sell: high_count_norm: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_ENDDATE_sum: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Active_count: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_low_action_count_norm: float (nullable = true)\n",
      " |-- client_cash_counts_mean: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_mean_max: float (nullable = true)\n",
      " |-- client_cash_MONTHS_BALANCE_sum_max: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_sum_min: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Active_count_norm: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_UPDATE_min: float (nullable = true)\n",
      " |-- DEF_60_CNT_SOCIAL_CIRCLE: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_3: float (nullable = true)\n",
      " |-- bureau_DAYS_ENDDATE_FACT_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_LIMIT_max: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_max_sum: float (nullable = true)\n",
      " |-- previous_loans_AMT_GOODS_PRICE_mean: float (nullable = true)\n",
      " |-- client_installments_AMT_PAYMENT_max_sum: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_sum_mean: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_min_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_TYPE_SUITE_Unaccompanied_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_norm_mean: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_NUMBER_max_mean: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_max_max: float (nullable = true)\n",
      " |-- client_installments_DAYS_ENTRY_PAYMENT_max_min: float (nullable = true)\n",
      " |-- client_installments_AMT_PAYMENT_max_mean: float (nullable = true)\n",
      " |-- client_cash_counts_sum: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Approved_count_norm: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_min_mean: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_mean_min: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_sum_max: float (nullable = true)\n",
      " |-- previous_loans_AMT_ANNUITY_sum: float (nullable = true)\n",
      " |-- client_installments_AMT_PAYMENT_mean_sum: float (nullable = true)\n",
      " |-- previous_loans_DAYS_DECISION_mean: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash Street: low_count_norm: float (nullable = true)\n",
      " |-- previous_loans_DAYS_LAST_DUE_1ST_VERSION_mean: float (nullable = true)\n",
      " |-- HOUR_APPR_PROCESS_START: float (nullable = true)\n",
      " |-- previous_loans_DAYS_LAST_DUE_1ST_VERSION_min: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Consumer credit_count_norm: float (nullable = true)\n",
      " |-- previous_loans_HOUR_APPR_PROCESS_START_max: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_HC_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_PRODUCT_TYPE_walk-in_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CNT_PAYMENT_sum: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_UPDATE_mean: float (nullable = true)\n",
      " |-- previous_loans_DAYS_DECISION_sum: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS industry with interest_count_norm: float (nullable = true)\n",
      " |-- client_cash_MONTHS_BALANCE_max_sum: float (nullable = true)\n",
      " |-- previous_loans_AMT_CREDIT_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_CLIENT_TYPE_Refreshed_count_norm: float (nullable = true)\n",
      " |-- client_bureau_balance_MONTHS_BALANCE_mean_sum: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash Street: high_count_norm: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_DEBT_min: float (nullable = true)\n",
      " |-- NAME_CONTRACT_TYPE_Cash loans: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_min_sum: float (nullable = true)\n",
      " |-- previous_loans_AMT_APPLICATION_max: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_sum_max: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_min: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_mean_mean: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_sum_sum: float (nullable = true)\n",
      " |-- previous_loans_NAME_TYPE_SUITE_Family_count_norm: float (nullable = true)\n",
      " |-- previous_loans_DAYS_TERMINATION_mean: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_FUTURE_max_sum: float (nullable = true)\n",
      " |-- previous_loans_NAME_PAYMENT_TYPE_XNA_count_norm: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Credit card_count_norm: float (nullable = true)\n",
      " |-- client_installments_DAYS_ENTRY_PAYMENT_min_sum: float (nullable = true)\n",
      " |-- OBS_30_CNT_SOCIAL_CIRCLE: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_sum_sum: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_mean_sum: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_18: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_FUTURE_min_sum: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Credit and cash offices_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Contact center_count_norm: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Car loan_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_PORTFOLIO_POS_count_norm: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_NUMBER_min_mean: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_TUESDAY_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Furniture_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Completed_count_norm_max: float (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_QRT: float (nullable = true)\n",
      " |-- NAME_INCOME_TYPE_Working: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_norm_min: float (nullable = true)\n",
      " |-- previous_loans_AMT_DOWN_PAYMENT_min: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Microloan_count_norm: float (nullable = true)\n",
      " |-- previous_loans_DAYS_TERMINATION_min: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Channel of corporate sales_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_SUNDAY_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Stone_count_norm: float (nullable = true)\n",
      " |-- NAME_EDUCATION_TYPE_Secondary / secondary special: float (nullable = true)\n",
      " |-- client_bureau_balance_STATUS_0_count_sum: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_Core staff: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_middle_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Completed_count_norm_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_OVERDUE_max: float (nullable = true)\n",
      " |-- REG_CITY_NOT_LIVE_CITY: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Business Entity Type 3: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Closed_count: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Self-employed: float (nullable = true)\n",
      " |-- previous_loans_NAME_PORTFOLIO_Cash_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_WEDNESDAY_count_norm: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_mean_min: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Completed_count_sum: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_SCO_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Computers_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_XNA_count_norm: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS household without interest_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_PRODUCT_TYPE_XNA_count_norm: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_LIMIT_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_PRODUCT_TYPE_x-sell_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_MONDAY_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_SATURDAY_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_CLIENT_TYPE_New_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_AP+ (Cash loan)_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NFLAG_INSURED_ON_APPROVAL_mean: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Audio/Video_count_norm: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_mean_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Clothing and Accessories_count_norm: float (nullable = true)\n",
      " |-- DEF_30_CNT_SOCIAL_CIRCLE: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_NUMBER_mean_min: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_NUMBER_sum_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_PORTFOLIO_XNA_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_CLIENT_TYPE_Repeater_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_SELLER_INDUSTRY_Connectivity_count_norm: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Credit card_count: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Canceled_count_norm: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_Drivers: float (nullable = true)\n",
      " |-- previous_loans_HOUR_APPR_PROCESS_START_min: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Signed_count_norm_max: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_Laborers: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_min_max: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_THURSDAY_count_norm: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Construction: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_max: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_DEF_sum_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_TYPE_Cash loans_count_norm: float (nullable = true)\n",
      " |-- previous_loans_DAYS_FIRST_DRAWING_mean: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash X-Sell: middle_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_CASH_LOAN_PURPOSE_Other_count_norm: float (nullable = true)\n",
      " |-- client_bureau_balance_STATUS_0_count_norm_sum: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_sum_min: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Country-wide_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Signed_count_sum: float (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_YEAR: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Microloan_count: float (nullable = true)\n",
      " |-- NAME_INCOME_TYPE_State servant: float (nullable = true)\n",
      " |-- previous_loans_DAYS_TERMINATION_max: float (nullable = true)\n",
      " |-- FLAG_WORK_PHONE: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Photo / Cinema Equipment_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_FRIDAY_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_THURSDAY_count: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Military: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS household with interest_count_norm: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_min_sum: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Completed_count_norm_sum: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_16: float (nullable = true)\n",
      " |-- previous_loans_NAME_TYPE_SUITE_Spouse, partner_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Returned to the store_count_sum: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Industry: type 9: float (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_MON: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Approved_count: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Sold_count_norm: float (nullable = true)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |-- previous_loans_NAME_GOODS_CATEGORY_Mobile_count_norm: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_OVERDUE_mean: float (nullable = true)\n",
      " |-- previous_loans_NAME_SELLER_INDUSTRY_Construction_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_SCOFR_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_SELLER_INDUSTRY_Furniture_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_norm_max: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Construction Materials_count_norm: float (nullable = true)\n",
      " |-- client_bureau_balance_STATUS_1_count_norm_sum: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash Street: middle_count_norm: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS other with interest_count_norm: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_min_min: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_VERIF_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_LIMIT_count_norm: float (nullable = true)\n",
      " |-- client_bureau_balance_STATUS_X_count_norm_sum: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Consumer Electronics_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Regional / Local_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_SUNDAY_count: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_13: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_CLIENT_count_norm: float (nullable = true)\n",
      " |-- client_bureau_balance_STATUS_X_count_sum: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_middle_count: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_TYPE_Revolving loans_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_PRODUCT_TYPE_XNA_count: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Jewelry_count_norm: float (nullable = true)\n",
      " |-- WALLSMATERIAL_MODE_Stone, brick: float (nullable = true)\n",
      " |-- previous_loans_NAME_SELLER_INDUSTRY_Consumer electronics_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CNT_PAYMENT_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_high_count: float (nullable = true)\n",
      " |-- NAME_FAMILY_STATUS_Separated: float (nullable = true)\n",
      " |-- NAME_HOUSING_TYPE_House / apartment: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Signed_count_norm_mean: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Refused_count: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Card Street_count_norm: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_NUMBER_max_min: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS industry without interest_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Regional / Local_count: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Transport: type 3: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Returned to the store_count_norm_max: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_School: float (nullable = true)\n",
      " |-- previous_loans_NAME_TYPE_SUITE_Other_B_count_norm: float (nullable = true)\n",
      " |-- bureau_CNT_CREDIT_PROLONG_mean: float (nullable = true)\n",
      " |-- client_bureau_balance_MONTHS_BALANCE_max_sum: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_min: float (nullable = true)\n",
      " |-- WEEKDAY_APPR_PROCESS_START_WEDNESDAY: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_High skill tech staff: float (nullable = true)\n",
      " |-- previous_loans_NAME_PAYMENT_TYPE_XNA_count: float (nullable = true)\n",
      " |-- FLAG_PHONE: float (nullable = true)\n",
      " |-- TARGET: float (nullable = true)\n",
      " |-- SK_ID_CURR: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "creditRiskdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskFeatures: Array[String] = [Ljava.lang.String;@5a2fab75\n",
       "res4: Int = 281\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//string array of features without label \"TARGET\"\n",
    "\n",
    "creditRiskFeatures = creditRiskFeatures.filter(! _.contains(\"TARGET\"))\n",
    "creditRiskFeatures.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskFeatures1: Array[String] = Array(EXT_SOURCE_2, EXT_SOURCE_3, client_installments_AMT_PAYMENT_min_sum, DAYS_BIRTH, AMT_CREDIT, AMT_ANNUITY, DAYS_EMPLOYED, bureau_DAYS_CREDIT_ENDDATE_max, bureau_DAYS_CREDIT_max, DAYS_ID_PUBLISH, bureau_DAYS_ENDDATE_FACT_max, bureau_AMT_CREDIT_SUM_DEBT_mean, previous_loans_CNT_PAYMENT_mean, client_cash_CNT_INSTALMENT_FUTURE_min_max, previous_loans_SELLERPLACE_AREA_max, DAYS_LAST_PHONE_CHANGE, bureau_AMT_CREDIT_SUM_max, bureau_AMT_CREDIT_SUM_mean, DAYS_REGISTRATION, client_installments_DAYS_INSTALMENT_max_max, previous_loans_AMT_DOWN_PAYMENT_max, CODE_GENDER_F, REGION_POPULATION_RELATIVE, client_cash_CNT_INSTALMENT_max_mean, bureau_DAYS_CREDIT_UPDATE_max, bureau_AMT_CREDIT_SUM_sum, client_installments_NUM_INSTALMENT_VERSION_mean_max, bureau_DAYS_C..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// split features array to facilitate processing. Exclude feature SK_ID.\n",
    "\n",
    "val creditRiskFeatures1 = creditRiskFeatures.slice(0,39)\n",
    "val creditRiskFeatures2 = creditRiskFeatures.slice(40,79)\n",
    "val creditRiskFeatures3 = creditRiskFeatures.slice(80,119)\n",
    "val creditRiskFeatures4 = creditRiskFeatures.slice(120,159)\n",
    "val creditRiskFeatures5 = creditRiskFeatures.slice(160,199)\n",
    "val creditRiskFeatures6 = creditRiskFeatures.slice(200,239)\n",
    "val creditRiskFeatures7 = creditRiskFeatures.slice(240,280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rows: Long = 183875\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// check number of data lines in dataframe creditRiskdf\n",
    "\n",
    "val rows = creditRiskdf.select($\"TARGET\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 280 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// set up global variables\n",
    "\n",
    "var creditRiskdf1 = creditRiskdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run above cells up to this block if directly loading highOutlier and lowOutlier lists from text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "highOutlier_list: List[String] = List()\n",
       "lowOutlier_list: List[String] = List()\n",
       "feature_dropThreshold: Double = 9193.75\n",
       "row_dropLower: Double = 3677.5\n",
       "outliers: (feature_set: Array[String], creditRiskdf: org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//write function to check if there are any outliers in a given domain and show boxplot [Sean]\n",
    "//slice a small test list to ensure functionality\n",
    "//var test_lis = creditRiskFeatures.slice(0,2) //TEST var for codeblock\n",
    "\n",
    "var highOutlier_list = List[String]() //empty list to append features containing outliers in\n",
    "var lowOutlier_list = List[String]() //empty list to append features containing outliers in\n",
    "var feature_dropThreshold = 0.05*rows\n",
    "var row_dropLower = 0.02*rows\n",
    "\n",
    "def outliers(feature_set: Array[String],creditRiskdf:org.apache.spark.sql.DataFrame){    \n",
    "    for (feature <- feature_set) {\n",
    "        //summarise column and query the DataFrame output\n",
    "        var firstQ = creditRiskdf.select(feature).summary().where(\n",
    "            $\"summary\" === \"25%\").select(feature).first().mkString.toFloat\n",
    "        var thirdQ = creditRiskdf.select(feature).summary().where(\n",
    "            $\"summary\" === \"75%\").select(feature).first().mkString.toFloat\n",
    "        //use formulas below to test threshold of outliers\n",
    "        var testValHigh = thirdQ + (1.5 * (thirdQ - firstQ))\n",
    "        var testValLow = firstQ - (1.5 * (thirdQ - firstQ))\n",
    "        if(testValHigh == 0 & testValLow == 0){}\n",
    "        //check to see if thresholds are exceeded in the column and count\n",
    "        else{\n",
    "            var outHigh = creditRiskdf.select(col(feature)).filter(\n",
    "                col(feature) > lit(testValHigh)).count()\n",
    "            var outLow = creditRiskdf.select(col(feature)).filter(\n",
    "                col(feature) < lit(testValLow)).count()\n",
    "            // collate a list of features with outliers > 5% of total data rows\n",
    "            if (outHigh + outLow >= feature_dropThreshold ){       \n",
    "                highOutlier_list = feature :: highOutlier_list\n",
    "            }\n",
    "            // collate a list of features with outliers 2%-5% of total data rows    \n",
    "            else if (outHigh + outLow > 0.02*rows & outHigh + outLow < 0.05*rows){\n",
    "                lowOutlier_list = feature :: lowOutlier_list\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colNames: Array[org.apache.spark.sql.Column] = Array(EXT_SOURCE_2, EXT_SOURCE_3, client_installments_AMT_PAYMENT_min_sum, DAYS_BIRTH, AMT_CREDIT, AMT_ANNUITY, DAYS_EMPLOYED, bureau_DAYS_CREDIT_ENDDATE_max, bureau_DAYS_CREDIT_max, DAYS_ID_PUBLISH, bureau_DAYS_ENDDATE_FACT_max, bureau_AMT_CREDIT_SUM_DEBT_mean, previous_loans_CNT_PAYMENT_mean, client_cash_CNT_INSTALMENT_FUTURE_min_max, previous_loans_SELLERPLACE_AREA_max, DAYS_LAST_PHONE_CHANGE, bureau_AMT_CREDIT_SUM_max, bureau_AMT_CREDIT_SUM_mean, DAYS_REGISTRATION, client_installments_DAYS_INSTALMENT_max_max, previous_loans_AMT_DOWN_PAYMENT_max, CODE_GENDER_F, REGION_POPULATION_RELATIVE, client_cash_CNT_INSTALMENT_max_mean, bureau_DAYS_CREDIT_UPDATE_max, bureau_AMT_CREDIT_SUM_sum, client_installments_NUM_INSTALMENT_VERSION_mean_max, bur..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// processing outliers list 1 of 7\n",
    "\n",
    "val colNames = creditRiskFeatures1.map(col(_))\n",
    "var creditRiskdf1_1 = creditRiskdf1.select(colNames:_*)\n",
    "creditRiskdf1_1.cache()\n",
    "outliers(creditRiskFeatures1, creditRiskdf1_1)\n",
    "println(highOutlier_list.length)\n",
    "println(lowOutlier_list.length)\n",
    "\n",
    "creditRiskdf1_1.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colNames: Array[org.apache.spark.sql.Column] = Array(previous_loans_DAYS_DECISION_max, client_installments_AMT_INSTALMENT_min_mean, bureau_AMT_CREDIT_SUM_LIMIT_mean, bureau_DAYS_CREDIT_sum, previous_loans_DAYS_LAST_DUE_1ST_VERSION_max, previous_loans_PRODUCT_COMBINATION_Cash X-Sell: low_count_norm, previous_loans_HOUR_APPR_PROCESS_START_mean, bureau_AMT_CREDIT_SUM_DEBT_max, bureau_AMT_CREDIT_SUM_min, previous_loans_AMT_DOWN_PAYMENT_mean, NAME_FAMILY_STATUS_Married, previous_loans_CNT_PAYMENT_max, client_installments_AMT_INSTALMENT_mean_min, AMT_INCOME_TOTAL, previous_loans_DAYS_LAST_DUE_sum, client_cash_MONTHS_BALANCE_max_mean, NAME_EDUCATION_TYPE_Higher education, client_cash_MONTHS_BALANCE_sum_mean, bureau_DAYS_CREDIT_ENDDATE_mean, REGION_RATING_CLIENT, previous_loans_AMT_GOODS_PRICE_..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// processing outliers list 2 of 7\n",
    "\n",
    "val colNames = creditRiskFeatures2.map(col(_))\n",
    "var creditRiskdf1_2 = creditRiskdf1.select(colNames:_*)\n",
    "creditRiskdf1_2.cache()\n",
    "outliers(creditRiskFeatures2, creditRiskdf1_2)\n",
    "println(highOutlier_list.length)\n",
    "println(lowOutlier_list.length)\n",
    "creditRiskdf1_2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colNames: Array[org.apache.spark.sql.Column] = Array(bureau_CREDIT_ACTIVE_Active_count_norm, bureau_DAYS_CREDIT_UPDATE_min, DEF_60_CNT_SOCIAL_CIRCLE, FLAG_DOCUMENT_3, bureau_DAYS_ENDDATE_FACT_mean, bureau_AMT_CREDIT_SUM_LIMIT_max, client_cash_SK_DPD_max_sum, previous_loans_AMT_GOODS_PRICE_mean, client_installments_AMT_PAYMENT_max_sum, client_installments_NUM_INSTALMENT_VERSION_sum_mean, client_installments_AMT_INSTALMENT_min_min, previous_loans_NAME_TYPE_SUITE_Unaccompanied_count_norm, client_cash_NAME_CONTRACT_STATUS_Active_count_norm_mean, client_installments_NUM_INSTALMENT_NUMBER_max_mean, client_cash_CNT_INSTALMENT_max_max, client_installments_DAYS_ENTRY_PAYMENT_max_min, client_installments_AMT_PAYMENT_max_mean, client_cash_counts_sum, previous_loans_NAME_CONTRACT_STATUS_Approved_co..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// processing outliers list 3 of 7\n",
    "\n",
    "val colNames = creditRiskFeatures3.map(col(_))\n",
    "var creditRiskdf1_3 = creditRiskdf1.select(colNames:_*)\n",
    "creditRiskdf1_3.cache()\n",
    "outliers(creditRiskFeatures3, creditRiskdf1_3)\n",
    "println(highOutlier_list.length)\n",
    "println(lowOutlier_list.length)\n",
    "creditRiskdf1_3.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.io._\n",
       "file1: String = highOutlier_list.txt\n",
       "file2: String = lowOutlier_list.txt\n",
       "writer1: java.io.BufferedWriter = java.io.BufferedWriter@1db7651f\n",
       "writer2: java.io.BufferedWriter = java.io.BufferedWriter@5d2426f6\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// persist storage of intermediate lists\n",
    "\n",
    "import java.io._\n",
    "\n",
    "val file1 = \"highOutlier_list.txt\"\n",
    "val file2 = \"lowOutlier_list.txt\"\n",
    "val writer1 = new BufferedWriter(new FileWriter(file1))\n",
    "highOutlier_list.map(_+\"\\n\").foreach(writer1.write)\n",
    "writer1.close()\n",
    "val writer2 = new BufferedWriter(new FileWriter(file2))\n",
    "lowOutlier_list.map(_+\"\\n\").foreach(writer2.write)\n",
    "writer2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.io.Source\n",
       "highOutlier_list: List[String] = List(previous_loans_AMT_CREDIT_min, previous_loans_CNT_PAYMENT_sum, previous_loans_DAYS_LAST_DUE_1ST_VERSION_mean, client_installments_AMT_PAYMENT_mean_sum, previous_loans_AMT_ANNUITY_sum, client_installments_AMT_INSTALMENT_sum_max, client_cash_CNT_INSTALMENT_mean_min, client_cash_counts_sum, client_installments_AMT_PAYMENT_max_mean, client_cash_CNT_INSTALMENT_max_max, client_installments_NUM_INSTALMENT_NUMBER_max_mean, client_installments_AMT_INSTALMENT_min_min, client_installments_NUM_INSTALMENT_VERSION_sum_mean, client_installments_AMT_PAYMENT_max_sum, previous_loans_AMT_GOODS_PRICE_mean, client_cash_MONTHS_BALANCE_sum_max, client_installments_AMT_INSTALMENT_mean_max, bureau_DAYS_CREDIT_ENDDATE_sum, previous_loans_NAME_YIELD_GRO..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// load intermediate lists\n",
    "import scala.io.Source\n",
    "\n",
    "var highOutlier_list = Source.fromFile(\"highOutlier_list.txt\").getLines.toList\n",
    "var lowOutlier_list = Source.fromFile(\"lowOutlier_list.txt\").getLines.toList\n",
    "var feature_dropThreshold = 0.05*rows\n",
    "var row_dropLower = 0.02*rows\n",
    "\n",
    "def outliers(feature_set: Array[String],creditRiskdf:org.apache.spark.sql.DataFrame){    \n",
    "    for (feature <- feature_set) {\n",
    "        //summarise column and query the DataFrame output\n",
    "        var firstQ = creditRiskdf.select(feature).summary().where(\n",
    "            $\"summary\" === \"25%\").select(feature).first().mkString.toFloat\n",
    "        var thirdQ = creditRiskdf.select(feature).summary().where(\n",
    "            $\"summary\" === \"75%\").select(feature).first().mkString.toFloat\n",
    "        //use formulas below to test threshold of outliers\n",
    "        var testValHigh = thirdQ + (1.5 * (thirdQ - firstQ))\n",
    "        var testValLow = firstQ - (1.5 * (thirdQ - firstQ))\n",
    "        if(testValHigh == 0 & testValLow == 0){}\n",
    "        //check to see if thresholds are exceeded in the column and count\n",
    "        else{\n",
    "            var outHigh = creditRiskdf.select(col(feature)).filter(\n",
    "                col(feature) > lit(testValHigh)).count()\n",
    "            var outLow = creditRiskdf.select(col(feature)).filter(\n",
    "                col(feature) < lit(testValLow)).count()\n",
    "            // collate a list of features with outliers > 5% of total data rows\n",
    "            if (outHigh + outLow >= feature_dropThreshold ){       \n",
    "                highOutlier_list = feature :: highOutlier_list\n",
    "            }\n",
    "            // collate a list of features with outliers 2%-5% of total data rows    \n",
    "            else if (outHigh + outLow > 0.02*rows & outHigh + outLow < 0.05*rows){\n",
    "                lowOutlier_list = feature :: lowOutlier_list\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colNames: Array[org.apache.spark.sql.Column] = Array(client_bureau_balance_MONTHS_BALANCE_mean_sum, previous_loans_PRODUCT_COMBINATION_Cash Street: high_count_norm, previous_loans_RATE_DOWN_PAYMENT_mean, bureau_AMT_CREDIT_SUM_DEBT_min, NAME_CONTRACT_TYPE_Cash loans, client_installments_AMT_INSTALMENT_min_sum, previous_loans_AMT_APPLICATION_max, client_installments_NUM_INSTALMENT_VERSION_sum_max, previous_loans_RATE_DOWN_PAYMENT_min, client_installments_AMT_INSTALMENT_mean_mean, client_installments_NUM_INSTALMENT_VERSION_sum_sum, previous_loans_NAME_TYPE_SUITE_Family_count_norm, previous_loans_DAYS_TERMINATION_mean, client_cash_CNT_INSTALMENT_FUTURE_max_sum, previous_loans_NAME_PAYMENT_TYPE_XNA_count_norm, bureau_CREDIT_TYPE_Credit card_count_norm, client_installments_DAYS_ENTRY_PAYMENT_..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// processing outliers list 4 of 7\n",
    "\n",
    "val colNames = creditRiskFeatures4.map(col(_))\n",
    "var creditRiskdf1_4 = creditRiskdf1.select(colNames:_*)\n",
    "creditRiskdf1_4.cache()\n",
    "outliers(creditRiskFeatures4, creditRiskdf1_4)\n",
    "println(highOutlier_list.length)\n",
    "println(lowOutlier_list.length)\n",
    "creditRiskdf1_4.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colNames: Array[org.apache.spark.sql.Column] = Array(client_bureau_balance_STATUS_0_count_sum, OCCUPATION_TYPE_Core staff, previous_loans_NAME_YIELD_GROUP_middle_count_norm, client_cash_NAME_CONTRACT_STATUS_Completed_count_norm_mean, bureau_AMT_CREDIT_SUM_OVERDUE_max, REG_CITY_NOT_LIVE_CITY, ORGANIZATION_TYPE_Business Entity Type 3, bureau_CREDIT_ACTIVE_Closed_count, ORGANIZATION_TYPE_Self-employed, previous_loans_NAME_PORTFOLIO_Cash_count_norm, previous_loans_WEEKDAY_APPR_PROCESS_START_WEDNESDAY_count_norm, client_installments_NUM_INSTALMENT_VERSION_mean_min, client_cash_NAME_CONTRACT_STATUS_Completed_count_sum, previous_loans_CODE_REJECT_REASON_SCO_count_norm, previous_loans_NAME_GOODS_CATEGORY_Computers_count_norm, previous_loans_NAME_YIELD_GROUP_XNA_count_norm, previous_loans_PRODUC..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// processing outliers list 5 of 7\n",
    "\n",
    "val colNames = creditRiskFeatures5.map(col(_))\n",
    "var creditRiskdf1_5 = creditRiskdf1.select(colNames:_*)\n",
    "creditRiskdf1_5.cache()\n",
    "outliers(creditRiskFeatures5, creditRiskdf1_5)\n",
    "println(highOutlier_list.length)\n",
    "println(lowOutlier_list.length)\n",
    "creditRiskdf1_5.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colNames: Array[org.apache.spark.sql.Column] = Array(client_cash_CNT_INSTALMENT_min_max, previous_loans_WEEKDAY_APPR_PROCESS_START_THURSDAY_count_norm, ORGANIZATION_TYPE_Construction, client_cash_NAME_CONTRACT_STATUS_Active_count_max, client_cash_SK_DPD_DEF_sum_min, previous_loans_NAME_CONTRACT_TYPE_Cash loans_count_norm, previous_loans_DAYS_FIRST_DRAWING_mean, previous_loans_PRODUCT_COMBINATION_Cash X-Sell: middle_count_norm, previous_loans_NAME_CASH_LOAN_PURPOSE_Other_count_norm, client_bureau_balance_STATUS_0_count_norm_sum, client_installments_NUM_INSTALMENT_VERSION_sum_min, previous_loans_CHANNEL_TYPE_Country-wide_count_norm, client_cash_NAME_CONTRACT_STATUS_Signed_count_sum, AMT_REQ_CREDIT_BUREAU_YEAR, bureau_CREDIT_TYPE_Microloan_count, NAME_INCOME_TYPE_State servant, previous_lo..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// processing outliers list 6 of 7\n",
    "\n",
    "val colNames = creditRiskFeatures6.map(col(_))\n",
    "var creditRiskdf1_6 = creditRiskdf1.select(colNames:_*)\n",
    "creditRiskdf1_6.cache()\n",
    "outliers(creditRiskFeatures6, creditRiskdf1_6)\n",
    "println(highOutlier_list.length)\n",
    "println(lowOutlier_list.length)\n",
    "creditRiskdf1_6.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colNames: Array[org.apache.spark.sql.Column] = Array(previous_loans_PRODUCT_COMBINATION_Cash Street: middle_count_norm, previous_loans_PRODUCT_COMBINATION_POS other with interest_count_norm, client_cash_CNT_INSTALMENT_min_min, previous_loans_CODE_REJECT_REASON_VERIF_count_norm, previous_loans_CODE_REJECT_REASON_LIMIT_count_norm, client_bureau_balance_STATUS_X_count_norm_sum, previous_loans_PRODUCT_COMBINATION_Cash_count_norm, previous_loans_NAME_GOODS_CATEGORY_Consumer Electronics_count_norm, previous_loans_CHANNEL_TYPE_Regional / Local_count_norm, previous_loans_WEEKDAY_APPR_PROCESS_START_SUNDAY_count, FLAG_DOCUMENT_13, previous_loans_CODE_REJECT_REASON_CLIENT_count_norm, client_bureau_balance_STATUS_X_count_sum, previous_loans_NAME_YIELD_GROUP_middle_count, previous_loans_NAME_CONTRAC..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// processing outliers list 7 of 7\n",
    "\n",
    "val colNames = creditRiskFeatures7.map(col(_))\n",
    "var creditRiskdf1_7 = creditRiskdf1.select(colNames:_*)\n",
    "creditRiskdf1_7.cache()\n",
    "outliers(creditRiskFeatures7, creditRiskdf1_7)\n",
    "println(highOutlier_list.length)\n",
    "println(lowOutlier_list.length)\n",
    "creditRiskdf1_7.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.io._\n",
       "file1: String = highOutlier_FinalList.txt\n",
       "file2: String = lowOutlier_FinalList.txt\n",
       "writer1: java.io.BufferedWriter = java.io.BufferedWriter@bb849c\n",
       "writer2: java.io.BufferedWriter = java.io.BufferedWriter@7f253bd8\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// persist storage of intermediate lists\n",
    "\n",
    "import java.io._\n",
    "\n",
    "val file1 = \"highOutlier_FinalList.txt\"\n",
    "val file2 = \"lowOutlier_FinalList.txt\"\n",
    "val writer1 = new BufferedWriter(new FileWriter(file1))\n",
    "highOutlier_list.map(_+\"\\n\").foreach(writer1.write)\n",
    "writer1.close()\n",
    "val writer2 = new BufferedWriter(new FileWriter(file2))\n",
    "lowOutlier_list.map(_+\"\\n\").foreach(writer2.write)\n",
    "writer2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue from cells below if directly loading highOutlier and lowOutlier lists from text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.io.Source\n",
       "highOutlier_list: List[String] = List(client_bureau_balance_MONTHS_BALANCE_max_sum, previous_loans_NAME_CONTRACT_STATUS_Refused_count, NAME_HOUSING_TYPE_House / apartment, previous_loans_NAME_PRODUCT_TYPE_XNA_count, previous_loans_NAME_CONTRACT_TYPE_Revolving loans_count_norm, client_bureau_balance_STATUS_X_count_sum, previous_loans_NAME_GOODS_CATEGORY_Consumer Electronics_count_norm, client_bureau_balance_STATUS_X_count_norm_sum, previous_loans_NAME_GOODS_CATEGORY_Mobile_count_norm, previous_loans_PRODUCT_COMBINATION_POS household with interest_count_norm, previous_loans_WEEKDAY_APPR_PROCESS_START_THURSDAY_count, client_bureau_balance_STATUS_0_count_norm_sum, previous_loans_PRODUCT_COMBINATION_Cash X-Sell: middle_count_norm, previous_loans_DAYS_FIRST_DRAWING_mean..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source\n",
    "\n",
    "var highOutlier_list = Source.fromFile(\"highOutlier_FinalList.txt\").getLines.toList\n",
    "var lowOutlier_list = Source.fromFile(\"lowOutlier_FinalList.txt\").getLines.toList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res5: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 280 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(highOutlier_list.length)\n",
    "println(lowOutlier_list.length)\n",
    "creditRiskdf.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded lists has identical rows to outliers variables processed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 280 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditRiskdf1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// drop features in highOutlier_list\n",
    "creditRiskdf1 = creditRiskdf1.drop(highOutlier_list:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183875\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "println(creditRiskdf1.count())\n",
    "println(creditRiskdf1.columns.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Output**\n",
    "creditRiskdf1 - cleansed dataset with features with outliers >5% removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var creditRiskdf2 = creditRiskdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditRiskdf2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "//code block takes features in lowOutlier_list and filters main dataframe against them\n",
    "//code will generate a subset of main dataset without outliers.\n",
    "\n",
    "//input feature list conatining features with outliers persisting\n",
    "\n",
    "for (feature <- lowOutlier_list){\n",
    "    //summarise column and query the DataFrame output\n",
    "    var firstQ = creditRiskdf2.select(feature).summary().where(\n",
    "        $\"summary\" === \"25%\").select(feature).first().mkString.toFloat\n",
    "    var thirdQ = creditRiskdf2.select(feature).summary().where(\n",
    "        $\"summary\" === \"75%\").select(feature).first().mkString.toFloat\n",
    "    //use formulas below to test threshold of outliers\n",
    "    var testValHigh = thirdQ + (1.5 * (thirdQ - firstQ))\n",
    "    var testValLow = firstQ - (1.5 * (thirdQ - firstQ))\n",
    "    if (testValHigh == 0 & testValLow == 0){}\n",
    "    else {\n",
    "        creditRiskdf2 = creditRiskdf2.filter(col(feature) <= lit(testValHigh)).filter(\n",
    "            col(feature) > lit(testValLow))        \n",
    "    }  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35583\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "println(creditRiskdf2.count())\n",
    "println(creditRiskdf2.columns.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Output**\n",
    "creditRiskdf2 - cleansed dataset with features with outliers >5% removed and data rows for features with outliers between 2-5% removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditRiskdf1.write.option(\"header\", \"true\").csv(\"creditRiskdf1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditRiskdf2.write.option(\"header\", \"true\").csv(\"creditRiskdf2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditRiskdf1.unpersist()\n",
    "creditRiskdf2.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the csv files to skip the outlier processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1_1: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "creditRiskdf1_2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "creditRiskdf1_3: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// Load creditRiskdf1 datasets: which exclude features with outliers >5%\n",
    "\n",
    "val creditRiskdf1_1 = spark.read.option(\"header\",\"true\").csv(\n",
    "    \"creditRiskdf1_1.csv\")\n",
    "val creditRiskdf1_2 = spark.read.option(\"header\",\"true\").csv(\n",
    "    \"creditRiskdf1_2.csv\")\n",
    "val creditRiskdf1_3 = spark.read.option(\"header\",\"true\").csv(\n",
    "    \"creditRiskdf1_3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1and2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "creditRiskdf1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// combine the 3 dataframes to 1\n",
    "\n",
    "val creditRiskdf1and2 = creditRiskdf1_1.union(creditRiskdf1_2)\n",
    "var creditRiskdf1 = creditRiskdf1and2.union(creditRiskdf1_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf2_1: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "creditRiskdf2_2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "creditRiskdf2_3: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// Load creditRiskdf2 datasets: which exclude features with outliers >5% \n",
    "// and datarows of features with outliers between 2-5%\n",
    "\n",
    "val creditRiskdf2_1 = spark.read.option(\"header\",\"true\").csv(\n",
    "    \"creditRiskdf2_1.csv\")\n",
    "val creditRiskdf2_2 = spark.read.option(\"header\",\"true\").csv(\n",
    "    \"creditRiskdf2_2.csv\")\n",
    "val creditRiskdf2_3 = spark.read.option(\"header\",\"true\").csv(\n",
    "    \"creditRiskdf2_3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1and2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "creditRiskdf2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// combine the 3 dataframes to 1\n",
    "\n",
    "val creditRiskdf1and2 = creditRiskdf2_1.union(creditRiskdf2_2)\n",
    "var creditRiskdf2= creditRiskdf1and2.union(creditRiskdf2_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "creditRiskdf1.cache()\n",
    "creditRiskdf2.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creditRiskdf1 structure:\n",
      "183875\n",
      "188\n",
      "creditRiskdf2 structure:\n",
      "35583\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "\n",
    "println(\"creditRiskdf1 structure:\")\n",
    "println(creditRiskdf1.count())\n",
    "println(creditRiskdf1.columns.size)\n",
    "println(\"creditRiskdf2 structure:\")\n",
    "println(creditRiskdf2.count())\n",
    "println(creditRiskdf2.columns.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskFeatures1: Array[String] = Array(EXT_SOURCE_2, EXT_SOURCE_3, DAYS_BIRTH, AMT_CREDIT, AMT_ANNUITY, DAYS_ID_PUBLISH, previous_loans_CNT_PAYMENT_mean, previous_loans_SELLERPLACE_AREA_max, DAYS_LAST_PHONE_CHANGE, DAYS_REGISTRATION, CODE_GENDER_F, REGION_POPULATION_RELATIVE, client_cash_CNT_INSTALMENT_max_mean, bureau_DAYS_CREDIT_mean, previous_loans_NAME_CONTRACT_STATUS_Refused_count_norm, client_installments_NUM_INSTALMENT_VERSION_max_mean, bureau_DAYS_CREDIT_min, previous_loans_NAME_YIELD_GROUP_high_count_norm, previous_loans_RATE_DOWN_PAYMENT_sum, previous_loans_DAYS_FIRST_DUE_min, client_cash_SK_DPD_DEF_sum_max, bureau_AMT_CREDIT_SUM_LIMIT_mean, bureau_DAYS_CREDIT_sum, previous_loans_DAYS_LAST_DUE_1ST_VERSION_max, previous_loans_PRODUCT_COMBINATION_Cash X-Sell: low_count_norm,..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// change datatype of data from String to Float\n",
    "var creditRiskFeatures1 = creditRiskdf1.columns\n",
    "var creditRiskFeatures2 = creditRiskdf2.columns\n",
    "\n",
    "for (colName<-creditRiskFeatures1){\n",
    "     |   creditRiskdf1 = creditRiskdf1.withColumn(\n",
    "         colName,col(colName).cast(\"Float\"))\n",
    "     | }\n",
    "\n",
    "for (colName<-creditRiskFeatures2){\n",
    "     |   creditRiskdf2 = creditRiskdf2.withColumn(\n",
    "         colName,col(colName).cast(\"Float\"))\n",
    "     | }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditRiskdf1.unpersist()\n",
    "creditRiskdf2.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Outputs**\n",
    "- creditRiskdf1: Cleansed dataset - features with outliers > 5% data rows removed.\n",
    "- creditRiskdf2 - cleansed dataset with features with outliers >5% removed and data rows for features with outliers between 2-5% removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Prepare training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_df1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "test_df1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "validation_df1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "train_df2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "test_df2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "validation_df2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// random split of cleansed dataset to training set (70%), testing set (20%)\n",
    "// and validation set (10%)\n",
    "\n",
    "var Array(train_df1,test_df1,validation_df1) = creditRiskdf1.randomSplit(\n",
    "    Array(0.7,0.2,0.1))\n",
    "var Array(train_df2,test_df2,validation_df2) = creditRiskdf2.randomSplit(\n",
    "    Array(0.7,0.2,0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outputs**\n",
    "- train_df1: to be used for fitting model1.\n",
    "- validation_df1: to be used for evaluating model1.\n",
    "- test_df1: to be kept hidden throughout fitting process and used to test the final model1.\n",
    "- train_df2: to be used for fitting model2.\n",
    "- validation_df2: to be used for evaluating model2.\n",
    "- test_df2: to be kept hidden throughout fitting process and used to test the final model2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Model fitting and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df1.cache()\n",
    "test_df1.cache()\n",
    "validation_df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_df1Features: Array[String] = Array(EXT_SOURCE_2, EXT_SOURCE_3, DAYS_BIRTH, AMT_CREDIT, AMT_ANNUITY, DAYS_ID_PUBLISH, previous_loans_CNT_PAYMENT_mean, previous_loans_SELLERPLACE_AREA_max, DAYS_LAST_PHONE_CHANGE, DAYS_REGISTRATION, CODE_GENDER_F, REGION_POPULATION_RELATIVE, client_cash_CNT_INSTALMENT_max_mean, bureau_DAYS_CREDIT_mean, previous_loans_NAME_CONTRACT_STATUS_Refused_count_norm, client_installments_NUM_INSTALMENT_VERSION_max_mean, bureau_DAYS_CREDIT_min, previous_loans_NAME_YIELD_GROUP_high_count_norm, previous_loans_RATE_DOWN_PAYMENT_sum, previous_loans_DAYS_FIRST_DUE_min, client_cash_SK_DPD_DEF_sum_max, bureau_AMT_CREDIT_SUM_LIMIT_mean, bureau_DAYS_CREDIT_sum, previous_loans_DAYS_LAST_DUE_1ST_VERSION_max, previous_loans_PRODUCT_COMBINATION_Cash X-Sell: low_count_norm, p..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// collate arrays of column names with and without the label \"TARGET\"\n",
    "\n",
    "val train_df1Features = train_df1.columns\n",
    "var features = train_df1Features.filter(!_.contains(\"TARGET\"))\n",
    "features = features.filter(!_.contains(\"SK_ID_CURR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Array[String] = Array()\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ensure that \"features\" does not contain SK_ID_CURR, TARGET or label\n",
    "features.filter(x => x == \"SK_ID_CURR\" | x == \"TARGET\" | x == \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2.1 Binomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_46f2062de2b1\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// place all features except for \"TARGET\" under column name \"features\"\n",
    "\n",
    "val lrAssembler = new VectorAssembler().setInputCols(features).setOutputCol(\n",
    "    \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrLabelIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_a75f174614e8\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// identify label for logistic regression\n",
    "\n",
    "val lrLabelIndexer = new StringIndexer().setInputCol(\"TARGET\").setOutputCol(\n",
    "    \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelLr: org.apache.spark.ml.classification.LogisticRegression = logreg_917f071b016d\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// fit logistic model\n",
    "\n",
    "val modelLr = new LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare pipeline for future validation, optimisation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrPipeline: org.apache.spark.ml.Pipeline = pipeline_092f8c8273b1\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrPipeline = new Pipeline().setStages(Array(\n",
    "    lrLabelIndexer, lrAssembler, modelLr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-09 09:41:10,913 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2019-06-09 09:41:10,921 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lrModel: org.apache.spark.ml.PipelineModel = pipeline_092f8c8273b1\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModel = lrPipeline.fit(train_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the metric to evaluate the model, e.g. ROC AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict using validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+--------------------+----------+\n",
      "|SK_ID_CURR|            features|label|         probability|prediction|\n",
      "+----------+--------------------+-----+--------------------+----------+\n",
      "|  147356.0|(186,[0,1,2,3,4,5...|  0.0|[0.89727683521013...|       0.0|\n",
      "|  125091.0|(186,[0,1,2,3,4,5...|  1.0|[0.59622894388730...|       0.0|\n",
      "|  125159.0|(186,[0,1,2,3,4,5...|  0.0|[0.54393536048157...|       0.0|\n",
      "|  116133.0|(186,[0,1,2,3,4,5...|  1.0|[0.86694266148273...|       0.0|\n",
      "|  139376.0|(186,[0,1,2,3,4,5...|  0.0|[0.91901451770221...|       0.0|\n",
      "|  162796.0|(186,[0,1,2,3,4,5...|  0.0|[0.67739912162574...|       0.0|\n",
      "|  169646.0|(186,[0,1,2,3,4,5...|  0.0|[0.62198363722784...|       0.0|\n",
      "|  181824.0|(186,[0,1,2,3,4,5...|  1.0|[0.71096236422142...|       0.0|\n",
      "|  153344.0|(186,[0,1,2,3,4,5...|  0.0|[0.57556155290442...|       0.0|\n",
      "|  206971.0|(186,[0,1,2,3,4,5...|  0.0|[0.92910830381305...|       0.0|\n",
      "|  210708.0|(186,[0,1,2,3,4,5...|  0.0|[0.92689831260893...|       0.0|\n",
      "|  160510.0|(186,[0,1,2,3,4,5...|  0.0|[0.54606338721107...|       0.0|\n",
      "|  120060.0|(186,[0,1,2,3,4,5...|  0.0|[0.90427623386002...|       0.0|\n",
      "|  159374.0|(186,[0,1,2,3,4,5...|  0.0|[0.87876022192900...|       0.0|\n",
      "|  199543.0|(186,[0,1,2,3,4,5...|  0.0|[0.78342094227701...|       0.0|\n",
      "|  100992.0|(186,[0,1,2,3,4,5...|  1.0|[0.62594858130524...|       0.0|\n",
      "|  108315.0|(186,[0,1,2,3,4,5...|  0.0|[0.83461550696406...|       0.0|\n",
      "|  187817.0|(186,[0,1,2,3,4,5...|  0.0|[0.88556598455214...|       0.0|\n",
      "|  109644.0|(186,[0,1,2,3,4,5...|  0.0|[0.94038672554174...|       0.0|\n",
      "|  199006.0|(186,[0,1,2,3,4,5...|  0.0|[0.68712806219240...|       0.0|\n",
      "+----------+--------------------+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions1: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 191 more fields]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions1 = lrModel.transform(validation_df1)\n",
    "predictions1.select (\"SK_ID_CURR\",\"features\", \"label\",\"probability\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "// println(s\"Coefficients: ${model.coefficients} Intercept: ${model.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy measurement 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.NaiveBayes\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.sql.SparkSession\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.NaiveBayes\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.9258536585365854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_371445436a86\n",
       "accuracy: Double = 0.9258536585365854\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions1)\n",
    "println(\"Test set accuracy = \" + accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy of our prediction is over 92.58%. Seems pretty good(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t1: Float = 1365.0\n",
       "t0: Float = 17085.0\n",
       "t1_percent: Float = 0.07398374\n",
       "t0_percent: Float = 0.9260163\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t1: Float = validation_df1.filter($\"TARGET\" ===  1).count()\n",
    "val t0: Float = validation_df1.filter($\"TARGET\" ===  0).count()\n",
    "val t1_percent = t1/validation_df1.count()\n",
    "val t0_percent = t0/validation_df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying data is split 7.40% and 92.60% for TARGET 1 and 0 respectively. Although the accuracy of the model is 92.58%, it is predicting marginally worse than the statistic means of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy measurement 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "import org.apache.spark.mllib.regression.LabeledPoint\n",
       "import org.apache.spark.mllib.util.MLUtils\n",
       "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lp: org.apache.spark.sql.DataFrame = [label: double, prediction: double]\n",
       "counttotal: Long = 18450\n",
       "correct: Long = 17082\n",
       "wrong: Long = 1368\n",
       "truep: Long = 17055\n",
       "falseN: Long = 1338\n",
       "falseP: Long = 30\n",
       "ratioWrong: Double = 0.07414634146341463\n",
       "ratioCorrect: Double = 0.9258536585365854\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lp = predictions1.select( \"label\", \"prediction\")\n",
    "val counttotal = predictions1.count()\n",
    "val correct = lp.filter($\"label\" === $\"prediction\").count() //correct prediction\n",
    "val wrong = lp.filter(not($\"label\" === $\"prediction\")).count() //wrong prediction\n",
    "val truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" === $\"prediction\").count() //Correct prediction of label=0\n",
    "val falseN = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" === $\"prediction\")).count() //Wrong prediction when label=1\n",
    "val falseP = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" === $\"prediction\")).count() //Wrong prediction when lable =0\n",
    "val ratioWrong=wrong.toDouble/counttotal.toDouble //Wrong prediction ratio\n",
    "val ratioCorrect=correct.toDouble/counttotal.toDouble //Correct predicition ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy measurement 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[453] at map at <console>:48\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@67ec21c2\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionLabelsRDD = predictions1.select(\"prediction\", \"label\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the receiver operating characteristic (ROC) curve : 0.5090121467645612\n"
     ]
    }
   ],
   "source": [
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC ROC of 0.51 is indicating the logistic regression model has not provided a strong class separation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "// Cross check training data set is being used\n",
    "println(\"Total data rows in combined dataframe: \"+ train_df1.count())\n",
    "var RF_df = train_df1\n",
    "val train_df1Features = train_df1.columns\n",
    "for (colName<-creditRiskFeatures){\n",
    "     |   RF_df=RF_df.withColumn(colName,col(colName).cast(\"Double\"))\n",
    "     | }\n",
    "//RF_df.printSchema()\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_df1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "test_df1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "validation_df1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var Array(train_df1,test_df1,validation_df1)=creditRiskdf1.randomSplit(\n",
    "    Array(0.7,0.2,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val c1 = train_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfLabelIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_484f47968833\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Index labels, adding metadata to the label column.\n",
    "// Fit on whole dataset to include all labels in index.\n",
    "val rfLabelIndexer = new StringIndexer().setInputCol(\"TARGET\").setOutputCol(\n",
    "    \"indexedLabel\").fit(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_df99d5945cca\n",
       "df1: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 187 more fields]\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfAssembler = new VectorAssembler().setInputCols(features).setOutputCol(\n",
    "    \"features\")\n",
    "val df1 = rfAssembler.transform(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfFeatureIndexer: org.apache.spark.ml.feature.VectorIndexerModel = vecIdx_6461f536c3df\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Automatically identify categorical features, and index them.\n",
    "// Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "val rfFeatureIndexer = new VectorIndexer()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"indexedFeatures\")\n",
    "  .setMaxCategories(4).fit(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelRf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_5c511328a8c1\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Train a RandomForest model.\n",
    "val modelRf = new RandomForestClassifier()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setFeaturesCol(\"indexedFeatures\")  \n",
    "  .setNumTrees(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfLabelConverter: org.apache.spark.ml.feature.IndexToString = idxToStr_41a5791c024a\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Convert indexed labels back to original labels.\n",
    "val rfLabelConverter = new IndexToString()\n",
    "  .setInputCol(\"prediction\")\n",
    "  .setOutputCol(\"predictedLabel\")\n",
    "  .setLabels(rfLabelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfPipeline: org.apache.spark.ml.Pipeline = pipeline_f17db21b15b0\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Chain indexers and forest in a Pipeline.\n",
    "val rfPipeline = new Pipeline()\n",
    "  .setStages(Array(rfLabelIndexer, rfFeatureIndexer, modelRf, rfLabelConverter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfModel: org.apache.spark.ml.PipelineModel = pipeline_f17db21b15b0\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Train model. This also runs the indexers.\n",
    "val rfModel = rfPipeline.fit(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 187 more fields]\n",
       "predictions2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 193 more fields]\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = rfAssembler.transform(validation_df1)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions2 = rfModel.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------+\n",
      "|            features|indexedLabel|         probability|predictedLabel|\n",
      "+--------------------+------------+--------------------+--------------+\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.91563597649114...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.91590035869383...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.92766246019460...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         1.0|[0.85817118937111...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.91994216494559...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         1.0|[0.91543889284909...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.92249828864000...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         1.0|[0.90517976178519...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.92491810062623...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         1.0|[0.88952982283765...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.92429906460496...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.90383704003198...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.93124993535982...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.92240039616163...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.92484964395240...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         1.0|[0.89048644374531...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.90343884529473...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         1.0|[0.91383622062069...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         1.0|[0.90183834917478...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.91985716971924...|           0.0|\n",
      "+--------------------+------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Select example rows to display.\n",
    "\n",
    "predictions2.select(\"features\", \"indexedLabel\", \"probability\",\"predictedLabel\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy measurement 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.07264887287811805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_8a8c5f50906a\n",
       "accuracy: Double = 0.927351127121882\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions2)\n",
    "println(s\"Test Error = ${(1.0 - accuracy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy of our prediction is 92.74%. However, as noted with the logistic regression, this accuracy appears to be marginally better than the dataset means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy measurement 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[589] at map at <console>:54\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@63a9476f\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionLabelsRDD = predictions2.select(\"prediction\", \"indexedLabel\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the receiver operating characteristic (ROC) curve : 0.5\n"
     ]
    }
   ],
   "source": [
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At an AUC ROC of 0.5, the Random forest classifier has not yielded separation of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df1.unpersist()\n",
    "test_df1.unpersist()\n",
    "validation_df1.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2.3 Multilayer Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfTrain: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n",
       "dfTrain: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n",
       "dfTest: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n",
       "dfTest: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var dfTrain = train_df1.withColumnRenamed(\"TARGET\", \"label\")\n",
    "dfTrain = dfTrain.drop(\"SK_ID_CURR\")\n",
    "\n",
    "var dfTest = test_df1.withColumnRenamed(\"TARGET\", \"label\")\n",
    "dfTest = dfTest.drop(\"SK_ID_CURR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res20: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain.cache()\n",
    "dfTest.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set count: 128324\n",
      "Test set count: 37230\n",
      "Training column count: 187\n",
      "Test column count: 187\n"
     ]
    }
   ],
   "source": [
    "println(\"Training set count: \"+dfTrain.count() + \"\\n\" +\n",
    "        \"Test set count: \"+dfTest.count())\n",
    "println(\"Training column count: \"+dfTrain.columns.size + \"\\n\" +\n",
    "        \"Test column count: \"+dfTest.columns.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlpcLabelIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_56d4a92d6718\n",
       "mlpcAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_08b91696fadb\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Index labels, adding metadata to the label column.\n",
    "// Fit on whole dataset to include all labels in index.\n",
    "var mlpcLabelIndexer = new StringIndexer().setInputCol(\"label\").setOutputCol(\n",
    "    \"indexedLabel\").fit(dfTrain)\n",
    "\n",
    "var mlpcAssembler = new VectorAssembler()\n",
    "    .setInputCols(features).setOutputCol(\"features\")\n",
    "\n",
    "//var mlpcTrain = Assembler.transform(dfTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "layers: Array[Int] = Array(186, 50, 25, 2)\n",
       "mlpcModel: org.apache.spark.ml.classification.MultilayerPerceptronClassifier = mlpc_3ec4a2453930\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Automatically identify categorical features, and index them.\n",
    "//var mlpcFeatureIndexer = new VectorIndexer()\n",
    "//  .setInputCol(\"features\")\n",
    "//  .setOutputCol(\"indexedFeatures\")\n",
    "\n",
    "// specify layers for the neural network:\n",
    "// input layer of size 187 (features), two intermediate of size 45 and 10\n",
    "// and output of size 2 (classes)\n",
    "//https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/\n",
    "\n",
    "var layers = Array[Int](186, 50, 25, 2)\n",
    "\n",
    "// create the trainer and set its parameters\n",
    "var mlpcModel = new MultilayerPerceptronClassifier()\n",
    "  .setLayers(layers)\n",
    "  .setBlockSize(128)\n",
    "  .setTol(0.0005)\n",
    "  .setSeed(123)\n",
    "  .setMaxIter(700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlpcLabelConverter: org.apache.spark.ml.feature.IndexToString = idxToStr_79d8010d7ec5\n",
       "mlpcPipeline: org.apache.spark.ml.Pipeline = pipeline_c31d42af4b3a\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Convert indexed labels back to original labels.\n",
    "val mlpcLabelConverter = new IndexToString()\n",
    "  .setInputCol(\"prediction\")\n",
    "  .setOutputCol(\"predictedLabel\")\n",
    "  .setLabels(mlpcLabelIndexer.labels)\n",
    "\n",
    "// Chain indexers and forest in a Pipeline.\n",
    "var mlpcPipeline = new Pipeline().setStages(\n",
    "    Array(mlpcLabelIndexer, mlpcAssembler, mlpcModel, mlpcLabelConverter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlpcModelTrained: org.apache.spark.ml.PipelineModel = pipeline_c31d42af4b3a\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// train the model\n",
    "var mlpcModelTrained = mlpcPipeline.fit(dfTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 191 more fields]\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var result = mlpcModelTrained.transform(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EXT_SOURCE_2: float (nullable = true)\n",
      " |-- EXT_SOURCE_3: float (nullable = true)\n",
      " |-- DAYS_BIRTH: float (nullable = true)\n",
      " |-- AMT_CREDIT: float (nullable = true)\n",
      " |-- AMT_ANNUITY: float (nullable = true)\n",
      " |-- DAYS_ID_PUBLISH: float (nullable = true)\n",
      " |-- previous_loans_CNT_PAYMENT_mean: float (nullable = true)\n",
      " |-- previous_loans_SELLERPLACE_AREA_max: float (nullable = true)\n",
      " |-- DAYS_LAST_PHONE_CHANGE: float (nullable = true)\n",
      " |-- DAYS_REGISTRATION: float (nullable = true)\n",
      " |-- CODE_GENDER_F: float (nullable = true)\n",
      " |-- REGION_POPULATION_RELATIVE: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_max_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_mean: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Refused_count_norm: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_max_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_high_count_norm: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_sum: float (nullable = true)\n",
      " |-- previous_loans_DAYS_FIRST_DUE_min: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_DEF_sum_max: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_LIMIT_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_sum: float (nullable = true)\n",
      " |-- previous_loans_DAYS_LAST_DUE_1ST_VERSION_max: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash X-Sell: low_count_norm: float (nullable = true)\n",
      " |-- previous_loans_HOUR_APPR_PROCESS_START_mean: float (nullable = true)\n",
      " |-- NAME_FAMILY_STATUS_Married: float (nullable = true)\n",
      " |-- previous_loans_CNT_PAYMENT_max: float (nullable = true)\n",
      " |-- AMT_INCOME_TOTAL: float (nullable = true)\n",
      " |-- previous_loans_DAYS_LAST_DUE_sum: float (nullable = true)\n",
      " |-- client_cash_MONTHS_BALANCE_max_mean: float (nullable = true)\n",
      " |-- NAME_EDUCATION_TYPE_Higher education: float (nullable = true)\n",
      " |-- client_cash_MONTHS_BALANCE_sum_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_ENDDATE_FACT_min: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_ENDDATE_min: float (nullable = true)\n",
      " |-- previous_loans_AMT_ANNUITY_mean: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Mortgage_count_norm: float (nullable = true)\n",
      " |-- previous_loans_AMT_ANNUITY_max: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_max: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash X-Sell: high_count_norm: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Active_count: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_low_action_count_norm: float (nullable = true)\n",
      " |-- client_cash_counts_mean: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_sum_min: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Active_count_norm: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_UPDATE_min: float (nullable = true)\n",
      " |-- DEF_60_CNT_SOCIAL_CIRCLE: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_3: float (nullable = true)\n",
      " |-- bureau_DAYS_ENDDATE_FACT_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_LIMIT_max: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_max_sum: float (nullable = true)\n",
      " |-- previous_loans_NAME_TYPE_SUITE_Unaccompanied_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_norm_mean: float (nullable = true)\n",
      " |-- client_installments_DAYS_ENTRY_PAYMENT_max_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Approved_count_norm: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_min_mean: float (nullable = true)\n",
      " |-- previous_loans_DAYS_DECISION_mean: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash Street: low_count_norm: float (nullable = true)\n",
      " |-- HOUR_APPR_PROCESS_START: float (nullable = true)\n",
      " |-- previous_loans_DAYS_LAST_DUE_1ST_VERSION_min: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Consumer credit_count_norm: float (nullable = true)\n",
      " |-- previous_loans_HOUR_APPR_PROCESS_START_max: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_HC_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_PRODUCT_TYPE_walk-in_count_norm: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_UPDATE_mean: float (nullable = true)\n",
      " |-- previous_loans_DAYS_DECISION_sum: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS industry with interest_count_norm: float (nullable = true)\n",
      " |-- client_cash_MONTHS_BALANCE_max_sum: float (nullable = true)\n",
      " |-- previous_loans_NAME_CLIENT_TYPE_Refreshed_count_norm: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash Street: high_count_norm: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_DEBT_min: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_min: float (nullable = true)\n",
      " |-- previous_loans_DAYS_TERMINATION_mean: float (nullable = true)\n",
      " |-- previous_loans_NAME_PAYMENT_TYPE_XNA_count_norm: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Credit card_count_norm: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_18: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Credit and cash offices_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Contact center_count_norm: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Car loan_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_PORTFOLIO_POS_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_TUESDAY_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Furniture_count_norm: float (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_QRT: float (nullable = true)\n",
      " |-- NAME_INCOME_TYPE_Working: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Microloan_count_norm: float (nullable = true)\n",
      " |-- previous_loans_DAYS_TERMINATION_min: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Channel of corporate sales_count_norm: float (nullable = true)\n",
      " |-- NAME_EDUCATION_TYPE_Secondary / secondary special: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_Core staff: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_middle_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Completed_count_norm_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_OVERDUE_max: float (nullable = true)\n",
      " |-- REG_CITY_NOT_LIVE_CITY: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Business Entity Type 3: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Closed_count: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Self-employed: float (nullable = true)\n",
      " |-- previous_loans_NAME_PORTFOLIO_Cash_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_WEDNESDAY_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Completed_count_sum: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_SCO_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_XNA_count_norm: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS household without interest_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_PRODUCT_TYPE_XNA_count_norm: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_LIMIT_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_PRODUCT_TYPE_x-sell_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_MONDAY_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_CLIENT_TYPE_New_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_AP+ (Cash loan)_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NFLAG_INSURED_ON_APPROVAL_mean: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_mean_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Clothing and Accessories_count_norm: float (nullable = true)\n",
      " |-- DEF_30_CNT_SOCIAL_CIRCLE: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_NUMBER_mean_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_PORTFOLIO_XNA_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_CLIENT_TYPE_Repeater_count_norm: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Credit card_count: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Canceled_count_norm: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_Drivers: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Signed_count_norm_max: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_Laborers: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_THURSDAY_count_norm: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Construction: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_max: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_DEF_sum_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_TYPE_Cash loans_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_CASH_LOAN_PURPOSE_Other_count_norm: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_sum_min: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Country-wide_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Signed_count_sum: float (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_YEAR: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Microloan_count: float (nullable = true)\n",
      " |-- NAME_INCOME_TYPE_State servant: float (nullable = true)\n",
      " |-- previous_loans_DAYS_TERMINATION_max: float (nullable = true)\n",
      " |-- FLAG_WORK_PHONE: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Photo / Cinema Equipment_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_FRIDAY_count_norm: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Military: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_min_sum: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Completed_count_norm_sum: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_16: float (nullable = true)\n",
      " |-- previous_loans_NAME_TYPE_SUITE_Spouse, partner_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Returned to the store_count_sum: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Industry: type 9: float (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_MON: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Approved_count: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Sold_count_norm: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_OVERDUE_mean: float (nullable = true)\n",
      " |-- previous_loans_NAME_SELLER_INDUSTRY_Construction_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_SCOFR_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_SELLER_INDUSTRY_Furniture_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_norm_max: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Construction Materials_count_norm: float (nullable = true)\n",
      " |-- client_bureau_balance_STATUS_1_count_norm_sum: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash Street: middle_count_norm: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS other with interest_count_norm: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_min_min: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_VERIF_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_LIMIT_count_norm: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Regional / Local_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_SUNDAY_count: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_13: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_CLIENT_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_middle_count: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Jewelry_count_norm: float (nullable = true)\n",
      " |-- WALLSMATERIAL_MODE_Stone, brick: float (nullable = true)\n",
      " |-- previous_loans_NAME_SELLER_INDUSTRY_Consumer electronics_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CNT_PAYMENT_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_high_count: float (nullable = true)\n",
      " |-- NAME_FAMILY_STATUS_Separated: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Signed_count_norm_mean: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Card Street_count_norm: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_NUMBER_max_min: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS industry without interest_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Regional / Local_count: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Transport: type 3: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Returned to the store_count_norm_max: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_School: float (nullable = true)\n",
      " |-- previous_loans_NAME_TYPE_SUITE_Other_B_count_norm: float (nullable = true)\n",
      " |-- bureau_CNT_CREDIT_PROLONG_mean: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_min: float (nullable = true)\n",
      " |-- WEEKDAY_APPR_PROCESS_START_WEDNESDAY: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_High skill tech staff: float (nullable = true)\n",
      " |-- previous_loans_NAME_PAYMENT_TYPE_XNA_count: float (nullable = true)\n",
      " |-- FLAG_PHONE: float (nullable = true)\n",
      " |-- label: float (nullable = true)\n",
      " |-- indexedLabel: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      " |-- predictedLabel: string (nullable = true)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+----------+\n",
      "|            features|label|         probability|prediction|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "|(186,[0,1,2,3,4,5...|  0.0|[0.91647943660435...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  0.0|[0.92927620747784...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  1.0|[0.92810015207010...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  0.0|[0.94362794007198...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  0.0|[0.92892181267675...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  0.0|[0.91408102866485...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  1.0|[0.94448131627065...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  0.0|[0.93883166118189...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  0.0|[0.92601032327863...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  0.0|[0.92683238317496...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  1.0|[0.93545668765673...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  1.0|[0.92410409078157...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  0.0|[0.92615580562018...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  0.0|[0.92892181267675...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  0.0|[0.93355384812593...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  0.0|[0.94178277503137...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  1.0|[0.91741819381355...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  0.0|[0.91522437579923...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  0.0|[0.93883166118189...|       0.0|\n",
      "|(186,[0,1,2,3,4,5...|  0.0|[0.94429459589276...|       0.0|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select(\"features\", \"label\", \"probability\",\"prediction\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.9284448025785657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_1b0b13971df5\n",
       "accuracy: Double = 0.9284448025785657\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "\n",
    "val accuracy = evaluator.evaluate(result)\n",
    "\n",
    "println(\"Test set accuracy = \" + accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[768] at map at <console>:56\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@1ee42fd4\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionLabelsRDD = result.select(\"prediction\", \"indexedLabel\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the receiver operating characteristic (ROC) curve : 0.5\n"
     ]
    }
   ],
   "source": [
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res28: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain.unpersist()\n",
    "dfTest.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2.4 Model fitting conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binomial Logistic Regression**\n",
    "- Accuracy: 92.58%\n",
    "- AUC ROC: 0.51\n",
    "\n",
    "**Random Forest**\n",
    "- Accuracy: 92.74%\n",
    "- AUC ROC: 0.50\n",
    "\n",
    "**Multilayer Perceptron Classifier**\n",
    "- Accuracy: 92.84%\n",
    "- AUC ROC: 0.50\n",
    "\n",
    "Although all 3 models generated high accuracy scores, they compared close to statistic means of the underlying data which is split 7.40% and 92.60% for TARGET 1 and 0 respectively. \n",
    "\n",
    "Using another measure AUC ROC, it indicates that the models were not able to generate further class separation of the potential defaulting and non-defaulting customers.  This is due to a high skew of dataset towards non-defaulting customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Model Optimisation 1 -  outlier curtailment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model optimisation, dataset creditRiskdf2 will be used. As a refresher, this dataset has outlier data rows for features with 2-5% outliers (as a % of total data rows) removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res29: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df2.cache()\n",
    "test_df2.cache()\n",
    "validation_df2.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3.1 Binomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrModel: org.apache.spark.ml.PipelineModel = pipeline_092f8c8273b1\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModel = lrPipeline.fit(train_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+--------------------+----------+\n",
      "|SK_ID_CURR|            features|label|         probability|prediction|\n",
      "+----------+--------------------+-----+--------------------+----------+\n",
      "|  147356.0|(186,[0,1,2,3,4,5...|  0.0|[0.89727683521013...|       0.0|\n",
      "|  125091.0|(186,[0,1,2,3,4,5...|  1.0|[0.59622894388730...|       0.0|\n",
      "|  125159.0|(186,[0,1,2,3,4,5...|  0.0|[0.54393536048157...|       0.0|\n",
      "|  116133.0|(186,[0,1,2,3,4,5...|  1.0|[0.86694266148273...|       0.0|\n",
      "|  139376.0|(186,[0,1,2,3,4,5...|  0.0|[0.91901451770221...|       0.0|\n",
      "|  162796.0|(186,[0,1,2,3,4,5...|  0.0|[0.67739912162574...|       0.0|\n",
      "|  169646.0|(186,[0,1,2,3,4,5...|  0.0|[0.62198363722784...|       0.0|\n",
      "|  181824.0|(186,[0,1,2,3,4,5...|  1.0|[0.71096236422142...|       0.0|\n",
      "|  153344.0|(186,[0,1,2,3,4,5...|  0.0|[0.57556155290442...|       0.0|\n",
      "|  206971.0|(186,[0,1,2,3,4,5...|  0.0|[0.92910830381305...|       0.0|\n",
      "|  210708.0|(186,[0,1,2,3,4,5...|  0.0|[0.92689831260893...|       0.0|\n",
      "|  160510.0|(186,[0,1,2,3,4,5...|  0.0|[0.54606338721107...|       0.0|\n",
      "|  120060.0|(186,[0,1,2,3,4,5...|  0.0|[0.90427623386002...|       0.0|\n",
      "|  159374.0|(186,[0,1,2,3,4,5...|  0.0|[0.87876022192900...|       0.0|\n",
      "|  199543.0|(186,[0,1,2,3,4,5...|  0.0|[0.78342094227701...|       0.0|\n",
      "|  100992.0|(186,[0,1,2,3,4,5...|  1.0|[0.62594858130524...|       0.0|\n",
      "|  108315.0|(186,[0,1,2,3,4,5...|  0.0|[0.83461550696406...|       0.0|\n",
      "|  187817.0|(186,[0,1,2,3,4,5...|  0.0|[0.88556598455214...|       0.0|\n",
      "|  109644.0|(186,[0,1,2,3,4,5...|  0.0|[0.94038672554174...|       0.0|\n",
      "|  199006.0|(186,[0,1,2,3,4,5...|  0.0|[0.68712806219240...|       0.0|\n",
      "+----------+--------------------+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions3: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 191 more fields]\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions3 = lrModel.transform(validation_df2)\n",
    "predictions1.select (\"SK_ID_CURR\",\"features\", \"label\",\"probability\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.923054587688734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_3997623069d8\n",
       "accuracy: Double = 0.923054587688734\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions3)\n",
    "println(\"Test set accuracy = \" + accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t1: Float = 1331.0\n",
       "t0: Float = 16990.0\n",
       "t1_percent: Float = 0.072648875\n",
       "t0_percent: Float = 0.9273511\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t1: Float = validation_df1.filter($\"TARGET\" ===  1).count()\n",
    "val t0: Float = validation_df1.filter($\"TARGET\" ===  0).count()\n",
    "val t1_percent = t1/validation_df1.count()\n",
    "val t0_percent = t0/validation_df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lp: org.apache.spark.sql.DataFrame = [label: double, prediction: double]\n",
       "counttotal: Long = 18450\n",
       "correct: Long = 3179\n",
       "wrong: Long = 265\n",
       "truep: Long = 3177\n",
       "falseN: Long = 261\n",
       "falseP: Long = 4\n",
       "ratioWrong: Double = 0.014363143631436315\n",
       "ratioCorrect: Double = 0.17230352303523036\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lp = predictions3.select( \"label\", \"prediction\")\n",
    "val counttotal = predictions1.count()\n",
    "val correct = lp.filter($\"label\" === $\"prediction\").count() //correct prediction\n",
    "val wrong = lp.filter(not($\"label\" === $\"prediction\")).count() //wrong prediction\n",
    "val truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" === $\"prediction\").count() //Correct prediction of label=0\n",
    "val falseN = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" === $\"prediction\")).count() //Wrong prediction when label=1\n",
    "val falseP = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" === $\"prediction\")).count() //Wrong prediction when lable =0\n",
    "val ratioWrong=wrong.toDouble/counttotal.toDouble //Wrong prediction ratio\n",
    "val ratioCorrect=correct.toDouble/counttotal.toDouble //Correct predicition ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[1096] at map at <console>:56\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@52b4af98\n"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionLabelsRDD = predictions3.select(\"prediction\", \"label\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the receiver operating characteristic (ROC) curve : 0.5031735482660235\n"
     ]
    }
   ],
   "source": [
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC ROC for logistic regression model decreased from 0.51 to 0.50 using creditRiskdf2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "rfLabelIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_a5ea39c94308\n",
       "df3: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 187 more fields]\n",
       "rfModel: org.apache.spark.ml.PipelineModel = pipeline_f17db21b15b0\n"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val c2 = train_df2\n",
    "\n",
    "// Index labels, adding metadata to the label column.\n",
    "// Fit on whole dataset to include all labels in index.\n",
    "val rfLabelIndexer = new StringIndexer().setInputCol(\"TARGET\").setOutputCol(\n",
    "    \"indexedLabel\").fit(c2)\n",
    "\n",
    "val df3 = rfAssembler.transform(c2)\n",
    "\n",
    "// Train model. This also runs the indexers.\n",
    "val rfModel = rfPipeline.fit(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df4: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 187 more fields]\n",
       "predictions4: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 193 more fields]\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df4 = rfAssembler.transform(validation_df2)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions4 = rfModel.transform(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------+\n",
      "|            features|indexedLabel|         probability|predictedLabel|\n",
      "+--------------------+------------+--------------------+--------------+\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.90582830218447...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         1.0|[0.90093370608547...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         1.0|[0.91498151754966...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         1.0|[0.93468911124092...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.93519520871848...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.91448862058862...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.89567564996996...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         1.0|[0.90133440043016...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.90184265770812...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.87752822973581...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.91688584539600...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         1.0|[0.91813251678526...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.89990646598775...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.89277946354024...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.91949142809172...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         1.0|[0.85359671121982...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         1.0|[0.88962586447459...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.88416111450434...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.93514243126739...|           0.0|\n",
      "|(186,[0,1,2,3,4,5...|         0.0|[0.90252235102835...|           0.0|\n",
      "+--------------------+------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Select example rows to display.\n",
    "\n",
    "predictions4.select(\"features\", \"indexedLabel\", \"probability\",\"predictedLabel\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.07636469221835074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_ec570692784f\n",
       "accuracy: Double = 0.9236353077816493\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions4)\n",
    "println(s\"Test Error = ${(1.0 - accuracy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[1188] at map at <console>:56\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@53572ad7\n"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionLabelsRDD = predictions4.select(\"prediction\", \"indexedLabel\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the receiver operating characteristic (ROC) curve : 0.5\n"
     ]
    }
   ],
   "source": [
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement. AUC ROC remained at 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "val rfModel = model.stages(2).asInstanceOf[RandomForestClassificationModel]\n",
    "println(s\"Learned classification forest model:\\n ${rfModel.toDebugString}\")\n",
    "\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3.3 Model Optimisation 1 conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binomial Logistic Regression**\n",
    "- Accuracy: 92.31%\n",
    "- AUC ROC: 0.50\n",
    "\n",
    "**Random Forest**\n",
    "- Accuracy: 92.36%\n",
    "- AUC ROC: 0.50\n",
    "\n",
    "**Multilayer Perceptron Classifier**\n",
    "- did not run as there was no evidence of optimisation happening with the logistic regression and random forest models\n",
    "\n",
    "Again, the models generated high accuracy scores being trained with smaller dataset. However, they compared close to statistic means of the underlying data which is split 7.27% and 92.74% for TARGET 1 and 0 respectively. \n",
    "\n",
    "Both models generated AUC ROC of around 0.50, which again indicates that the models were not able to generate further class separation of the potential defaulting and non-defaulting customers.  This is due to a high skew of dataset towards non-defaulting customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4 Model Optimisation 2 - Bias reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this optimisation, we will train the models using a balanced dataset of defaulting and non-defaulting customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4.1 Prepare unbiased training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of TARGET = 1: 9317\n",
      "# of TARGET = 0: 119007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df9: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "df10: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df9 = train_df1.filter($\"TARGET\" === 1.0)\n",
    "println(\"# of TARGET = 1: \"+df9.count())\n",
    "\n",
    "val df10 = train_df1.filter($\"TARGET\" === 0.0)\n",
    "println(\"# of TARGET = 0: \"+df10.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EXT_SOURCE_2: float (nullable = true)\n",
      " |-- EXT_SOURCE_3: float (nullable = true)\n",
      " |-- DAYS_BIRTH: float (nullable = true)\n",
      " |-- AMT_CREDIT: float (nullable = true)\n",
      " |-- AMT_ANNUITY: float (nullable = true)\n",
      " |-- DAYS_ID_PUBLISH: float (nullable = true)\n",
      " |-- previous_loans_CNT_PAYMENT_mean: float (nullable = true)\n",
      " |-- previous_loans_SELLERPLACE_AREA_max: float (nullable = true)\n",
      " |-- DAYS_LAST_PHONE_CHANGE: float (nullable = true)\n",
      " |-- DAYS_REGISTRATION: float (nullable = true)\n",
      " |-- CODE_GENDER_F: float (nullable = true)\n",
      " |-- REGION_POPULATION_RELATIVE: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_max_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_mean: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Refused_count_norm: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_max_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_high_count_norm: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_sum: float (nullable = true)\n",
      " |-- previous_loans_DAYS_FIRST_DUE_min: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_DEF_sum_max: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_LIMIT_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_sum: float (nullable = true)\n",
      " |-- previous_loans_DAYS_LAST_DUE_1ST_VERSION_max: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash X-Sell: low_count_norm: float (nullable = true)\n",
      " |-- previous_loans_HOUR_APPR_PROCESS_START_mean: float (nullable = true)\n",
      " |-- NAME_FAMILY_STATUS_Married: float (nullable = true)\n",
      " |-- previous_loans_CNT_PAYMENT_max: float (nullable = true)\n",
      " |-- AMT_INCOME_TOTAL: float (nullable = true)\n",
      " |-- previous_loans_DAYS_LAST_DUE_sum: float (nullable = true)\n",
      " |-- client_cash_MONTHS_BALANCE_max_mean: float (nullable = true)\n",
      " |-- NAME_EDUCATION_TYPE_Higher education: float (nullable = true)\n",
      " |-- client_cash_MONTHS_BALANCE_sum_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_ENDDATE_FACT_min: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_ENDDATE_min: float (nullable = true)\n",
      " |-- previous_loans_AMT_ANNUITY_mean: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Mortgage_count_norm: float (nullable = true)\n",
      " |-- previous_loans_AMT_ANNUITY_max: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_max: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash X-Sell: high_count_norm: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Active_count: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_low_action_count_norm: float (nullable = true)\n",
      " |-- client_cash_counts_mean: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_sum_min: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Active_count_norm: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_UPDATE_min: float (nullable = true)\n",
      " |-- DEF_60_CNT_SOCIAL_CIRCLE: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_3: float (nullable = true)\n",
      " |-- bureau_DAYS_ENDDATE_FACT_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_LIMIT_max: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_max_sum: float (nullable = true)\n",
      " |-- previous_loans_NAME_TYPE_SUITE_Unaccompanied_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_norm_mean: float (nullable = true)\n",
      " |-- client_installments_DAYS_ENTRY_PAYMENT_max_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Approved_count_norm: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_min_mean: float (nullable = true)\n",
      " |-- previous_loans_DAYS_DECISION_mean: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash Street: low_count_norm: float (nullable = true)\n",
      " |-- HOUR_APPR_PROCESS_START: float (nullable = true)\n",
      " |-- previous_loans_DAYS_LAST_DUE_1ST_VERSION_min: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Consumer credit_count_norm: float (nullable = true)\n",
      " |-- previous_loans_HOUR_APPR_PROCESS_START_max: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_HC_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_PRODUCT_TYPE_walk-in_count_norm: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_UPDATE_mean: float (nullable = true)\n",
      " |-- previous_loans_DAYS_DECISION_sum: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS industry with interest_count_norm: float (nullable = true)\n",
      " |-- client_cash_MONTHS_BALANCE_max_sum: float (nullable = true)\n",
      " |-- previous_loans_NAME_CLIENT_TYPE_Refreshed_count_norm: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash Street: high_count_norm: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_DEBT_min: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_min: float (nullable = true)\n",
      " |-- previous_loans_DAYS_TERMINATION_mean: float (nullable = true)\n",
      " |-- previous_loans_NAME_PAYMENT_TYPE_XNA_count_norm: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Credit card_count_norm: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_18: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Credit and cash offices_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Contact center_count_norm: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Car loan_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_PORTFOLIO_POS_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_TUESDAY_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Furniture_count_norm: float (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_QRT: float (nullable = true)\n",
      " |-- NAME_INCOME_TYPE_Working: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Microloan_count_norm: float (nullable = true)\n",
      " |-- previous_loans_DAYS_TERMINATION_min: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Channel of corporate sales_count_norm: float (nullable = true)\n",
      " |-- NAME_EDUCATION_TYPE_Secondary / secondary special: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_Core staff: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_middle_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Completed_count_norm_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_OVERDUE_max: float (nullable = true)\n",
      " |-- REG_CITY_NOT_LIVE_CITY: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Business Entity Type 3: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Closed_count: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Self-employed: float (nullable = true)\n",
      " |-- previous_loans_NAME_PORTFOLIO_Cash_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_WEDNESDAY_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Completed_count_sum: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_SCO_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_XNA_count_norm: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS household without interest_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_PRODUCT_TYPE_XNA_count_norm: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_LIMIT_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_PRODUCT_TYPE_x-sell_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_MONDAY_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_CLIENT_TYPE_New_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_AP+ (Cash loan)_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NFLAG_INSURED_ON_APPROVAL_mean: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_mean_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Clothing and Accessories_count_norm: float (nullable = true)\n",
      " |-- DEF_30_CNT_SOCIAL_CIRCLE: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_NUMBER_mean_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_PORTFOLIO_XNA_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_CLIENT_TYPE_Repeater_count_norm: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Credit card_count: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Canceled_count_norm: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_Drivers: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Signed_count_norm_max: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_Laborers: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_THURSDAY_count_norm: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Construction: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_max: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_DEF_sum_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_TYPE_Cash loans_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_CASH_LOAN_PURPOSE_Other_count_norm: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_sum_min: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Country-wide_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Signed_count_sum: float (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_YEAR: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Microloan_count: float (nullable = true)\n",
      " |-- NAME_INCOME_TYPE_State servant: float (nullable = true)\n",
      " |-- previous_loans_DAYS_TERMINATION_max: float (nullable = true)\n",
      " |-- FLAG_WORK_PHONE: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Photo / Cinema Equipment_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_FRIDAY_count_norm: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Military: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_min_sum: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Completed_count_norm_sum: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_16: float (nullable = true)\n",
      " |-- previous_loans_NAME_TYPE_SUITE_Spouse, partner_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Returned to the store_count_sum: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Industry: type 9: float (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_MON: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Approved_count: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Sold_count_norm: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_OVERDUE_mean: float (nullable = true)\n",
      " |-- previous_loans_NAME_SELLER_INDUSTRY_Construction_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_SCOFR_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_SELLER_INDUSTRY_Furniture_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_norm_max: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Construction Materials_count_norm: float (nullable = true)\n",
      " |-- client_bureau_balance_STATUS_1_count_norm_sum: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash Street: middle_count_norm: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS other with interest_count_norm: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_min_min: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_VERIF_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_LIMIT_count_norm: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Regional / Local_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_SUNDAY_count: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_13: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_CLIENT_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_middle_count: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Jewelry_count_norm: float (nullable = true)\n",
      " |-- WALLSMATERIAL_MODE_Stone, brick: float (nullable = true)\n",
      " |-- previous_loans_NAME_SELLER_INDUSTRY_Consumer electronics_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CNT_PAYMENT_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_high_count: float (nullable = true)\n",
      " |-- NAME_FAMILY_STATUS_Separated: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Signed_count_norm_mean: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Card Street_count_norm: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_NUMBER_max_min: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS industry without interest_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Regional / Local_count: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Transport: type 3: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Returned to the store_count_norm_max: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_School: float (nullable = true)\n",
      " |-- previous_loans_NAME_TYPE_SUITE_Other_B_count_norm: float (nullable = true)\n",
      " |-- bureau_CNT_CREDIT_PROLONG_mean: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_min: float (nullable = true)\n",
      " |-- WEEKDAY_APPR_PROCESS_START_WEDNESDAY: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_High skill tech staff: float (nullable = true)\n",
      " |-- previous_loans_NAME_PAYMENT_TYPE_XNA_count: float (nullable = true)\n",
      " |-- FLAG_PHONE: float (nullable = true)\n",
      " |-- TARGET: float (nullable = true)\n",
      " |-- SK_ID_CURR: float (nullable = true)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df1.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df11: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "res39: Long = 9174\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Now from 119007 rows of TARGET==0, we randomly extract c.9317 rows and combine it with df9\n",
    "//To get a dataframe of c.50% target==1 and c.50% target ==0\n",
    "\n",
    "val df11 = df10.sample(false, 9222.toFloat/119415.toFloat)\n",
    "df11.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df12: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df12 = df9.union(df11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res40: df12.type = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df12.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res41: Array[String] = Array()\n"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// making sure that \"features\" does not include \"SK_ID_CURR\", \"TARGET\" or \"label\"\n",
    "\n",
    "features.filter(x => x == \"SK_ID_CURR\" | x == \"TARGET\" | x == \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4.2 Binomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrModel_unbiased: org.apache.spark.ml.PipelineModel = pipeline_092f8c8273b1\n"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModel_unbiased = lrPipeline.fit(df12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.6951039790404454\n",
      "Area under the receiver operating characteristic (ROC) curve : 0.690885742220752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testLr_unbiased: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 191 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_b41ce57d6eda\n",
       "accuracy: Double = 0.6951039790404454\n",
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[1805] at map at <console>:73\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@34555342\n"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testLr_unbiased = lrModel_unbiased.transform(validation_df1)\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "\n",
    "val accuracy = evaluator.evaluate(testLr_unbiased)\n",
    "println(\"Test set accuracy = \" + accuracy)\n",
    "\n",
    "val predictionLabelsRDD = testLr_unbiased.select(\n",
    "    \"prediction\", \"label\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)\n",
    "\n",
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df13: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 187 more fields]\n",
       "rfModel_unbiased: org.apache.spark.ml.PipelineModel = pipeline_f17db21b15b0\n"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df13 = rfAssembler.transform(df12)\n",
    "\n",
    "// Train model. This also runs the indexers.\n",
    "val rfModel_unbiased = rfPipeline.fit(df13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.6710878227171005\n",
      "Area under the receiver operating characteristic (ROC) curve : 0.6547396510697723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df14: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 187 more fields]\n",
       "testRf_unbiased: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 193 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_e0921f6ed999\n",
       "accuracy: Double = 0.6710878227171005\n",
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[1899] at map at <console>:75\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@4cd63015\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df14 = rfAssembler.transform(validation_df1)\n",
    "\n",
    "val testRf_unbiased = rfModel_unbiased.transform(df14)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(testRf_unbiased)\n",
    "println(\"Test set accuracy = \" + accuracy)\n",
    "\n",
    "val predictionLabelsRDD = testRf_unbiased.select(\"prediction\", \"indexedLabel\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)\n",
    "\n",
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4.4 Multilayer Perceptron Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfTrain_unbiased: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n",
       "dfTrain_unbiased: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n",
       "mlpcModel_unbiased: org.apache.spark.ml.PipelineModel = pipeline_c31d42af4b3a\n"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var dfTrain_unbiased = df12.withColumnRenamed(\"TARGET\", \"label\")\n",
    "dfTrain_unbiased = dfTrain_unbiased.drop(\"SK_ID_CURR\")\n",
    "\n",
    "var mlpcModel_unbiased = mlpcPipeline.fit(dfTrain_unbiased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.5241526117570001\n",
      "Area under the receiver operating characteristic (ROC) curve : 0.5059246854449672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfTest_unbiased: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n",
       "dfTest_unbiased: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n",
       "testMlpc_unbiased: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 191 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_cc0ff9419847\n",
       "accuracy: Double = 0.5241526117570001\n",
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[2244] at map at <console>:74\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@30778e0f\n"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var dfTest_unbiased = validation_df1.withColumnRenamed(\"TARGET\", \"label\")\n",
    "dfTest_unbiased = dfTest_unbiased.drop(\"SK_ID_CURR\")\n",
    "\n",
    "var testMlpc_unbiased = mlpcModel_unbiased.transform(dfTest_unbiased)\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "\n",
    "val accuracy = evaluator.evaluate(testMlpc_unbiased)\n",
    "println(\"Test set accuracy = \" + accuracy)\n",
    "\n",
    "val predictionLabelsRDD = testMlpc_unbiased.select(\"prediction\", \"indexedLabel\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)\n",
    "\n",
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4.5 Model Optimisation 2 - conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binomial Logistic Regression**\n",
    "- Accuracy: 69.51%\n",
    "- AUC ROC: 0.69\n",
    "\n",
    "**Random Forest**\n",
    "- Accuracy: 67.11%\n",
    "- AUC ROC: 0.65\n",
    "\n",
    "**Multilayer Perceptron Classifier**\n",
    "- Accuracy: 52.42%\n",
    "- AUC ROC: 0.51\n",
    "\n",
    "There were significant improvements to the metric AUC ROC for the logistic regression and random forest models. The models are now providing class separation of defaulting and non-defaulting customers.\n",
    "\n",
    "The optmisation has resulted in decreases in accuracy.  The appropriateness of trade-off of accuracy and AUC ROC in this optimisation would be determined with business considerations. The higher AUC ROC indicates we can predict potential defaults better but it leads to more false positives.  Would the reduced risk of taking on potential defaults compensate for the profits forfeited from turning down new loans?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the model fitting and optimisation, the best performing models are the ones fitted with unbiased datasets.\n",
    "- lrModel_unbiased\n",
    "- rfModel_unbiased\n",
    "- mlpcModel_unbiased\n",
    "\n",
    "These have been selected for testing with the hidden test dataset - test_df1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.692344883158743\n",
      "Area under the receiver operating characteristic (ROC) curve : 0.6889831920968008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testLr: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 191 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_7f619c3aba97\n",
       "accuracy: Double = 0.692344883158743\n",
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[2290] at map at <console>:71\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@55cb7833\n"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testLr = lrModel_unbiased.transform(test_df1)\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "\n",
    "val accuracy = evaluator.evaluate(testLr)\n",
    "println(\"Test set accuracy = \" + accuracy)\n",
    "\n",
    "val predictionLabelsRDD = testLr.select(\"prediction\", \"label\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)\n",
    "\n",
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.6846689895470384\n",
      "Area under the receiver operating characteristic (ROC) curve : 0.6531592643105513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df5: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 187 more fields]\n",
       "testRf: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 193 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_227784265861\n",
       "accuracy: Double = 0.6846689895470384\n",
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[2326] at map at <console>:77\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@4b42dcf5\n"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df5 = rfAssembler.transform(test_df1)\n",
    "\n",
    "val testRf = rfModel_unbiased.transform(df4)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(testRf)\n",
    "println(\"Test set accuracy = \" + accuracy)\n",
    "\n",
    "val predictionLabelsRDD = testRf.select(\"prediction\", \"indexedLabel\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)\n",
    "\n",
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 Multilayer Perceptron Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.5170024174053183\n",
      "Area under the receiver operating characteristic (ROC) curve : 0.509329619065342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfTest: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n",
       "dfTest: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n",
       "testMlpc: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 191 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_4cb4762162fc\n",
       "accuracy: Double = 0.5170024174053183\n",
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[2369] at map at <console>:76\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@78eaf4bd\n"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var dfTest = test_df1.withColumnRenamed(\"TARGET\", \"label\")\n",
    "dfTest = dfTest.drop(\"SK_ID_CURR\")\n",
    "\n",
    "var testMlpc = mlpcModel_unbiased.transform(dfTest)\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "\n",
    "val accuracy = evaluator.evaluate(testMlpc)\n",
    "println(\"Test set accuracy = \" + accuracy)\n",
    "\n",
    "val predictionLabelsRDD = testMlpc.select(\"prediction\", \"indexedLabel\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)\n",
    "\n",
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.4 Model testing conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binomial Logistic Regression**\n",
    "- Accuracy: 69.23%\n",
    "- AUC ROC: 0.69\n",
    "\n",
    "**Random Forest**\n",
    "- Accuracy: 68.47%\n",
    "- AUC ROC: 0.65\n",
    "\n",
    "**Multilayer Perceptron Classifier**\n",
    "- Accuracy: 51.70%\n",
    "- AUC ROC: 0.51\n",
    "\n",
    "Testing results are similar to those in validation.  The Binomial Logistic Regression provides the best results for both the Accuracy and AUC ROC metrics.  This is followed by Random Forest with comparable results and then the Multilayer Perceptron Classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
