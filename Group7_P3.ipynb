{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 7\n",
    "# Big Data Group Project\n",
    "\n",
    "## Contributors:\n",
    "- 29233798: Joel Shien Yee Chin [Joel]\n",
    "- 18435688: Tim O'Doherty [Tim]\n",
    "- 22606127: Sean Whitehead [Sean]\n",
    "- 29650437: Lin Bai [Lin]\n",
    "\n",
    "\n",
    "## Phase 3: Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 283 more fields]\n",
       "creditRiskdf2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 283 more fields]\n",
       "creditRiskdf3: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 283 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load wrangled datasets from Phase 2: part1.csv; part2.csv; part3.csv\n",
    "\n",
    "val creditRiskdf1 = spark.read.option(\"header\",\"true\").csv(\"part1.csv\")\n",
    "val creditRiskdf2 = spark.read.option(\"header\",\"true\").csv(\"part2.csv\")\n",
    "val creditRiskdf3 = spark.read.option(\"header\",\"true\").csv(\"part3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1and2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 283 more fields]\n",
       "creditRiskdf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 283 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// combine the 3 dataframes to 1\n",
    "val creditRiskdf1and2 = creditRiskdf1.union(creditRiskdf2)\n",
    "val creditRiskdf = creditRiskdf1and2.union(creditRiskdf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: creditRiskdf.type = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 283 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditRiskdf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-08 09:28:41,368 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "Total data rows in loaded dataframes:183875\n",
      "Total data rows in combined dataframe:183875\n"
     ]
    }
   ],
   "source": [
    "// check that the rows have been aggregated correctly\n",
    "\n",
    "println(\"Total data rows in loaded dataframes:\"+(\n",
    "    creditRiskdf1.count()+creditRiskdf2.count()+creditRiskdf3.count()))\n",
    "\n",
    "println(\"Total data rows in combined dataframe:\"+creditRiskdf.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**\n",
    "- creditRiskdf: loaded and consolidated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Data cleansing and reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "incon_droplist: List[String] = List(previous_loans_DAYS_FIRST_DUE_mean, previous_loans_SELLERPLACE_AREA_min, previous_loans_DAYS_FIRST_DUE_sum)\n",
       "df: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 280 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Drop features with inconsistent values from dataset.\n",
    "\n",
    "val incon_droplist = List(\"previous_loans_DAYS_FIRST_DUE_mean\",\n",
    "                      \"previous_loans_SELLERPLACE_AREA_min\",\n",
    "                      \"previous_loans_DAYS_FIRST_DUE_sum\")\n",
    "\n",
    "val df = creditRiskdf.drop(incon_droplist:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 280 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var creditRiskdf = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskFeatures: Array[String] = Array(EXT_SOURCE_2, EXT_SOURCE_3, client_installments_AMT_PAYMENT_min_sum, DAYS_BIRTH, AMT_CREDIT, AMT_ANNUITY, DAYS_EMPLOYED, bureau_DAYS_CREDIT_ENDDATE_max, bureau_DAYS_CREDIT_max, DAYS_ID_PUBLISH, bureau_DAYS_ENDDATE_FACT_max, bureau_AMT_CREDIT_SUM_DEBT_mean, previous_loans_CNT_PAYMENT_mean, client_cash_CNT_INSTALMENT_FUTURE_min_max, previous_loans_SELLERPLACE_AREA_max, DAYS_LAST_PHONE_CHANGE, bureau_AMT_CREDIT_SUM_max, bureau_AMT_CREDIT_SUM_mean, DAYS_REGISTRATION, client_installments_DAYS_INSTALMENT_max_max, previous_loans_AMT_DOWN_PAYMENT_max, CODE_GENDER_F, REGION_POPULATION_RELATIVE, client_cash_CNT_INSTALMENT_max_mean, bureau_DAYS_CREDIT_UPDATE_max, bureau_AMT_CREDIT_SUM_sum, client_installments_NUM_INSTALMENT_VERSION_mean_max, bureau_DAYS_CR..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// change datatype of data from String to Float\n",
    "var creditRiskFeatures = creditRiskdf.columns\n",
    "\n",
    "for (colName<-creditRiskFeatures){\n",
    "     |   creditRiskdf = creditRiskdf.withColumn(\n",
    "         colName,col(colName).cast(\"Float\"))\n",
    "     | }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EXT_SOURCE_2: float (nullable = true)\n",
      " |-- EXT_SOURCE_3: float (nullable = true)\n",
      " |-- client_installments_AMT_PAYMENT_min_sum: float (nullable = true)\n",
      " |-- DAYS_BIRTH: float (nullable = true)\n",
      " |-- AMT_CREDIT: float (nullable = true)\n",
      " |-- AMT_ANNUITY: float (nullable = true)\n",
      " |-- DAYS_EMPLOYED: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_ENDDATE_max: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_max: float (nullable = true)\n",
      " |-- DAYS_ID_PUBLISH: float (nullable = true)\n",
      " |-- bureau_DAYS_ENDDATE_FACT_max: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_DEBT_mean: float (nullable = true)\n",
      " |-- previous_loans_CNT_PAYMENT_mean: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_FUTURE_min_max: float (nullable = true)\n",
      " |-- previous_loans_SELLERPLACE_AREA_max: float (nullable = true)\n",
      " |-- DAYS_LAST_PHONE_CHANGE: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_max: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_mean: float (nullable = true)\n",
      " |-- DAYS_REGISTRATION: float (nullable = true)\n",
      " |-- client_installments_DAYS_INSTALMENT_max_max: float (nullable = true)\n",
      " |-- previous_loans_AMT_DOWN_PAYMENT_max: float (nullable = true)\n",
      " |-- CODE_GENDER_F: float (nullable = true)\n",
      " |-- REGION_POPULATION_RELATIVE: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_max_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_UPDATE_max: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_sum: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_mean_max: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_mean: float (nullable = true)\n",
      " |-- client_installments_AMT_PAYMENT_sum_mean: float (nullable = true)\n",
      " |-- client_installments_DAYS_INSTALMENT_min_max: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Refused_count_norm: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_max_max: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_FUTURE_min_mean: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_max_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_high_count_norm: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_sum: float (nullable = true)\n",
      " |-- client_installments_DAYS_ENTRY_PAYMENT_sum_max: float (nullable = true)\n",
      " |-- previous_loans_DAYS_FIRST_DUE_min: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_DEF_sum_max: float (nullable = true)\n",
      " |-- previous_loans_DAYS_DECISION_max: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_min_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_LIMIT_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_sum: float (nullable = true)\n",
      " |-- previous_loans_DAYS_LAST_DUE_1ST_VERSION_max: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash X-Sell: low_count_norm: float (nullable = true)\n",
      " |-- previous_loans_HOUR_APPR_PROCESS_START_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_DEBT_max: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_min: float (nullable = true)\n",
      " |-- previous_loans_AMT_DOWN_PAYMENT_mean: float (nullable = true)\n",
      " |-- NAME_FAMILY_STATUS_Married: float (nullable = true)\n",
      " |-- previous_loans_CNT_PAYMENT_max: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_mean_min: float (nullable = true)\n",
      " |-- AMT_INCOME_TOTAL: float (nullable = true)\n",
      " |-- previous_loans_DAYS_LAST_DUE_sum: float (nullable = true)\n",
      " |-- client_cash_MONTHS_BALANCE_max_mean: float (nullable = true)\n",
      " |-- NAME_EDUCATION_TYPE_Higher education: float (nullable = true)\n",
      " |-- client_cash_MONTHS_BALANCE_sum_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_ENDDATE_mean: float (nullable = true)\n",
      " |-- REGION_RATING_CLIENT: float (nullable = true)\n",
      " |-- previous_loans_AMT_GOODS_PRICE_min: float (nullable = true)\n",
      " |-- bureau_DAYS_ENDDATE_FACT_min: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_ENDDATE_min: float (nullable = true)\n",
      " |-- previous_loans_AMT_ANNUITY_mean: float (nullable = true)\n",
      " |-- client_installments_AMT_PAYMENT_max_min: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_min_max: float (nullable = true)\n",
      " |-- client_installments_AMT_PAYMENT_sum_min: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Mortgage_count_norm: float (nullable = true)\n",
      " |-- previous_loans_AMT_ANNUITY_max: float (nullable = true)\n",
      " |-- previous_loans_AMT_ANNUITY_min: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_max: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_low_normal_count_norm: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash X-Sell: high_count_norm: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_ENDDATE_sum: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Active_count: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_low_action_count_norm: float (nullable = true)\n",
      " |-- client_cash_counts_mean: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_mean_max: float (nullable = true)\n",
      " |-- client_cash_MONTHS_BALANCE_sum_max: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_sum_min: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Active_count_norm: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_UPDATE_min: float (nullable = true)\n",
      " |-- DEF_60_CNT_SOCIAL_CIRCLE: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_3: float (nullable = true)\n",
      " |-- bureau_DAYS_ENDDATE_FACT_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_LIMIT_max: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_max_sum: float (nullable = true)\n",
      " |-- previous_loans_AMT_GOODS_PRICE_mean: float (nullable = true)\n",
      " |-- client_installments_AMT_PAYMENT_max_sum: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_sum_mean: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_min_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_TYPE_SUITE_Unaccompanied_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_norm_mean: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_NUMBER_max_mean: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_max_max: float (nullable = true)\n",
      " |-- client_installments_DAYS_ENTRY_PAYMENT_max_min: float (nullable = true)\n",
      " |-- client_installments_AMT_PAYMENT_max_mean: float (nullable = true)\n",
      " |-- client_cash_counts_sum: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Approved_count_norm: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_min_mean: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_mean_min: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_sum_max: float (nullable = true)\n",
      " |-- previous_loans_AMT_ANNUITY_sum: float (nullable = true)\n",
      " |-- client_installments_AMT_PAYMENT_mean_sum: float (nullable = true)\n",
      " |-- previous_loans_DAYS_DECISION_mean: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash Street: low_count_norm: float (nullable = true)\n",
      " |-- previous_loans_DAYS_LAST_DUE_1ST_VERSION_mean: float (nullable = true)\n",
      " |-- HOUR_APPR_PROCESS_START: float (nullable = true)\n",
      " |-- previous_loans_DAYS_LAST_DUE_1ST_VERSION_min: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Consumer credit_count_norm: float (nullable = true)\n",
      " |-- previous_loans_HOUR_APPR_PROCESS_START_max: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_HC_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_PRODUCT_TYPE_walk-in_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CNT_PAYMENT_sum: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_UPDATE_mean: float (nullable = true)\n",
      " |-- previous_loans_DAYS_DECISION_sum: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS industry with interest_count_norm: float (nullable = true)\n",
      " |-- client_cash_MONTHS_BALANCE_max_sum: float (nullable = true)\n",
      " |-- previous_loans_AMT_CREDIT_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_CLIENT_TYPE_Refreshed_count_norm: float (nullable = true)\n",
      " |-- client_bureau_balance_MONTHS_BALANCE_mean_sum: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash Street: high_count_norm: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_DEBT_min: float (nullable = true)\n",
      " |-- NAME_CONTRACT_TYPE_Cash loans: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_min_sum: float (nullable = true)\n",
      " |-- previous_loans_AMT_APPLICATION_max: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_sum_max: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_min: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_mean_mean: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_sum_sum: float (nullable = true)\n",
      " |-- previous_loans_NAME_TYPE_SUITE_Family_count_norm: float (nullable = true)\n",
      " |-- previous_loans_DAYS_TERMINATION_mean: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_FUTURE_max_sum: float (nullable = true)\n",
      " |-- previous_loans_NAME_PAYMENT_TYPE_XNA_count_norm: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Credit card_count_norm: float (nullable = true)\n",
      " |-- client_installments_DAYS_ENTRY_PAYMENT_min_sum: float (nullable = true)\n",
      " |-- OBS_30_CNT_SOCIAL_CIRCLE: float (nullable = true)\n",
      " |-- client_installments_AMT_INSTALMENT_sum_sum: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_mean_sum: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_18: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_FUTURE_min_sum: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Credit and cash offices_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Contact center_count_norm: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Car loan_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_PORTFOLIO_POS_count_norm: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_NUMBER_min_mean: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_TUESDAY_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Furniture_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Completed_count_norm_max: float (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_QRT: float (nullable = true)\n",
      " |-- NAME_INCOME_TYPE_Working: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_norm_min: float (nullable = true)\n",
      " |-- previous_loans_AMT_DOWN_PAYMENT_min: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Microloan_count_norm: float (nullable = true)\n",
      " |-- previous_loans_DAYS_TERMINATION_min: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Channel of corporate sales_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_SUNDAY_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Stone_count_norm: float (nullable = true)\n",
      " |-- NAME_EDUCATION_TYPE_Secondary / secondary special: float (nullable = true)\n",
      " |-- client_bureau_balance_STATUS_0_count_sum: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_Core staff: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_middle_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Completed_count_norm_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_OVERDUE_max: float (nullable = true)\n",
      " |-- REG_CITY_NOT_LIVE_CITY: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Business Entity Type 3: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Closed_count: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Self-employed: float (nullable = true)\n",
      " |-- previous_loans_NAME_PORTFOLIO_Cash_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_WEDNESDAY_count_norm: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_mean_min: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Completed_count_sum: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_SCO_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Computers_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_XNA_count_norm: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS household without interest_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_PRODUCT_TYPE_XNA_count_norm: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_LIMIT_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_PRODUCT_TYPE_x-sell_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_MONDAY_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_SATURDAY_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_CLIENT_TYPE_New_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_AP+ (Cash loan)_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NFLAG_INSURED_ON_APPROVAL_mean: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Audio/Video_count_norm: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_mean_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Clothing and Accessories_count_norm: float (nullable = true)\n",
      " |-- DEF_30_CNT_SOCIAL_CIRCLE: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_NUMBER_mean_min: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_NUMBER_sum_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_PORTFOLIO_XNA_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_CLIENT_TYPE_Repeater_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_SELLER_INDUSTRY_Connectivity_count_norm: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Credit card_count: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Canceled_count_norm: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_Drivers: float (nullable = true)\n",
      " |-- previous_loans_HOUR_APPR_PROCESS_START_min: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Signed_count_norm_max: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_Laborers: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_min_max: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_THURSDAY_count_norm: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Construction: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_max: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_DEF_sum_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_TYPE_Cash loans_count_norm: float (nullable = true)\n",
      " |-- previous_loans_DAYS_FIRST_DRAWING_mean: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash X-Sell: middle_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_CASH_LOAN_PURPOSE_Other_count_norm: float (nullable = true)\n",
      " |-- client_bureau_balance_STATUS_0_count_norm_sum: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_sum_min: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Country-wide_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Signed_count_sum: float (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_YEAR: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Microloan_count: float (nullable = true)\n",
      " |-- NAME_INCOME_TYPE_State servant: float (nullable = true)\n",
      " |-- previous_loans_DAYS_TERMINATION_max: float (nullable = true)\n",
      " |-- FLAG_WORK_PHONE: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Photo / Cinema Equipment_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_FRIDAY_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_THURSDAY_count: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Military: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS household with interest_count_norm: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_min_sum: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Completed_count_norm_sum: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_16: float (nullable = true)\n",
      " |-- previous_loans_NAME_TYPE_SUITE_Spouse, partner_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Returned to the store_count_sum: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Industry: type 9: float (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_MON: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Approved_count: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Sold_count_norm: float (nullable = true)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |-- previous_loans_NAME_GOODS_CATEGORY_Mobile_count_norm: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_OVERDUE_mean: float (nullable = true)\n",
      " |-- previous_loans_NAME_SELLER_INDUSTRY_Construction_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_SCOFR_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_SELLER_INDUSTRY_Furniture_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_norm_max: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Construction Materials_count_norm: float (nullable = true)\n",
      " |-- client_bureau_balance_STATUS_1_count_norm_sum: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash Street: middle_count_norm: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS other with interest_count_norm: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_min_min: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_VERIF_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_LIMIT_count_norm: float (nullable = true)\n",
      " |-- client_bureau_balance_STATUS_X_count_norm_sum: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Consumer Electronics_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Regional / Local_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_SUNDAY_count: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_13: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_CLIENT_count_norm: float (nullable = true)\n",
      " |-- client_bureau_balance_STATUS_X_count_sum: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_middle_count: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_TYPE_Revolving loans_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_PRODUCT_TYPE_XNA_count: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Jewelry_count_norm: float (nullable = true)\n",
      " |-- WALLSMATERIAL_MODE_Stone, brick: float (nullable = true)\n",
      " |-- previous_loans_NAME_SELLER_INDUSTRY_Consumer electronics_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CNT_PAYMENT_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_high_count: float (nullable = true)\n",
      " |-- NAME_FAMILY_STATUS_Separated: float (nullable = true)\n",
      " |-- NAME_HOUSING_TYPE_House / apartment: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Signed_count_norm_mean: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Refused_count: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Card Street_count_norm: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_NUMBER_max_min: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS industry without interest_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Regional / Local_count: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Transport: type 3: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Returned to the store_count_norm_max: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_School: float (nullable = true)\n",
      " |-- previous_loans_NAME_TYPE_SUITE_Other_B_count_norm: float (nullable = true)\n",
      " |-- bureau_CNT_CREDIT_PROLONG_mean: float (nullable = true)\n",
      " |-- client_bureau_balance_MONTHS_BALANCE_max_sum: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_min: float (nullable = true)\n",
      " |-- WEEKDAY_APPR_PROCESS_START_WEDNESDAY: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_High skill tech staff: float (nullable = true)\n",
      " |-- previous_loans_NAME_PAYMENT_TYPE_XNA_count: float (nullable = true)\n",
      " |-- FLAG_PHONE: float (nullable = true)\n",
      " |-- TARGET: float (nullable = true)\n",
      " |-- SK_ID_CURR: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "creditRiskdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskFeatures: Array[String] = [Ljava.lang.String;@7ea45d01\n",
       "res4: Int = 281\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//string array of features without label \"TARGET\"\n",
    "\n",
    "creditRiskFeatures = creditRiskFeatures.filter(! _.contains(\"TARGET\"))\n",
    "creditRiskFeatures.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskFeatures1: Array[String] = Array(EXT_SOURCE_2, EXT_SOURCE_3, client_installments_AMT_PAYMENT_min_sum, DAYS_BIRTH, AMT_CREDIT, AMT_ANNUITY, DAYS_EMPLOYED, bureau_DAYS_CREDIT_ENDDATE_max, bureau_DAYS_CREDIT_max, DAYS_ID_PUBLISH, bureau_DAYS_ENDDATE_FACT_max, bureau_AMT_CREDIT_SUM_DEBT_mean, previous_loans_CNT_PAYMENT_mean, client_cash_CNT_INSTALMENT_FUTURE_min_max, previous_loans_SELLERPLACE_AREA_max, DAYS_LAST_PHONE_CHANGE, bureau_AMT_CREDIT_SUM_max, bureau_AMT_CREDIT_SUM_mean, DAYS_REGISTRATION, client_installments_DAYS_INSTALMENT_max_max, previous_loans_AMT_DOWN_PAYMENT_max, CODE_GENDER_F, REGION_POPULATION_RELATIVE, client_cash_CNT_INSTALMENT_max_mean, bureau_DAYS_CREDIT_UPDATE_max, bureau_AMT_CREDIT_SUM_sum, client_installments_NUM_INSTALMENT_VERSION_mean_max, bureau_DAYS_C..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// split features array to facilitate processing. Exclude feature SK_ID.\n",
    "\n",
    "val creditRiskFeatures1 = creditRiskFeatures.slice(0,39)\n",
    "val creditRiskFeatures2 = creditRiskFeatures.slice(40,79)\n",
    "val creditRiskFeatures3 = creditRiskFeatures.slice(80,119)\n",
    "val creditRiskFeatures4 = creditRiskFeatures.slice(120,159)\n",
    "val creditRiskFeatures5 = creditRiskFeatures.slice(160,199)\n",
    "val creditRiskFeatures6 = creditRiskFeatures.slice(200,239)\n",
    "val creditRiskFeatures7 = creditRiskFeatures.slice(240,280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rows: Long = 183875\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// check number of data lines in dataframe creditRiskdf\n",
    "\n",
    "val rows = creditRiskdf.select($\"TARGET\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 280 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// set up global variables\n",
    "\n",
    "var creditRiskdf1 = creditRiskdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run above cells up to this block if directly loading highOutlier and lowOutlier lists from text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "highOutlier_list: List[String] = List()\n",
       "lowOutlier_list: List[String] = List()\n",
       "feature_dropThreshold: Double = 9193.75\n",
       "row_dropLower: Double = 3677.5\n",
       "outliers: (feature_set: Array[String], creditRiskdf: org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//write function to check if there are any outliers in a given domain and show boxplot [Sean]\n",
    "//slice a small test list to ensure functionality\n",
    "//var test_lis = creditRiskFeatures.slice(0,2) //TEST var for codeblock\n",
    "\n",
    "var highOutlier_list = List[String]() //empty list to append features containing outliers in\n",
    "var lowOutlier_list = List[String]() //empty list to append features containing outliers in\n",
    "var feature_dropThreshold = 0.05*rows\n",
    "var row_dropLower = 0.02*rows\n",
    "\n",
    "def outliers(feature_set: Array[String],creditRiskdf:org.apache.spark.sql.DataFrame){    \n",
    "    for (feature <- feature_set) {\n",
    "        //summarise column and query the DataFrame output\n",
    "        var firstQ = creditRiskdf.select(feature).summary().where(\n",
    "            $\"summary\" === \"25%\").select(feature).first().mkString.toFloat\n",
    "        var thirdQ = creditRiskdf.select(feature).summary().where(\n",
    "            $\"summary\" === \"75%\").select(feature).first().mkString.toFloat\n",
    "        //use formulas below to test threshold of outliers\n",
    "        var testValHigh = thirdQ + (1.5 * (thirdQ - firstQ))\n",
    "        var testValLow = firstQ - (1.5 * (thirdQ - firstQ))\n",
    "        if(testValHigh == 0 & testValLow == 0){}\n",
    "        //check to see if thresholds are exceeded in the column and count\n",
    "        else{\n",
    "            var outHigh = creditRiskdf.select(col(feature)).filter(\n",
    "                col(feature) > lit(testValHigh)).count()\n",
    "            var outLow = creditRiskdf.select(col(feature)).filter(\n",
    "                col(feature) < lit(testValLow)).count()\n",
    "            // collate a list of features with outliers > 5% of total data rows\n",
    "            if (outHigh + outLow >= feature_dropThreshold ){       \n",
    "                highOutlier_list = feature :: highOutlier_list\n",
    "            }\n",
    "            // collate a list of features with outliers 2%-5% of total data rows    \n",
    "            else if (outHigh + outLow > 0.02*rows & outHigh + outLow < 0.05*rows){\n",
    "                lowOutlier_list = feature :: lowOutlier_list\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colNames: Array[org.apache.spark.sql.Column] = Array(EXT_SOURCE_2, EXT_SOURCE_3, client_installments_AMT_PAYMENT_min_sum, DAYS_BIRTH, AMT_CREDIT, AMT_ANNUITY, DAYS_EMPLOYED, bureau_DAYS_CREDIT_ENDDATE_max, bureau_DAYS_CREDIT_max, DAYS_ID_PUBLISH, bureau_DAYS_ENDDATE_FACT_max, bureau_AMT_CREDIT_SUM_DEBT_mean, previous_loans_CNT_PAYMENT_mean, client_cash_CNT_INSTALMENT_FUTURE_min_max, previous_loans_SELLERPLACE_AREA_max, DAYS_LAST_PHONE_CHANGE, bureau_AMT_CREDIT_SUM_max, bureau_AMT_CREDIT_SUM_mean, DAYS_REGISTRATION, client_installments_DAYS_INSTALMENT_max_max, previous_loans_AMT_DOWN_PAYMENT_max, CODE_GENDER_F, REGION_POPULATION_RELATIVE, client_cash_CNT_INSTALMENT_max_mean, bureau_DAYS_CREDIT_UPDATE_max, bureau_AMT_CREDIT_SUM_sum, client_installments_NUM_INSTALMENT_VERSION_mean_max, bur..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// processing outliers list 1 of 7\n",
    "\n",
    "val colNames = creditRiskFeatures1.map(col(_))\n",
    "var creditRiskdf1_1 = creditRiskdf1.select(colNames:_*)\n",
    "creditRiskdf1_1.cache()\n",
    "outliers(creditRiskFeatures1, creditRiskdf1_1)\n",
    "println(highOutlier_list.length)\n",
    "println(lowOutlier_list.length)\n",
    "\n",
    "creditRiskdf1_1.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colNames: Array[org.apache.spark.sql.Column] = Array(previous_loans_DAYS_DECISION_max, client_installments_AMT_INSTALMENT_min_mean, bureau_AMT_CREDIT_SUM_LIMIT_mean, bureau_DAYS_CREDIT_sum, previous_loans_DAYS_LAST_DUE_1ST_VERSION_max, previous_loans_PRODUCT_COMBINATION_Cash X-Sell: low_count_norm, previous_loans_HOUR_APPR_PROCESS_START_mean, bureau_AMT_CREDIT_SUM_DEBT_max, bureau_AMT_CREDIT_SUM_min, previous_loans_AMT_DOWN_PAYMENT_mean, NAME_FAMILY_STATUS_Married, previous_loans_CNT_PAYMENT_max, client_installments_AMT_INSTALMENT_mean_min, AMT_INCOME_TOTAL, previous_loans_DAYS_LAST_DUE_sum, client_cash_MONTHS_BALANCE_max_mean, NAME_EDUCATION_TYPE_Higher education, client_cash_MONTHS_BALANCE_sum_mean, bureau_DAYS_CREDIT_ENDDATE_mean, REGION_RATING_CLIENT, previous_loans_AMT_GOODS_PRICE_..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// processing outliers list 2 of 7\n",
    "\n",
    "val colNames = creditRiskFeatures2.map(col(_))\n",
    "var creditRiskdf1_2 = creditRiskdf1.select(colNames:_*)\n",
    "creditRiskdf1_2.cache()\n",
    "outliers(creditRiskFeatures2, creditRiskdf1_2)\n",
    "println(highOutlier_list.length)\n",
    "println(lowOutlier_list.length)\n",
    "creditRiskdf1_2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colNames: Array[org.apache.spark.sql.Column] = Array(bureau_CREDIT_ACTIVE_Active_count_norm, bureau_DAYS_CREDIT_UPDATE_min, DEF_60_CNT_SOCIAL_CIRCLE, FLAG_DOCUMENT_3, bureau_DAYS_ENDDATE_FACT_mean, bureau_AMT_CREDIT_SUM_LIMIT_max, client_cash_SK_DPD_max_sum, previous_loans_AMT_GOODS_PRICE_mean, client_installments_AMT_PAYMENT_max_sum, client_installments_NUM_INSTALMENT_VERSION_sum_mean, client_installments_AMT_INSTALMENT_min_min, previous_loans_NAME_TYPE_SUITE_Unaccompanied_count_norm, client_cash_NAME_CONTRACT_STATUS_Active_count_norm_mean, client_installments_NUM_INSTALMENT_NUMBER_max_mean, client_cash_CNT_INSTALMENT_max_max, client_installments_DAYS_ENTRY_PAYMENT_max_min, client_installments_AMT_PAYMENT_max_mean, client_cash_counts_sum, previous_loans_NAME_CONTRACT_STATUS_Approved_co..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// processing outliers list 3 of 7\n",
    "\n",
    "val colNames = creditRiskFeatures3.map(col(_))\n",
    "var creditRiskdf1_3 = creditRiskdf1.select(colNames:_*)\n",
    "creditRiskdf1_3.cache()\n",
    "outliers(creditRiskFeatures3, creditRiskdf1_3)\n",
    "println(highOutlier_list.length)\n",
    "println(lowOutlier_list.length)\n",
    "creditRiskdf1_3.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.io._\n",
       "file1: String = highOutlier_list.txt\n",
       "file2: String = lowOutlier_list.txt\n",
       "writer1: java.io.BufferedWriter = java.io.BufferedWriter@1db7651f\n",
       "writer2: java.io.BufferedWriter = java.io.BufferedWriter@5d2426f6\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// persist storage of intermediate lists\n",
    "\n",
    "import java.io._\n",
    "\n",
    "val file1 = \"highOutlier_list.txt\"\n",
    "val file2 = \"lowOutlier_list.txt\"\n",
    "val writer1 = new BufferedWriter(new FileWriter(file1))\n",
    "highOutlier_list.map(_+\"\\n\").foreach(writer1.write)\n",
    "writer1.close()\n",
    "val writer2 = new BufferedWriter(new FileWriter(file2))\n",
    "lowOutlier_list.map(_+\"\\n\").foreach(writer2.write)\n",
    "writer2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.io.Source\n",
       "highOutlier_list: List[String] = List(previous_loans_AMT_CREDIT_min, previous_loans_CNT_PAYMENT_sum, previous_loans_DAYS_LAST_DUE_1ST_VERSION_mean, client_installments_AMT_PAYMENT_mean_sum, previous_loans_AMT_ANNUITY_sum, client_installments_AMT_INSTALMENT_sum_max, client_cash_CNT_INSTALMENT_mean_min, client_cash_counts_sum, client_installments_AMT_PAYMENT_max_mean, client_cash_CNT_INSTALMENT_max_max, client_installments_NUM_INSTALMENT_NUMBER_max_mean, client_installments_AMT_INSTALMENT_min_min, client_installments_NUM_INSTALMENT_VERSION_sum_mean, client_installments_AMT_PAYMENT_max_sum, previous_loans_AMT_GOODS_PRICE_mean, client_cash_MONTHS_BALANCE_sum_max, client_installments_AMT_INSTALMENT_mean_max, bureau_DAYS_CREDIT_ENDDATE_sum, previous_loans_NAME_YIELD_GRO..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// load intermediate lists\n",
    "import scala.io.Source\n",
    "\n",
    "var highOutlier_list = Source.fromFile(\"highOutlier_list.txt\").getLines.toList\n",
    "var lowOutlier_list = Source.fromFile(\"lowOutlier_list.txt\").getLines.toList\n",
    "var feature_dropThreshold = 0.05*rows\n",
    "var row_dropLower = 0.02*rows\n",
    "\n",
    "def outliers(feature_set: Array[String],creditRiskdf:org.apache.spark.sql.DataFrame){    \n",
    "    for (feature <- feature_set) {\n",
    "        //summarise column and query the DataFrame output\n",
    "        var firstQ = creditRiskdf.select(feature).summary().where(\n",
    "            $\"summary\" === \"25%\").select(feature).first().mkString.toFloat\n",
    "        var thirdQ = creditRiskdf.select(feature).summary().where(\n",
    "            $\"summary\" === \"75%\").select(feature).first().mkString.toFloat\n",
    "        //use formulas below to test threshold of outliers\n",
    "        var testValHigh = thirdQ + (1.5 * (thirdQ - firstQ))\n",
    "        var testValLow = firstQ - (1.5 * (thirdQ - firstQ))\n",
    "        if(testValHigh == 0 & testValLow == 0){}\n",
    "        //check to see if thresholds are exceeded in the column and count\n",
    "        else{\n",
    "            var outHigh = creditRiskdf.select(col(feature)).filter(\n",
    "                col(feature) > lit(testValHigh)).count()\n",
    "            var outLow = creditRiskdf.select(col(feature)).filter(\n",
    "                col(feature) < lit(testValLow)).count()\n",
    "            // collate a list of features with outliers > 5% of total data rows\n",
    "            if (outHigh + outLow >= feature_dropThreshold ){       \n",
    "                highOutlier_list = feature :: highOutlier_list\n",
    "            }\n",
    "            // collate a list of features with outliers 2%-5% of total data rows    \n",
    "            else if (outHigh + outLow > 0.02*rows & outHigh + outLow < 0.05*rows){\n",
    "                lowOutlier_list = feature :: lowOutlier_list\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colNames: Array[org.apache.spark.sql.Column] = Array(client_bureau_balance_MONTHS_BALANCE_mean_sum, previous_loans_PRODUCT_COMBINATION_Cash Street: high_count_norm, previous_loans_RATE_DOWN_PAYMENT_mean, bureau_AMT_CREDIT_SUM_DEBT_min, NAME_CONTRACT_TYPE_Cash loans, client_installments_AMT_INSTALMENT_min_sum, previous_loans_AMT_APPLICATION_max, client_installments_NUM_INSTALMENT_VERSION_sum_max, previous_loans_RATE_DOWN_PAYMENT_min, client_installments_AMT_INSTALMENT_mean_mean, client_installments_NUM_INSTALMENT_VERSION_sum_sum, previous_loans_NAME_TYPE_SUITE_Family_count_norm, previous_loans_DAYS_TERMINATION_mean, client_cash_CNT_INSTALMENT_FUTURE_max_sum, previous_loans_NAME_PAYMENT_TYPE_XNA_count_norm, bureau_CREDIT_TYPE_Credit card_count_norm, client_installments_DAYS_ENTRY_PAYMENT_..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// processing outliers list 4 of 7\n",
    "\n",
    "val colNames = creditRiskFeatures4.map(col(_))\n",
    "var creditRiskdf1_4 = creditRiskdf1.select(colNames:_*)\n",
    "creditRiskdf1_4.cache()\n",
    "outliers(creditRiskFeatures4, creditRiskdf1_4)\n",
    "println(highOutlier_list.length)\n",
    "println(lowOutlier_list.length)\n",
    "creditRiskdf1_4.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colNames: Array[org.apache.spark.sql.Column] = Array(client_bureau_balance_STATUS_0_count_sum, OCCUPATION_TYPE_Core staff, previous_loans_NAME_YIELD_GROUP_middle_count_norm, client_cash_NAME_CONTRACT_STATUS_Completed_count_norm_mean, bureau_AMT_CREDIT_SUM_OVERDUE_max, REG_CITY_NOT_LIVE_CITY, ORGANIZATION_TYPE_Business Entity Type 3, bureau_CREDIT_ACTIVE_Closed_count, ORGANIZATION_TYPE_Self-employed, previous_loans_NAME_PORTFOLIO_Cash_count_norm, previous_loans_WEEKDAY_APPR_PROCESS_START_WEDNESDAY_count_norm, client_installments_NUM_INSTALMENT_VERSION_mean_min, client_cash_NAME_CONTRACT_STATUS_Completed_count_sum, previous_loans_CODE_REJECT_REASON_SCO_count_norm, previous_loans_NAME_GOODS_CATEGORY_Computers_count_norm, previous_loans_NAME_YIELD_GROUP_XNA_count_norm, previous_loans_PRODUC..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// processing outliers list 5 of 7\n",
    "\n",
    "val colNames = creditRiskFeatures5.map(col(_))\n",
    "var creditRiskdf1_5 = creditRiskdf1.select(colNames:_*)\n",
    "creditRiskdf1_5.cache()\n",
    "outliers(creditRiskFeatures5, creditRiskdf1_5)\n",
    "println(highOutlier_list.length)\n",
    "println(lowOutlier_list.length)\n",
    "creditRiskdf1_5.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colNames: Array[org.apache.spark.sql.Column] = Array(client_cash_CNT_INSTALMENT_min_max, previous_loans_WEEKDAY_APPR_PROCESS_START_THURSDAY_count_norm, ORGANIZATION_TYPE_Construction, client_cash_NAME_CONTRACT_STATUS_Active_count_max, client_cash_SK_DPD_DEF_sum_min, previous_loans_NAME_CONTRACT_TYPE_Cash loans_count_norm, previous_loans_DAYS_FIRST_DRAWING_mean, previous_loans_PRODUCT_COMBINATION_Cash X-Sell: middle_count_norm, previous_loans_NAME_CASH_LOAN_PURPOSE_Other_count_norm, client_bureau_balance_STATUS_0_count_norm_sum, client_installments_NUM_INSTALMENT_VERSION_sum_min, previous_loans_CHANNEL_TYPE_Country-wide_count_norm, client_cash_NAME_CONTRACT_STATUS_Signed_count_sum, AMT_REQ_CREDIT_BUREAU_YEAR, bureau_CREDIT_TYPE_Microloan_count, NAME_INCOME_TYPE_State servant, previous_lo..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// processing outliers list 6 of 7\n",
    "\n",
    "val colNames = creditRiskFeatures6.map(col(_))\n",
    "var creditRiskdf1_6 = creditRiskdf1.select(colNames:_*)\n",
    "creditRiskdf1_6.cache()\n",
    "outliers(creditRiskFeatures6, creditRiskdf1_6)\n",
    "println(highOutlier_list.length)\n",
    "println(lowOutlier_list.length)\n",
    "creditRiskdf1_6.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colNames: Array[org.apache.spark.sql.Column] = Array(previous_loans_PRODUCT_COMBINATION_Cash Street: middle_count_norm, previous_loans_PRODUCT_COMBINATION_POS other with interest_count_norm, client_cash_CNT_INSTALMENT_min_min, previous_loans_CODE_REJECT_REASON_VERIF_count_norm, previous_loans_CODE_REJECT_REASON_LIMIT_count_norm, client_bureau_balance_STATUS_X_count_norm_sum, previous_loans_PRODUCT_COMBINATION_Cash_count_norm, previous_loans_NAME_GOODS_CATEGORY_Consumer Electronics_count_norm, previous_loans_CHANNEL_TYPE_Regional / Local_count_norm, previous_loans_WEEKDAY_APPR_PROCESS_START_SUNDAY_count, FLAG_DOCUMENT_13, previous_loans_CODE_REJECT_REASON_CLIENT_count_norm, client_bureau_balance_STATUS_X_count_sum, previous_loans_NAME_YIELD_GROUP_middle_count, previous_loans_NAME_CONTRAC..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// processing outliers list 7 of 7\n",
    "\n",
    "val colNames = creditRiskFeatures7.map(col(_))\n",
    "var creditRiskdf1_7 = creditRiskdf1.select(colNames:_*)\n",
    "creditRiskdf1_7.cache()\n",
    "outliers(creditRiskFeatures7, creditRiskdf1_7)\n",
    "println(highOutlier_list.length)\n",
    "println(lowOutlier_list.length)\n",
    "creditRiskdf1_7.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.io._\n",
       "file1: String = highOutlier_FinalList.txt\n",
       "file2: String = lowOutlier_FinalList.txt\n",
       "writer1: java.io.BufferedWriter = java.io.BufferedWriter@bb849c\n",
       "writer2: java.io.BufferedWriter = java.io.BufferedWriter@7f253bd8\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// persist storage of intermediate lists\n",
    "\n",
    "import java.io._\n",
    "\n",
    "val file1 = \"highOutlier_FinalList.txt\"\n",
    "val file2 = \"lowOutlier_FinalList.txt\"\n",
    "val writer1 = new BufferedWriter(new FileWriter(file1))\n",
    "highOutlier_list.map(_+\"\\n\").foreach(writer1.write)\n",
    "writer1.close()\n",
    "val writer2 = new BufferedWriter(new FileWriter(file2))\n",
    "lowOutlier_list.map(_+\"\\n\").foreach(writer2.write)\n",
    "writer2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue from cells below if directly loading highOutlier and lowOutlier lists from text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.io.Source\n",
       "highOutlier_list: List[String] = List(client_bureau_balance_MONTHS_BALANCE_max_sum, previous_loans_NAME_CONTRACT_STATUS_Refused_count, NAME_HOUSING_TYPE_House / apartment, previous_loans_NAME_PRODUCT_TYPE_XNA_count, previous_loans_NAME_CONTRACT_TYPE_Revolving loans_count_norm, client_bureau_balance_STATUS_X_count_sum, previous_loans_NAME_GOODS_CATEGORY_Consumer Electronics_count_norm, client_bureau_balance_STATUS_X_count_norm_sum, previous_loans_NAME_GOODS_CATEGORY_Mobile_count_norm, previous_loans_PRODUCT_COMBINATION_POS household with interest_count_norm, previous_loans_WEEKDAY_APPR_PROCESS_START_THURSDAY_count, client_bureau_balance_STATUS_0_count_norm_sum, previous_loans_PRODUCT_COMBINATION_Cash X-Sell: middle_count_norm, previous_loans_DAYS_FIRST_DRAWING_mean..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source\n",
    "\n",
    "var highOutlier_list = Source.fromFile(\"highOutlier_FinalList.txt\").getLines.toList\n",
    "var lowOutlier_list = Source.fromFile(\"lowOutlier_FinalList.txt\").getLines.toList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res5: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 280 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(highOutlier_list.length)\n",
    "println(lowOutlier_list.length)\n",
    "creditRiskdf.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded lists has identical rows to outliers variables processed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 280 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditRiskdf1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// drop features in highOutlier_list\n",
    "creditRiskdf1 = creditRiskdf1.drop(highOutlier_list:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183875\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "println(creditRiskdf1.count())\n",
    "println(creditRiskdf1.columns.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Output**\n",
    "creditRiskdf1 - cleansed dataset with features with outliers >5% removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var creditRiskdf2 = creditRiskdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditRiskdf2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "//code block takes features in lowOutlier_list and filters main dataframe against them\n",
    "//code will generate a subset of main dataset without outliers.\n",
    "\n",
    "//input feature list conatining features with outliers persisting\n",
    "\n",
    "for (feature <- lowOutlier_list){\n",
    "    //summarise column and query the DataFrame output\n",
    "    var firstQ = creditRiskdf2.select(feature).summary().where(\n",
    "        $\"summary\" === \"25%\").select(feature).first().mkString.toFloat\n",
    "    var thirdQ = creditRiskdf2.select(feature).summary().where(\n",
    "        $\"summary\" === \"75%\").select(feature).first().mkString.toFloat\n",
    "    //use formulas below to test threshold of outliers\n",
    "    var testValHigh = thirdQ + (1.5 * (thirdQ - firstQ))\n",
    "    var testValLow = firstQ - (1.5 * (thirdQ - firstQ))\n",
    "    if (testValHigh == 0 & testValLow == 0){}\n",
    "    else {\n",
    "        creditRiskdf2 = creditRiskdf2.filter(col(feature) <= lit(testValHigh)).filter(\n",
    "            col(feature) > lit(testValLow))        \n",
    "    }  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35583\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "println(creditRiskdf2.count())\n",
    "println(creditRiskdf2.columns.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Output**\n",
    "creditRiskdf2 - cleansed dataset with features with outliers >5% removed and data rows for features with outliers between 2-5% removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditRiskdf1.write.option(\"header\", \"true\").csv(\"creditRiskdf1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditRiskdf2.write.option(\"header\", \"true\").csv(\"creditRiskdf2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditRiskdf1.unpersist()\n",
    "creditRiskdf2.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the csv files to skip the outlier processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1_1: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "creditRiskdf1_2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "creditRiskdf1_3: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// Load creditRiskdf1 datasets: which exclude features with outliers >5%\n",
    "\n",
    "val creditRiskdf1_1 = spark.read.option(\"header\",\"true\").csv(\n",
    "    \"creditRiskdf1_1.csv\")\n",
    "val creditRiskdf1_2 = spark.read.option(\"header\",\"true\").csv(\n",
    "    \"creditRiskdf1_2.csv\")\n",
    "val creditRiskdf1_3 = spark.read.option(\"header\",\"true\").csv(\n",
    "    \"creditRiskdf1_3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1and2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "creditRiskdf1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// combine the 3 dataframes to 1\n",
    "\n",
    "val creditRiskdf1and2 = creditRiskdf1_1.union(creditRiskdf1_2)\n",
    "var creditRiskdf1 = creditRiskdf1and2.union(creditRiskdf1_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf2_1: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "creditRiskdf2_2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "creditRiskdf2_3: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// Load creditRiskdf2 datasets: which exclude features with outliers >5% \n",
    "// and datarows of features with outliers between 2-5%\n",
    "\n",
    "val creditRiskdf2_1 = spark.read.option(\"header\",\"true\").csv(\n",
    "    \"creditRiskdf2_1.csv\")\n",
    "val creditRiskdf2_2 = spark.read.option(\"header\",\"true\").csv(\n",
    "    \"creditRiskdf2_2.csv\")\n",
    "val creditRiskdf2_3 = spark.read.option(\"header\",\"true\").csv(\n",
    "    \"creditRiskdf2_3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1and2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n",
       "creditRiskdf2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// combine the 3 dataframes to 1\n",
    "\n",
    "val creditRiskdf1and2 = creditRiskdf2_1.union(creditRiskdf2_2)\n",
    "var creditRiskdf2= creditRiskdf1and2.union(creditRiskdf2_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 186 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "creditRiskdf1.cache()\n",
    "creditRiskdf2.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creditRiskdf1 structure:\n",
      "183875\n",
      "188\n",
      "creditRiskdf2 structure:\n",
      "35583\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "\n",
    "println(\"creditRiskdf1 structure:\")\n",
    "println(creditRiskdf1.count())\n",
    "println(creditRiskdf1.columns.size)\n",
    "println(\"creditRiskdf2 structure:\")\n",
    "println(creditRiskdf2.count())\n",
    "println(creditRiskdf2.columns.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskFeatures1: Array[String] = Array(EXT_SOURCE_2, EXT_SOURCE_3, DAYS_BIRTH, AMT_CREDIT, AMT_ANNUITY, DAYS_ID_PUBLISH, previous_loans_CNT_PAYMENT_mean, previous_loans_SELLERPLACE_AREA_max, DAYS_LAST_PHONE_CHANGE, DAYS_REGISTRATION, CODE_GENDER_F, REGION_POPULATION_RELATIVE, client_cash_CNT_INSTALMENT_max_mean, bureau_DAYS_CREDIT_mean, previous_loans_NAME_CONTRACT_STATUS_Refused_count_norm, client_installments_NUM_INSTALMENT_VERSION_max_mean, bureau_DAYS_CREDIT_min, previous_loans_NAME_YIELD_GROUP_high_count_norm, previous_loans_RATE_DOWN_PAYMENT_sum, previous_loans_DAYS_FIRST_DUE_min, client_cash_SK_DPD_DEF_sum_max, bureau_AMT_CREDIT_SUM_LIMIT_mean, bureau_DAYS_CREDIT_sum, previous_loans_DAYS_LAST_DUE_1ST_VERSION_max, previous_loans_PRODUCT_COMBINATION_Cash X-Sell: low_count_norm,..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// change datatype of data from String to Float\n",
    "var creditRiskFeatures1 = creditRiskdf1.columns\n",
    "var creditRiskFeatures2 = creditRiskdf2.columns\n",
    "\n",
    "for (colName<-creditRiskFeatures1){\n",
    "     |   creditRiskdf1 = creditRiskdf1.withColumn(\n",
    "         colName,col(colName).cast(\"Float\"))\n",
    "     | }\n",
    "\n",
    "for (colName<-creditRiskFeatures2){\n",
    "     |   creditRiskdf2 = creditRiskdf2.withColumn(\n",
    "         colName,col(colName).cast(\"Float\"))\n",
    "     | }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Outputs**\n",
    "- creditRiskdf1: Cleansed dataset - features with outliers > 5% data rows removed.\n",
    "- creditRiskdf2 - cleansed dataset with features with outliers >5% removed and data rows for features with outliers between 2-5% removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Prepare training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_df1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "test_df1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "validation_df1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "train_df2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "test_df2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "validation_df2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// random split of cleansed dataset to training set (70%), testing set (20%)\n",
    "// and validation set (10%)\n",
    "\n",
    "var Array(train_df1,test_df1,validation_df1)=creditRiskdf1.randomSplit(\n",
    "    Array(0.7,0.2,0.1))\n",
    "var Array(train_df2,test_df2,validation_df2)=creditRiskdf2.randomSplit(\n",
    "    Array(0.7,0.2,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditRiskdf1.unpersist()\n",
    "creditRiskdf2.unpersist()\n",
    "train_df1.cache()\n",
    "test_df1.cache()\n",
    "validation_df1.cache()\n",
    "train_df2.cache()\n",
    "test_df2.cache()\n",
    "validation_df2.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outputs**\n",
    "- train_df1: to be used for fitting model1.\n",
    "- validation_df1: to be used for evaluating model1.\n",
    "- test_df1: to be kept hidden throughout fitting process and used to test the final model1.\n",
    "- train_df2: to be used for fitting model2.\n",
    "- validation_df2: to be used for evaluating model2.\n",
    "- test_df2: to be kept hidden throughout fitting process and used to test the final model2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Model fitting and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2.1 Binomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_df1Features: Array[String] = Array(EXT_SOURCE_2, EXT_SOURCE_3, DAYS_BIRTH, AMT_CREDIT, AMT_ANNUITY, DAYS_ID_PUBLISH, previous_loans_CNT_PAYMENT_mean, previous_loans_SELLERPLACE_AREA_max, DAYS_LAST_PHONE_CHANGE, DAYS_REGISTRATION, CODE_GENDER_F, REGION_POPULATION_RELATIVE, client_cash_CNT_INSTALMENT_max_mean, bureau_DAYS_CREDIT_mean, previous_loans_NAME_CONTRACT_STATUS_Refused_count_norm, client_installments_NUM_INSTALMENT_VERSION_max_mean, bureau_DAYS_CREDIT_min, previous_loans_NAME_YIELD_GROUP_high_count_norm, previous_loans_RATE_DOWN_PAYMENT_sum, previous_loans_DAYS_FIRST_DUE_min, client_cash_SK_DPD_DEF_sum_max, bureau_AMT_CREDIT_SUM_LIMIT_mean, bureau_DAYS_CREDIT_sum, previous_loans_DAYS_LAST_DUE_1ST_VERSION_max, previous_loans_PRODUCT_COMBINATION_Cash X-Sell: low_count_norm, p..."
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// collate arrays of column names with and without the label \"TARGET\"\n",
    "\n",
    "val train_df1Features = train_df1.columns\n",
    "val features = train_df1Features.filter(!_.contains(\"TARGET\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_2195ae5ae9dc\n"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// place all features except for \"TARGET\" under column name \"features\"\n",
    "\n",
    "val lrAssembler = new VectorAssembler().setInputCols(features).setOutputCol(\n",
    "    \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrLabelIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_02c37c1a0064\n"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// identify label for logistic regression\n",
    "\n",
    "val lrLabelIndexer = new StringIndexer().setInputCol(\"TARGET\").setOutputCol(\n",
    "    \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelLr: org.apache.spark.ml.classification.LogisticRegression = logreg_82d976c70da7\n"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// fit logistic model\n",
    "\n",
    "val modelLr = new LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare pipeline for future validation, optimisation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrPipeline: org.apache.spark.ml.Pipeline = pipeline_99b04f127e7e\n"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrPipeline = new Pipeline().setStages(Array(lrLabelIndexer, lrAssembler, modelLr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-08 09:35:15,072 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2019-06-08 09:35:15,077 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lrModel: org.apache.spark.ml.PipelineModel = pipeline_3ab0fcccf979\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModel = lrPipeline.fit(train_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the metric to evaluate the model, e.g. ROC AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict using validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+--------------------+----------+\n",
      "|SK_ID_CURR|            features|label|         probability|prediction|\n",
      "+----------+--------------------+-----+--------------------+----------+\n",
      "|  115467.0|(187,[0,1,2,3,4,5...|  0.0|[0.85171205996034...|       0.0|\n",
      "|  145702.0|(187,[0,1,2,3,4,5...|  0.0|[0.81270010056785...|       0.0|\n",
      "|  218200.0|(187,[0,1,2,3,4,5...|  0.0|[0.52112132627263...|       0.0|\n",
      "|  202641.0|(187,[0,1,2,3,4,5...|  1.0|[0.75660358947379...|       0.0|\n",
      "|  213147.0|(187,[0,1,2,3,4,5...|  1.0|[0.45392060778981...|       1.0|\n",
      "|  179975.0|(187,[0,1,2,3,4,5...|  0.0|[0.86612244056297...|       0.0|\n",
      "|  126095.0|(187,[0,1,2,3,4,5...|  0.0|[0.91784269329172...|       0.0|\n",
      "|  176593.0|(187,[0,1,2,3,4,5...|  0.0|[0.79318491001957...|       0.0|\n",
      "|  139376.0|(187,[0,1,2,3,4,5...|  0.0|[0.89295530536230...|       0.0|\n",
      "|  200184.0|(187,[0,1,2,3,4,5...|  0.0|[0.56096322783316...|       0.0|\n",
      "|  161901.0|(187,[0,1,2,3,4,5...|  0.0|[0.80255052276530...|       0.0|\n",
      "|  161215.0|(187,[0,1,2,3,4,5...|  0.0|[0.81705667052757...|       0.0|\n",
      "|  164916.0|(187,[0,1,2,3,4,5...|  0.0|[0.40212096600584...|       1.0|\n",
      "|  198635.0|(187,[0,1,2,3,4,5...|  1.0|[0.89116817444470...|       0.0|\n",
      "|  206971.0|(187,[0,1,2,3,4,5...|  0.0|[0.91842003912108...|       0.0|\n",
      "|  186659.0|(187,[0,1,2,3,4,5...|  1.0|[0.90102951239618...|       0.0|\n",
      "|  202779.0|(187,[0,1,2,3,4,5...|  1.0|[0.71055752626153...|       0.0|\n",
      "|  150707.0|(187,[0,1,2,3,4,5...|  0.0|[0.94679969970106...|       0.0|\n",
      "|  220234.0|(187,[0,1,2,3,4,5...|  0.0|[0.53081700989977...|       0.0|\n",
      "|  218501.0|(187,[0,1,2,3,4,5...|  0.0|[0.91479548561195...|       0.0|\n",
      "+----------+--------------------+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions1: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 191 more fields]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions1 = lrModel.transform(validation_df1)\n",
    "predictions1.select (\"SK_ID_CURR\",\"features\", \"label\",\"probability\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "// println(s\"Coefficients: ${model.coefficients} Intercept: ${model.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy measurement 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.NaiveBayes\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.sql.SparkSession\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.NaiveBayes\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.930598513212871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_57cf9616c82e\n",
       "accuracy: Double = 0.930598513212871\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions1)\n",
    "println(\"Test set accuracy = \" + accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy of our prediction is over 93.42%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t1: Float = 1276.0\n",
       "t0: Float = 17153.0\n",
       "t1_percent: Float = 0.0692387\n",
       "t0_percent: Float = 0.9307613\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t1: Float = validation_df1.filter($\"TARGET\" ===  1).count()\n",
    "val t0: Float = validation_df1.filter($\"TARGET\" ===  0).count()\n",
    "val t1_percent = t1/validation_df1.count()\n",
    "val t0_percent = t0/validation_df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying data is split 6.92% and 93.07% for TARGET 1 and 0 respectively. Although the accuracy of the model is 93.06%, it is predicting marginally worse than the data means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy measurement 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "import org.apache.spark.mllib.regression.LabeledPoint\n",
       "import org.apache.spark.mllib.util.MLUtils\n",
       "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lp: org.apache.spark.sql.DataFrame = [label: double, prediction: double]\n",
       "counttotal: Long = 18429\n",
       "correct: Long = 17150\n",
       "wrong: Long = 1279\n",
       "truep: Long = 17120\n",
       "falseN: Long = 1246\n",
       "falseP: Long = 33\n",
       "ratioWrong: Double = 0.06940148678712899\n",
       "ratioCorrect: Double = 0.930598513212871\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lp = predictions1.select( \"label\", \"prediction\")\n",
    "val counttotal = predictions1.count()\n",
    "val correct = lp.filter($\"label\" === $\"prediction\").count() //correct prediction\n",
    "val wrong = lp.filter(not($\"label\" === $\"prediction\")).count() //wrong prediction\n",
    "val truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" === $\"prediction\").count() //Correct prediction of label=0\n",
    "val falseN = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" === $\"prediction\")).count() //Wrong prediction when label=1\n",
    "val falseP = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" === $\"prediction\")).count() //Wrong prediction when lable =0\n",
    "val ratioWrong=wrong.toDouble/counttotal.toDouble //Wrong prediction ratio\n",
    "val ratioCorrect=correct.toDouble/counttotal.toDouble //Correct predicition ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy measurement 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[455] at map at <console>:48\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@73a4958c\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionLabelsRDD = predictions1.select(\"prediction\", \"label\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the receiver operating characteristic (ROC) curve : 0.5107935550358409\n"
     ]
    }
   ],
   "source": [
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC ROC of 51.01% is indicating the logistic regression model has not produced a strong class separation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "// Cross check training data set is being used\n",
    "println(\"Total data rows in combined dataframe: \"+ train_df1.count())\n",
    "var RF_df = train_df1\n",
    "val train_df1Features = train_df1.columns\n",
    "for (colName<-creditRiskFeatures){\n",
    "     |   RF_df=RF_df.withColumn(colName,col(colName).cast(\"Double\"))\n",
    "     | }\n",
    "//RF_df.printSchema()\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_df1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "test_df1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "validation_df1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var Array(train_df1,test_df1,validation_df1)=creditRiskdf1.randomSplit(\n",
    "    Array(0.7,0.2,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val c1 = train_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfLabelIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_6e44b4331c81\n"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Index labels, adding metadata to the label column.\n",
    "// Fit on whole dataset to include all labels in index.\n",
    "val rfLabelIndexer = new StringIndexer().setInputCol(\"TARGET\").setOutputCol(\n",
    "    \"indexedLabel\").fit(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_2624ac1ec63e\n",
       "df1: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 187 more fields]\n"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfAssembler = new VectorAssembler().setInputCols(features).setOutputCol(\n",
    "    \"features\")\n",
    "val df1 = rfAssembler.transform(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfFeatureIndexer: org.apache.spark.ml.feature.VectorIndexerModel = vecIdx_870b6b13cfa5\n"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Automatically identify categorical features, and index them.\n",
    "// Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "val rfFeatureIndexer = new VectorIndexer()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"indexedFeatures\")\n",
    "  .setMaxCategories(4).fit(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelRf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_90dd93cf5d88\n"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Train a RandomForest model.\n",
    "val modelRf = new RandomForestClassifier()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setFeaturesCol(\"indexedFeatures\")  \n",
    "  .setNumTrees(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfLabelConverter: org.apache.spark.ml.feature.IndexToString = idxToStr_91effc1e7550\n"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Convert indexed labels back to original labels.\n",
    "val rfLabelConverter = new IndexToString()\n",
    "  .setInputCol(\"prediction\")\n",
    "  .setOutputCol(\"predictedLabel\")\n",
    "  .setLabels(rfLabelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfPipeline: org.apache.spark.ml.Pipeline = pipeline_2aad45ab1049\n"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Chain indexers and forest in a Pipeline.\n",
    "val rfPipeline = new Pipeline()\n",
    "  .setStages(Array(rfLabelIndexer, rfFeatureIndexer, modelRf, rfLabelConverter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfModel: org.apache.spark.ml.PipelineModel = pipeline_2aad45ab1049\n"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Train model. This also runs the indexers.\n",
    "val rfModel = rfPipeline.fit(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 187 more fields]\n",
       "predictions2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 193 more fields]\n"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = rfAssembler.transform(validation_df1)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions2 = rfModel.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------+\n",
      "|            features|indexedLabel|         probability|predictedLabel|\n",
      "+--------------------+------------+--------------------+--------------+\n",
      "|(187,[0,1,2,3,4,5...|         1.0|[0.93529128416549...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.82937645222882...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         1.0|[0.85369003699841...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.92925488806979...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.92925488806979...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.94394300770324...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         1.0|[0.90494130330021...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.92925488806979...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.93529128416549...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         1.0|[0.90672383426802...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.94394300770324...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.92925488806979...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.93252553117951...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.94394300770324...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         1.0|[0.92060316453205...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.90672383426802...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.94394300770324...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.90672383426802...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.94394300770324...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         1.0|[0.81834421230279...|           0.0|\n",
      "+--------------------+------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Select example rows to display.\n",
    "\n",
    "predictions2.select(\"features\", \"indexedLabel\", \"probability\",\"predictedLabel\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy measurement 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0737182946573125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_5129581eb5e3\n",
       "accuracy: Double = 0.9262817053426875\n"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions2)\n",
    "println(s\"Test Error = ${(1.0 - accuracy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy of our prediction is 92.87%. However, as noted with the logistic regression, this accuracy appears to be marginally better than the dataset means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy measurement 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[2890] at map at <console>:56\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@5c1390d0\n"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionLabelsRDD = predictions2.select(\"prediction\", \"indexedLabel\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the receiver operating characteristic (ROC) curve : 0.5\n"
     ]
    }
   ],
   "source": [
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At an AUC ROC of 0.5, the Random forest classifier has not yielded separation of the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2.3 Multilayer Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfTrain: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n",
       "dfTrain: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n",
       "dfTest: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n",
       "dfTest: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var dfTrain = train_df1.withColumnRenamed(\"TARGET\", \"label\")\n",
    "dfTrain = dfTrain.drop(\"SK_ID_CURR\")\n",
    "\n",
    "var dfTest = test_df1.withColumnRenamed(\"TARGET\", \"label\")\n",
    "dfTest = dfTest.drop(\"SK_ID_CURR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res95: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain.cache()\n",
    "dfTest.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set count: 128637\n",
      "Test set count: 36708\n",
      "Training column count: 187\n",
      "Test column count: 187\n"
     ]
    }
   ],
   "source": [
    "println(\"Training set count: \"+dfTrain.count() + \"\\n\" +\n",
    "        \"Test set count: \"+dfTest.count())\n",
    "println(\"Training column count: \"+dfTrain.columns.size + \"\\n\" +\n",
    "        \"Test column count: \"+dfTest.columns.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlpcLabelIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_08a5573e8987\n",
       "features: Array[String] = Array(EXT_SOURCE_2, EXT_SOURCE_3, DAYS_BIRTH, AMT_CREDIT, AMT_ANNUITY, DAYS_ID_PUBLISH, previous_loans_CNT_PAYMENT_mean, previous_loans_SELLERPLACE_AREA_max, DAYS_LAST_PHONE_CHANGE, DAYS_REGISTRATION, CODE_GENDER_F, REGION_POPULATION_RELATIVE, client_cash_CNT_INSTALMENT_max_mean, bureau_DAYS_CREDIT_mean, previous_loans_NAME_CONTRACT_STATUS_Refused_count_norm, client_installments_NUM_INSTALMENT_VERSION_max_mean, bureau_DAYS_CREDIT_min, previous_loans_NAME_YIELD_GROUP_high_count_norm, previous_loans_RATE_DOWN_PAYMENT_sum, previous_loans_DAYS_FIRST_DUE_min, client_cash_SK_DPD_DEF_sum_max, bureau_AMT_CREDIT_SUM_LIMIT_mean, bureau_DAYS_CREDIT_sum, previous_loans_DAYS_LAST_DUE_1ST_..."
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Index labels, adding metadata to the label column.\n",
    "// Fit on whole dataset to include all labels in index.\n",
    "var mlpcLabelIndexer = new StringIndexer().setInputCol(\"label\").setOutputCol(\n",
    "    \"indexedLabel\").fit(dfTrain)\n",
    "\n",
    "var features = dfTrain.columns\n",
    "features = features.filter(!_.contains(\"TARGET\"))\n",
    "\n",
    "var mlpcAssembler = new VectorAssembler()\n",
    "    .setInputCols(features).setOutputCol(\"features\")\n",
    "\n",
    "//var mlpcTrain = Assembler.transform(dfTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "layers: Array[Int] = Array(187, 50, 25, 2)\n",
       "mlpcModel: org.apache.spark.ml.classification.MultilayerPerceptronClassifier = mlpc_7eecf80265b6\n"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Automatically identify categorical features, and index them.\n",
    "//var mlpcFeatureIndexer = new VectorIndexer()\n",
    "//  .setInputCol(\"features\")\n",
    "//  .setOutputCol(\"indexedFeatures\")\n",
    "\n",
    "// specify layers for the neural network:\n",
    "// input layer of size 187 (features), two intermediate of size 45 and 10\n",
    "// and output of size 2 (classes)\n",
    "//https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/\n",
    "\n",
    "var layers = Array[Int](187, 50, 25, 2)\n",
    "\n",
    "// create the trainer and set its parameters\n",
    "var mlpcModel = new MultilayerPerceptronClassifier()\n",
    "  .setLayers(layers)\n",
    "  .setBlockSize(128)\n",
    "  .setTol(0.0005)\n",
    "  .setSeed(123)\n",
    "  .setMaxIter(700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlpcLabelConverter: org.apache.spark.ml.feature.IndexToString = idxToStr_fb55786328c5\n",
       "mlpcPipeline: org.apache.spark.ml.Pipeline = pipeline_4aaf696f8300\n",
       "mlpcModelTrained: org.apache.spark.ml.PipelineModel = pipeline_4aaf696f8300\n"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Convert indexed labels back to original labels.\n",
    "val mlpcLabelConverter = new IndexToString()\n",
    "  .setInputCol(\"prediction\")\n",
    "  .setOutputCol(\"predictedLabel\")\n",
    "  .setLabels(mlpcLabelIndexer.labels)\n",
    "\n",
    "// Chain indexers and forest in a Pipeline.\n",
    "var mlpcPipeline = new Pipeline().setStages(\n",
    "    Array(mlpcLabelIndexer, mlpcAssembler, mlpcModel, mlpcLabelConverter))\n",
    "\n",
    "// train the model\n",
    "var mlpcModelTrained = mlpcPipeline.fit(dfTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 191 more fields]\n"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var result = mlpcModelTrained.transform(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EXT_SOURCE_2: float (nullable = true)\n",
      " |-- EXT_SOURCE_3: float (nullable = true)\n",
      " |-- DAYS_BIRTH: float (nullable = true)\n",
      " |-- AMT_CREDIT: float (nullable = true)\n",
      " |-- AMT_ANNUITY: float (nullable = true)\n",
      " |-- DAYS_ID_PUBLISH: float (nullable = true)\n",
      " |-- previous_loans_CNT_PAYMENT_mean: float (nullable = true)\n",
      " |-- previous_loans_SELLERPLACE_AREA_max: float (nullable = true)\n",
      " |-- DAYS_LAST_PHONE_CHANGE: float (nullable = true)\n",
      " |-- DAYS_REGISTRATION: float (nullable = true)\n",
      " |-- CODE_GENDER_F: float (nullable = true)\n",
      " |-- REGION_POPULATION_RELATIVE: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_max_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_mean: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Refused_count_norm: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_max_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_high_count_norm: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_sum: float (nullable = true)\n",
      " |-- previous_loans_DAYS_FIRST_DUE_min: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_DEF_sum_max: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_LIMIT_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_sum: float (nullable = true)\n",
      " |-- previous_loans_DAYS_LAST_DUE_1ST_VERSION_max: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash X-Sell: low_count_norm: float (nullable = true)\n",
      " |-- previous_loans_HOUR_APPR_PROCESS_START_mean: float (nullable = true)\n",
      " |-- NAME_FAMILY_STATUS_Married: float (nullable = true)\n",
      " |-- previous_loans_CNT_PAYMENT_max: float (nullable = true)\n",
      " |-- AMT_INCOME_TOTAL: float (nullable = true)\n",
      " |-- previous_loans_DAYS_LAST_DUE_sum: float (nullable = true)\n",
      " |-- client_cash_MONTHS_BALANCE_max_mean: float (nullable = true)\n",
      " |-- NAME_EDUCATION_TYPE_Higher education: float (nullable = true)\n",
      " |-- client_cash_MONTHS_BALANCE_sum_mean: float (nullable = true)\n",
      " |-- bureau_DAYS_ENDDATE_FACT_min: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_ENDDATE_min: float (nullable = true)\n",
      " |-- previous_loans_AMT_ANNUITY_mean: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Mortgage_count_norm: float (nullable = true)\n",
      " |-- previous_loans_AMT_ANNUITY_max: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_max: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash X-Sell: high_count_norm: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Active_count: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_low_action_count_norm: float (nullable = true)\n",
      " |-- client_cash_counts_mean: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_sum_min: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Active_count_norm: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_UPDATE_min: float (nullable = true)\n",
      " |-- DEF_60_CNT_SOCIAL_CIRCLE: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_3: float (nullable = true)\n",
      " |-- bureau_DAYS_ENDDATE_FACT_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_LIMIT_max: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_max_sum: float (nullable = true)\n",
      " |-- previous_loans_NAME_TYPE_SUITE_Unaccompanied_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_norm_mean: float (nullable = true)\n",
      " |-- client_installments_DAYS_ENTRY_PAYMENT_max_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Approved_count_norm: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_min_mean: float (nullable = true)\n",
      " |-- previous_loans_DAYS_DECISION_mean: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash Street: low_count_norm: float (nullable = true)\n",
      " |-- HOUR_APPR_PROCESS_START: float (nullable = true)\n",
      " |-- previous_loans_DAYS_LAST_DUE_1ST_VERSION_min: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Consumer credit_count_norm: float (nullable = true)\n",
      " |-- previous_loans_HOUR_APPR_PROCESS_START_max: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_HC_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_PRODUCT_TYPE_walk-in_count_norm: float (nullable = true)\n",
      " |-- bureau_DAYS_CREDIT_UPDATE_mean: float (nullable = true)\n",
      " |-- previous_loans_DAYS_DECISION_sum: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS industry with interest_count_norm: float (nullable = true)\n",
      " |-- client_cash_MONTHS_BALANCE_max_sum: float (nullable = true)\n",
      " |-- previous_loans_NAME_CLIENT_TYPE_Refreshed_count_norm: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash Street: high_count_norm: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_DEBT_min: float (nullable = true)\n",
      " |-- previous_loans_RATE_DOWN_PAYMENT_min: float (nullable = true)\n",
      " |-- previous_loans_DAYS_TERMINATION_mean: float (nullable = true)\n",
      " |-- previous_loans_NAME_PAYMENT_TYPE_XNA_count_norm: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Credit card_count_norm: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_18: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Credit and cash offices_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Contact center_count_norm: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Car loan_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_PORTFOLIO_POS_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_TUESDAY_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Furniture_count_norm: float (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_QRT: float (nullable = true)\n",
      " |-- NAME_INCOME_TYPE_Working: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Microloan_count_norm: float (nullable = true)\n",
      " |-- previous_loans_DAYS_TERMINATION_min: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Channel of corporate sales_count_norm: float (nullable = true)\n",
      " |-- NAME_EDUCATION_TYPE_Secondary / secondary special: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_Core staff: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_middle_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Completed_count_norm_mean: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_OVERDUE_max: float (nullable = true)\n",
      " |-- REG_CITY_NOT_LIVE_CITY: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Business Entity Type 3: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Closed_count: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Self-employed: float (nullable = true)\n",
      " |-- previous_loans_NAME_PORTFOLIO_Cash_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_WEDNESDAY_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Completed_count_sum: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_SCO_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_XNA_count_norm: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS household without interest_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_PRODUCT_TYPE_XNA_count_norm: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_LIMIT_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_PRODUCT_TYPE_x-sell_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_MONDAY_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_CLIENT_TYPE_New_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_AP+ (Cash loan)_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NFLAG_INSURED_ON_APPROVAL_mean: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_mean_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Clothing and Accessories_count_norm: float (nullable = true)\n",
      " |-- DEF_30_CNT_SOCIAL_CIRCLE: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_NUMBER_mean_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_PORTFOLIO_XNA_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_CLIENT_TYPE_Repeater_count_norm: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Credit card_count: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Canceled_count_norm: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_Drivers: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Signed_count_norm_max: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_Laborers: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_THURSDAY_count_norm: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Construction: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_max: float (nullable = true)\n",
      " |-- client_cash_SK_DPD_DEF_sum_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_TYPE_Cash loans_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_CASH_LOAN_PURPOSE_Other_count_norm: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_sum_min: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Country-wide_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Signed_count_sum: float (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_YEAR: float (nullable = true)\n",
      " |-- bureau_CREDIT_TYPE_Microloan_count: float (nullable = true)\n",
      " |-- NAME_INCOME_TYPE_State servant: float (nullable = true)\n",
      " |-- previous_loans_DAYS_TERMINATION_max: float (nullable = true)\n",
      " |-- FLAG_WORK_PHONE: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Photo / Cinema Equipment_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_FRIDAY_count_norm: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Military: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_VERSION_min_sum: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Completed_count_norm_sum: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_16: float (nullable = true)\n",
      " |-- previous_loans_NAME_TYPE_SUITE_Spouse, partner_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Returned to the store_count_sum: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Industry: type 9: float (nullable = true)\n",
      " |-- AMT_REQ_CREDIT_BUREAU_MON: float (nullable = true)\n",
      " |-- previous_loans_NAME_CONTRACT_STATUS_Approved_count: float (nullable = true)\n",
      " |-- bureau_CREDIT_ACTIVE_Sold_count_norm: float (nullable = true)\n",
      " |-- bureau_AMT_CREDIT_SUM_OVERDUE_mean: float (nullable = true)\n",
      " |-- previous_loans_NAME_SELLER_INDUSTRY_Construction_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_SCOFR_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_SELLER_INDUSTRY_Furniture_count_norm: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_norm_max: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Construction Materials_count_norm: float (nullable = true)\n",
      " |-- client_bureau_balance_STATUS_1_count_norm_sum: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash Street: middle_count_norm: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS other with interest_count_norm: float (nullable = true)\n",
      " |-- client_cash_CNT_INSTALMENT_min_min: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_VERIF_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_LIMIT_count_norm: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Cash_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Regional / Local_count_norm: float (nullable = true)\n",
      " |-- previous_loans_WEEKDAY_APPR_PROCESS_START_SUNDAY_count: float (nullable = true)\n",
      " |-- FLAG_DOCUMENT_13: float (nullable = true)\n",
      " |-- previous_loans_CODE_REJECT_REASON_CLIENT_count_norm: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_middle_count: float (nullable = true)\n",
      " |-- previous_loans_NAME_GOODS_CATEGORY_Jewelry_count_norm: float (nullable = true)\n",
      " |-- WALLSMATERIAL_MODE_Stone, brick: float (nullable = true)\n",
      " |-- previous_loans_NAME_SELLER_INDUSTRY_Consumer electronics_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CNT_PAYMENT_min: float (nullable = true)\n",
      " |-- previous_loans_NAME_YIELD_GROUP_high_count: float (nullable = true)\n",
      " |-- NAME_FAMILY_STATUS_Separated: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Signed_count_norm_mean: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_Card Street_count_norm: float (nullable = true)\n",
      " |-- client_installments_NUM_INSTALMENT_NUMBER_max_min: float (nullable = true)\n",
      " |-- previous_loans_PRODUCT_COMBINATION_POS industry without interest_count_norm: float (nullable = true)\n",
      " |-- previous_loans_CHANNEL_TYPE_Regional / Local_count: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_Transport: type 3: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Returned to the store_count_norm_max: float (nullable = true)\n",
      " |-- ORGANIZATION_TYPE_School: float (nullable = true)\n",
      " |-- previous_loans_NAME_TYPE_SUITE_Other_B_count_norm: float (nullable = true)\n",
      " |-- bureau_CNT_CREDIT_PROLONG_mean: float (nullable = true)\n",
      " |-- client_cash_NAME_CONTRACT_STATUS_Active_count_min: float (nullable = true)\n",
      " |-- WEEKDAY_APPR_PROCESS_START_WEDNESDAY: float (nullable = true)\n",
      " |-- OCCUPATION_TYPE_High skill tech staff: float (nullable = true)\n",
      " |-- previous_loans_NAME_PAYMENT_TYPE_XNA_count: float (nullable = true)\n",
      " |-- FLAG_PHONE: float (nullable = true)\n",
      " |-- label: float (nullable = true)\n",
      " |-- indexedLabel: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      " |-- predictedLabel: string (nullable = true)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+----------+\n",
      "|            features|label|         probability|prediction|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "|(187,[0,1,2,3,4,5...|  0.0|[0.92880782868067...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  1.0|[0.92880782868067...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  1.0|[0.92534729582439...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  1.0|[0.91163069319534...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  0.0|[0.92422412510971...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  1.0|[0.92422412510971...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  1.0|[0.93131838453576...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  1.0|[0.92640940332682...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  1.0|[0.93556655931217...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  0.0|[0.91201575548869...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  1.0|[0.93746329750383...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  0.0|[0.91428235420984...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  1.0|[0.93256628999691...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  0.0|[0.92534729582439...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  0.0|[0.93251652629116...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  0.0|[0.92811622866218...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  0.0|[0.93556655931217...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  1.0|[0.92422412510971...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  0.0|[0.92548353726721...|       0.0|\n",
      "|(187,[0,1,2,3,4,5...|  0.0|[0.92464057153126...|       0.0|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select(\"features\", \"label\", \"probability\",\"prediction\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.9257927427263811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_f716f30ce601\n",
       "accuracy: Double = 0.9257927427263811\n"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "\n",
    "val accuracy = evaluator.evaluate(result)\n",
    "\n",
    "println(\"Test set accuracy = \" + accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[3367] at map at <console>:59\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@7e3a34b2\n"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionLabelsRDD = result.select(\"prediction\", \"indexedLabel\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the receiver operating characteristic (ROC) curve : 0.5\n"
     ]
    }
   ],
   "source": [
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res113: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain.unpersist()\n",
    "dfTest.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Model Optimisation -  use dataset creditRiskdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3.1 Binomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrModel: org.apache.spark.ml.PipelineModel = pipeline_3ab0fcccf979\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModel = lrPipeline.fit(train_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+--------------------+----------+\n",
      "|SK_ID_CURR|            features|label|         probability|prediction|\n",
      "+----------+--------------------+-----+--------------------+----------+\n",
      "|  115467.0|(187,[0,1,2,3,4,5...|  0.0|[0.85171205996034...|       0.0|\n",
      "|  145702.0|(187,[0,1,2,3,4,5...|  0.0|[0.81270010056785...|       0.0|\n",
      "|  218200.0|(187,[0,1,2,3,4,5...|  0.0|[0.52112132627263...|       0.0|\n",
      "|  202641.0|(187,[0,1,2,3,4,5...|  1.0|[0.75660358947379...|       0.0|\n",
      "|  213147.0|(187,[0,1,2,3,4,5...|  1.0|[0.45392060778981...|       1.0|\n",
      "|  179975.0|(187,[0,1,2,3,4,5...|  0.0|[0.86612244056297...|       0.0|\n",
      "|  126095.0|(187,[0,1,2,3,4,5...|  0.0|[0.91784269329172...|       0.0|\n",
      "|  176593.0|(187,[0,1,2,3,4,5...|  0.0|[0.79318491001957...|       0.0|\n",
      "|  139376.0|(187,[0,1,2,3,4,5...|  0.0|[0.89295530536230...|       0.0|\n",
      "|  200184.0|(187,[0,1,2,3,4,5...|  0.0|[0.56096322783316...|       0.0|\n",
      "|  161901.0|(187,[0,1,2,3,4,5...|  0.0|[0.80255052276530...|       0.0|\n",
      "|  161215.0|(187,[0,1,2,3,4,5...|  0.0|[0.81705667052757...|       0.0|\n",
      "|  164916.0|(187,[0,1,2,3,4,5...|  0.0|[0.40212096600584...|       1.0|\n",
      "|  198635.0|(187,[0,1,2,3,4,5...|  1.0|[0.89116817444470...|       0.0|\n",
      "|  206971.0|(187,[0,1,2,3,4,5...|  0.0|[0.91842003912108...|       0.0|\n",
      "|  186659.0|(187,[0,1,2,3,4,5...|  1.0|[0.90102951239618...|       0.0|\n",
      "|  202779.0|(187,[0,1,2,3,4,5...|  1.0|[0.71055752626153...|       0.0|\n",
      "|  150707.0|(187,[0,1,2,3,4,5...|  0.0|[0.94679969970106...|       0.0|\n",
      "|  220234.0|(187,[0,1,2,3,4,5...|  0.0|[0.53081700989977...|       0.0|\n",
      "|  218501.0|(187,[0,1,2,3,4,5...|  0.0|[0.91479548561195...|       0.0|\n",
      "+----------+--------------------+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions3: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 191 more fields]\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions3 = lrModel.transform(validation_df2)\n",
    "predictions1.select (\"SK_ID_CURR\",\"features\", \"label\",\"probability\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.9327317473338802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_aaea8c133a9e\n",
       "accuracy: Double = 0.9327317473338802\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions3)\n",
    "println(\"Test set accuracy = \" + accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t1: Float = 1366.0\n",
       "t0: Float = 17164.0\n",
       "t1_percent: Float = 0.073718295\n",
       "t0_percent: Float = 0.9262817\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t1: Float = validation_df1.filter($\"TARGET\" ===  1).count()\n",
    "val t0: Float = validation_df1.filter($\"TARGET\" ===  0).count()\n",
    "val t1_percent = t1/validation_df1.count()\n",
    "val t0_percent = t0/validation_df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lp: org.apache.spark.sql.DataFrame = [label: double, prediction: double]\n",
       "counttotal: Long = 18429\n",
       "correct: Long = 3411\n",
       "wrong: Long = 246\n",
       "truep: Long = 3403\n",
       "falseN: Long = 240\n",
       "falseP: Long = 6\n",
       "ratioWrong: Double = 0.013348526778447013\n",
       "ratioCorrect: Double = 0.1850887188670031\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lp = predictions3.select( \"label\", \"prediction\")\n",
    "val counttotal = predictions1.count()\n",
    "val correct = lp.filter($\"label\" === $\"prediction\").count() //correct prediction\n",
    "val wrong = lp.filter(not($\"label\" === $\"prediction\")).count() //wrong prediction\n",
    "val truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" === $\"prediction\").count() //Correct prediction of label=0\n",
    "val falseN = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" === $\"prediction\")).count() //Wrong prediction when label=1\n",
    "val falseP = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" === $\"prediction\")).count() //Wrong prediction when lable =0\n",
    "val ratioWrong=wrong.toDouble/counttotal.toDouble //Wrong prediction ratio\n",
    "val ratioCorrect=correct.toDouble/counttotal.toDouble //Correct predicition ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[915] at map at <console>:54\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@61f07219\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionLabelsRDD = predictions3.select(\"prediction\", \"label\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the receiver operating characteristic (ROC) curve : 0.5152490087907721\n"
     ]
    }
   ],
   "source": [
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC ROC for logistic regression model decreased from 51.01% to 50.97% using creditRiskdf2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "rfLabelIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_701ca2f8f435\n",
       "df3: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 187 more fields]\n",
       "rfModel: org.apache.spark.ml.PipelineModel = pipeline_8cbc0cf21622\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val c2 = train_df2\n",
    "\n",
    "// Index labels, adding metadata to the label column.\n",
    "// Fit on whole dataset to include all labels in index.\n",
    "val rfLabelIndexer = new StringIndexer().setInputCol(\"TARGET\").setOutputCol(\n",
    "    \"indexedLabel\").fit(c2)\n",
    "\n",
    "val df3 = rfAssembler.transform(c2)\n",
    "\n",
    "// Train model. This also runs the indexers.\n",
    "val rfModel = rfPipeline.fit(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df4: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 187 more fields]\n",
       "predictions4: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 193 more fields]\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df4 = rfAssembler.transform(validation_df2)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions4 = rfModel.transform(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------+\n",
      "|            features|indexedLabel|         probability|predictedLabel|\n",
      "+--------------------+------------+--------------------+--------------+\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.89590680821617...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.92026439275642...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.93599846897849...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.93831482769461...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         1.0|[0.93341803071488...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.90736897624323...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.88799724098539...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.89050586060256...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.92636272906592...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.93046624745639...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.90596371529901...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.93905383944141...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.91158609369916...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.91626588712371...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         1.0|[0.89738500664669...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.90019710430800...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.92109851792685...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.92406523451960...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         0.0|[0.90335164641213...|           0.0|\n",
      "|(187,[0,1,2,3,4,5...|         1.0|[0.88147763800719...|           0.0|\n",
      "+--------------------+------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Select example rows to display.\n",
    "\n",
    "predictions4.select(\"features\", \"indexedLabel\", \"probability\",\"predictedLabel\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.067815149029259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_09a11bda7895\n",
       "accuracy: Double = 0.932184850970741\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions4)\n",
    "println(s\"Test Error = ${(1.0 - accuracy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[1007] at map at <console>:54\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@4ab84c1f\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionLabelsRDD = predictions4.select(\"prediction\", \"indexedLabel\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the receiver operating characteristic (ROC) curve : 0.5\n"
     ]
    }
   ],
   "source": [
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement. AUC ROC remained at 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "val rfModel = model.stages(2).asInstanceOf[RandomForestClassificationModel]\n",
    "println(s\"Learned classification forest model:\\n ${rfModel.toDebugString}\")\n",
    "\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4 Model Optimisation - Bias reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4.1 Prepare unbiased training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of TARGET = 1: 9222\n",
      "# of TARGET = 0: 119415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df9: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "df10: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df9 = train_df1.filter($\"TARGET\" === 1.0)\n",
    "println(\"# of TARGET = 1: \"+df9.count())\n",
    "\n",
    "val df10 = train_df1.filter($\"TARGET\" === 0.0)\n",
    "println(\"# of TARGET = 0: \"+df10.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df11: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n",
       "res112: Long = 9163\n"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Now from 33224 rows of TARGET==0, we randomly extract 2300 rows and combine it with df9\n",
    "//To get a dataframe of 2300 target==1 and 2300 target ==0\n",
    "val df11 = df10.sample(false, 9222.toFloat/119415.toFloat)\n",
    "df11.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df12: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df12 = df9.union(df11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res114: df12.type = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 186 more fields]\n"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df12.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4.2 Binomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrModel_unbiased: org.apache.spark.ml.PipelineModel = pipeline_99b04f127e7e\n"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrModel_unbiased = lrPipeline.fit(df12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.9998920669185105\n",
      "Area under the receiver operating characteristic (ROC) curve : 0.9999417385224889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testLr_unbiased: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 191 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_a393e403ce87\n",
       "accuracy: Double = 0.9998920669185105\n",
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[4788] at map at <console>:77\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@203b6f39\n"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testLr_unbiased = lrModel_unbiased.transform(validation_df1)\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "\n",
    "val accuracy = evaluator.evaluate(testLr_unbiased)\n",
    "println(\"Test set accuracy = \" + accuracy)\n",
    "\n",
    "val predictionLabelsRDD = testLr_unbiased.select(\n",
    "    \"prediction\", \"label\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)\n",
    "\n",
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+--------------------+----------+\n",
      "|SK_ID_CURR|            features|label|         probability|prediction|\n",
      "+----------+--------------------+-----+--------------------+----------+\n",
      "|  214330.0|(187,[0,1,2,3,4,5...|  1.0|[5.67252323625903...|       1.0|\n",
      "|  139370.0|(187,[0,1,2,3,4,5...|  0.0|[0.99999999996818...|       0.0|\n",
      "|  202641.0|(187,[0,1,2,3,4,5...|  0.0|[0.99999999987599...|       0.0|\n",
      "|  130112.0|(187,[0,1,2,3,4,5...|  0.0|[0.99999999980168...|       0.0|\n",
      "|  103041.0|(187,[0,1,2,3,4,5...|  1.0|[2.39219986246976...|       1.0|\n",
      "|  223977.0|(187,[0,1,2,3,4,5...|  0.0|[0.99999999964409...|       0.0|\n",
      "|  132315.0|(187,[0,1,2,3,4,5...|  0.0|[0.99999999984569...|       0.0|\n",
      "|  181339.0|(187,[0,1,2,3,4,5...|  0.0|[0.99999999994527...|       0.0|\n",
      "|  158660.0|(187,[0,1,2,3,4,5...|  0.0|[0.99999999998746...|       0.0|\n",
      "|  155684.0|(187,[0,1,2,3,4,5...|  1.0|[3.81188927543659...|       1.0|\n",
      "|  145351.0|(187,[0,1,2,3,4,5...|  0.0|[0.99999999998380...|       0.0|\n",
      "|  145262.0|(187,[0,1,2,3,4,5...|  1.0|[1.01938703908181...|       1.0|\n",
      "|  182646.0|(187,[0,1,2,3,4,5...|  0.0|[0.99999999987999...|       0.0|\n",
      "|  173459.0|(187,[0,1,2,3,4,5...|  1.0|[3.78419608474484...|       1.0|\n",
      "|  202887.0|(187,[0,1,2,3,4,5...|  1.0|[2.70833337702310...|       1.0|\n",
      "|  137266.0|(187,[0,1,2,3,4,5...|  1.0|[2.85572937203706...|       1.0|\n",
      "|  212709.0|(187,[0,1,2,3,4,5...|  1.0|[2.30115271300109...|       1.0|\n",
      "|  223096.0|(187,[0,1,2,3,4,5...|  0.0|[0.99999999989487...|       0.0|\n",
      "|  139376.0|(187,[0,1,2,3,4,5...|  1.0|[3.42337271941302...|       1.0|\n",
      "|  181052.0|(187,[0,1,2,3,4,5...|  1.0|[1.59829770382786...|       1.0|\n",
      "+----------+--------------------+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testLr_unbiased.select (\n",
    "    \"SK_ID_CURR\",\"features\", \"label\",\"probability\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df13: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 187 more fields]\n",
       "rfModel_unbiased: org.apache.spark.ml.PipelineModel = pipeline_2aad45ab1049\n"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df13 = rfAssembler.transform(df12)\n",
    "\n",
    "// Train model. This also runs the indexers.\n",
    "val rfModel_unbiased = rfPipeline.fit(df13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.6215326497571506\n",
      "Area under the receiver operating characteristic (ROC) curve : 0.6242232798192137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df14: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 187 more fields]\n",
       "testRf_unbiased: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 193 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_44a2bd660a9d\n",
       "accuracy: Double = 0.6215326497571506\n",
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[4830] at map at <console>:81\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@ae4868e\n"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df14 = rfAssembler.transform(validation_df1)\n",
    "\n",
    "val testRf_unbiased = rfModel_unbiased.transform(df14)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(testRf_unbiased)\n",
    "println(\"Test set accuracy = \" + accuracy)\n",
    "\n",
    "val predictionLabelsRDD = testRf_unbiased.select(\"prediction\", \"indexedLabel\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)\n",
    "\n",
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4.4 Multilayer Perceptron Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfTrain_unbiased: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n",
       "dfTrain_unbiased: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n",
       "mlpcModel_unbiased: org.apache.spark.ml.PipelineModel = pipeline_4aaf696f8300\n"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var dfTrain_unbiased = df12.withColumnRenamed(\"TARGET\", \"label\")\n",
    "dfTrain_unbiased = dfTrain_unbiased.drop(\"SK_ID_CURR\")\n",
    "\n",
    "var mlpcModel_unbiased = mlpcPipeline.fit(dfTrain_unbiased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.6226659471127901\n",
      "Area under the receiver operating characteristic (ROC) curve : 0.4941172541664207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfTest_unbiased: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n",
       "dfTest_unbiased: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n",
       "testMlpc_unbiased: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 191 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_1d10f90f6c59\n",
       "accuracy: Double = 0.6226659471127901\n",
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[4873] at map at <console>:80\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@4aeab5a\n"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var dfTest_unbiased = validation_df1.withColumnRenamed(\"TARGET\", \"label\")\n",
    "dfTest_unbiased = dfTest_unbiased.drop(\"SK_ID_CURR\")\n",
    "\n",
    "var testMlpc_unbiased = mlpcModel_unbiased.transform(dfTest_unbiased)\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "\n",
    "val accuracy = evaluator.evaluate(testMlpc_unbiased)\n",
    "println(\"Test set accuracy = \" + accuracy)\n",
    "\n",
    "val predictionLabelsRDD = testMlpc_unbiased.select(\"prediction\", \"indexedLabel\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)\n",
    "\n",
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4 Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the model fitting and optimisation, the best performing models are the ones fitted with unbiased datasets.\n",
    "- lrModel_unbiased\n",
    "- rfModel_unbiased\n",
    "- mlpcModel_unbiased\n",
    "\n",
    "These have been selected for testing with the hidden test dataset - test_df1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4.1 Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.9998910319276452\n",
      "Area under the receiver operating characteristic (ROC) curve : 0.9999411487758945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testLr: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 191 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_30b19c7e61b8\n",
       "accuracy: Double = 0.9998910319276452\n",
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[4663] at map at <console>:76\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@5ad3e9e9\n"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testLr = lrModel_unbiased.transform(test_df1)\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "\n",
    "val accuracy = evaluator.evaluate(testLr)\n",
    "println(\"Test set accuracy = \" + accuracy)\n",
    "\n",
    "val predictionLabelsRDD = testLr.select(\"prediction\", \"label\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)\n",
    "\n",
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4.2 Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.6305715066994805\n",
      "Area under the receiver operating characteristic (ROC) curve : 0.6092938284805874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df5: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 187 more fields]\n",
       "testRf: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 193 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_f1098d405bd7\n",
       "accuracy: Double = 0.6305715066994805\n",
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[4699] at map at <console>:82\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@43acba68\n"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df5 = rfAssembler.transform(test_df1)\n",
    "\n",
    "val testRf = rfModel_unbiased.transform(df4)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(testRf)\n",
    "println(\"Test set accuracy = \" + accuracy)\n",
    "\n",
    "val predictionLabelsRDD = testRf.select(\"prediction\", \"indexedLabel\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)\n",
    "\n",
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4.3 Multilayer Perceptron Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.6278195488721805\n",
      "Area under the receiver operating characteristic (ROC) curve : 0.501833850809295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfTest: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n",
       "dfTest: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 185 more fields]\n",
       "testMlpc: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: float, EXT_SOURCE_3: float ... 191 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_e1f832118127\n",
       "accuracy: Double = 0.6278195488721805\n",
       "predictionLabelsRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[4742] at map at <console>:80\n",
       "binMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@33ba83a2\n"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var dfTest = test_df1.withColumnRenamed(\"TARGET\", \"label\")\n",
    "dfTest = dfTest.drop(\"SK_ID_CURR\")\n",
    "\n",
    "var testMlpc = mlpcModel_unbiased.transform(dfTest)\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "\n",
    "val accuracy = evaluator.evaluate(testMlpc)\n",
    "println(\"Test set accuracy = \" + accuracy)\n",
    "\n",
    "val predictionLabelsRDD = testMlpc.select(\"prediction\", \"indexedLabel\").rdd.map(r => (r.getDouble(0), r.getDouble(1)))\n",
    "val binMetrics = new BinaryClassificationMetrics(predictionLabelsRDD)\n",
    "\n",
    "println(\"Area under the receiver operating characteristic (ROC) curve : \" + binMetrics.areaUnderROC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
