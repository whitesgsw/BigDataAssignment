{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://b2cc1a63ffdb:4043\n",
       "SparkContext available as 'sc' (version = 2.4.3, master = local[*], app id = local-1559458223502)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-02 06:50:19,624 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2019-06-02 06:50:23,017 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2019-06-02 06:50:23,021 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "2019-06-02 06:50:23,022 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "creditRiskdf1: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 283 more fields]\n",
       "creditRiskdf2: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 283 more fields]\n",
       "creditRiskdf3: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 283 more fields]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load wrangled datasets from Phase 2: part1.csv; part2.csv; part3.csv\n",
    "\n",
    "val creditRiskdf1 = spark.read.option(\"header\",\"true\").csv(\"part1.csv\")\n",
    "val creditRiskdf2 = spark.read.option(\"header\",\"true\").csv(\"part2.csv\")\n",
    "val creditRiskdf3 = spark.read.option(\"header\",\"true\").csv(\"part3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf1and2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 283 more fields]\n",
       "creditRiskdf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 283 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// combine the 3 dataframes to 1\n",
    "val creditRiskdf1and2 = creditRiskdf1.union(creditRiskdf2)\n",
    "val creditRiskdf = creditRiskdf1and2.union(creditRiskdf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-02 06:53:09,570 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "Total data rows in loaded dataframes:183875\n",
      "Total data rows in combined dataframe:183875\n"
     ]
    }
   ],
   "source": [
    "// check that the rows have been aggregated correctly\n",
    "\n",
    "println(\"Total data rows in loaded dataframes:\"+(\n",
    "    creditRiskdf1.count()+creditRiskdf2.count()+creditRiskdf3.count()))\n",
    "\n",
    "println(\"Total data rows in combined dataframe:\"+creditRiskdf.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleansing and reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "incon_droplist: List[String] = List(previous_loans_DAYS_FIRST_DUE_mean, previous_loans_SELLERPLACE_AREA_min, previous_loans_DAYS_FIRST_DUE_sum)\n",
       "df: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 280 more fields]\n",
       "res1: df.type = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 280 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Drop features with inconsistent values from dataset.\n",
    "\n",
    "val incon_droplist = List(\"previous_loans_DAYS_FIRST_DUE_mean\",\n",
    "                      \"previous_loans_SELLERPLACE_AREA_min\",\n",
    "                      \"previous_loans_DAYS_FIRST_DUE_sum\")\n",
    "\n",
    "val df = creditRiskdf.drop(incon_droplist:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskdf: org.apache.spark.sql.DataFrame = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 280 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val creditRiskdf = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creditRiskFeatures: Array[String] = Array(EXT_SOURCE_2, EXT_SOURCE_3, client_installments_AMT_PAYMENT_min_sum, DAYS_BIRTH, AMT_CREDIT, AMT_ANNUITY, DAYS_EMPLOYED, bureau_DAYS_CREDIT_ENDDATE_max, bureau_DAYS_CREDIT_max, DAYS_ID_PUBLISH, bureau_DAYS_ENDDATE_FACT_max, bureau_AMT_CREDIT_SUM_DEBT_mean, previous_loans_CNT_PAYMENT_mean, client_cash_CNT_INSTALMENT_FUTURE_min_max, previous_loans_SELLERPLACE_AREA_max, DAYS_LAST_PHONE_CHANGE, bureau_AMT_CREDIT_SUM_max, bureau_AMT_CREDIT_SUM_mean, DAYS_REGISTRATION, client_installments_DAYS_INSTALMENT_max_max, previous_loans_AMT_DOWN_PAYMENT_max, CODE_GENDER_F, REGION_POPULATION_RELATIVE, client_cash_CNT_INSTALMENT_max_mean, bureau_DAYS_CREDIT_UPDATE_max, bureau_AMT_CREDIT_SUM_sum, client_installments_NUM_INSTALMENT_VERSION_mean_max, bureau_DAYS_CR..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//string array of features\n",
    "val creditRiskFeatures = creditRiskdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-02 07:31:50,601 WARN  [Thread-4] execution.CacheManager (Logging.scala:logWarning(66)) - Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res14: creditRiskdf.type = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 280 more fields]\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creditRiskdf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "52: error: recursive variable feat_list needs type",
     "output_type": "error",
     "traceback": [
      "<console>:52: error: recursive variable feat_list needs type",
      "               var feat_list = List.concat(feat_list, feature)",
      "                                           ^",
      ""
     ]
    }
   ],
   "source": [
    "//write function to check if there are any outliers in a given domain and show boxplot [Sean]\n",
    "//slice a small test list to ensure functionality\n",
    "//var test_lis = creditRiskFeatures.slice(0,2) //TEST var for codeblock\n",
    "\n",
    "var feat_list = List() //empty list to append features containing outliers in\n",
    "for (feature <- creditRiskFeatures) {\n",
    "    //summarise column and query the DataFrame output\n",
    "    var firstQ = creditRiskdf.select(feature).summary().where(\n",
    "        $\"summary\" === \"25%\").select(feature).first().mkString.toFloat\n",
    "    var thirdQ = creditRiskdf.select(feature).summary().where(\n",
    "        $\"summary\" === \"75%\").select(feature).first().mkString.toFloat\n",
    "    //use formulas below to test threshold of outliers\n",
    "    var testValHigh = thirdQ + (1.5 * thirdQ - firstQ)\n",
    "    var testValLow = firstQ - (1.5 * thirdQ - firstQ)\n",
    "    //check to see if thresholds are exceeded in the column and count\n",
    "    var outHigh = creditRiskdf.filter(col(feature) > lit(testValHigh)).count()\n",
    "    var outLow = creditRiskdf.filter(col(feature) < lit(testValLow)).count()\n",
    "\n",
    "    //notify us whether or not column contains outliers\n",
    "    if (outHigh > 0 || outLow > 0 ){\n",
    "        println(feature.concat(\": Contains outliers, #\").concat((\n",
    "            outHigh + outLow).toString))\n",
    "        var feat_list = List.concat(feat_list, feature)\n",
    "    }else if (outHigh = 0 || outLow = 0){\n",
    "        println(feature.concat(\": Does not contain outliers\"))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//code block takes features containing outliers and filters main dataframe against them\n",
    "//code will generate a subset of main dataset without outliers.\n",
    "val clean_df = creditriskdf\n",
    "//input feature list conatining features with outliers persisting\n",
    "for (feature <- feat_list){\n",
    "    //summarise column and query the DataFrame output\n",
    "    var firstQ = creditRiskdf.select(feature).summary().where($\"summary\" === \"25%\").select(feature).first().mkString.toFloat\n",
    "    var thirdQ = creditRiskdf.select(feature).summary().where($\"summary\" === \"75%\").select(feature).first().mkString.toFloat\n",
    "    //use formulas below to test threshold of outliers\n",
    "    var testValHigh = thirdQ + (1.5 * thirdQ - firstQ)\n",
    "    var testValLow = firstQ - (1.5 * thirdQ - firstQ)\n",
    "\n",
    "    val clean_df = clean_df.filter(feature < lit(testValHigh))\n",
    "    val clean_df = clean_df.filter(feature > lit(testValLow))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 280 more fields]\n",
       "test_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 280 more fields]\n",
       "validation_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 280 more fields]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// random split of cleansed dataset to training set (70%), testing set (20%)\n",
    "// and validation set (10%)\n",
    "\n",
    "val Array(train_df,test_df,validation_df)=creditRiskdf.randomSplit(\n",
    "    Array(0.7,0.2,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: train_df.type = [EXT_SOURCE_2: string, EXT_SOURCE_3: string ... 280 more fields]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// cache train_df dataset for ML stage\n",
    "train_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
